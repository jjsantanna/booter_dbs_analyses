{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!<br>\n",
    "Brief explanation on our Booter database analysis methodology: <br>\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of: galaxybooter\n",
    "#### File originally retrieved from: http://4lz5rmnkd6f63tmm.onion/db/galaxybooter.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Libraries that I use to analyse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from io import StringIO\n",
    "import re\n",
    "\n",
    "from IPython.display import display,clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!<br>\n",
    "STEP 0: Reading an input Booter database file<br>\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dumpfile='galaxybooter.sql'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!<br>\n",
    "STEP 1: Adaptation to our Booter database schema<br>\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading the first 'N' (100) lines of the input Booter database file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- --------------------------------------------------------------------------------\n",
      "--\n",
      "-- @version: taysmy_boot.sql Apr 30, 2012 19:50 gewa\n",
      "-- @package CMS Pro\n",
      "-- @author wojoscripts.com.\n",
      "-- @copyright 2010\n",
      "--\n",
      "-- --------------------------------------------------------------------------------\n",
      "-- Host: localhost\n",
      "-- Database: taysmy_boot\n",
      "-- Time: Apr 30, 2012-19:50\n",
      "-- MySQL version: 5.5.21-55\n",
      "-- PHP version: 5.2.17\n",
      "-- --------------------------------------------------------------------------------\n",
      "\n",
      "#\n",
      "# Database: `taysmy_boot`\n",
      "#\n",
      "\n",
      "\n",
      "-- --------------------------------------------------\n",
      "# -- Table structure for table `email_templates`\n",
      "-- --------------------------------------------------\n",
      "DROP TABLE IF EXISTS `email_templates`;\n",
      "CREATE TABLE `email_templates` (\n",
      "`id` int(5) NOT NULL AUTO_INCREMENT,\n",
      "`name` varchar(200) NOT NULL,\n",
      "`subject` varchar(255) NOT NULL,\n",
      "`help` text,\n",
      "`body` text,\n",
      "PRIMARY KEY (`id`)\n",
      ") ENGINE=MyISAM AUTO_INCREMENT=15 DEFAULT CHARSET=utf8;\n",
      "\n",
      "-- --------------------------------------------------\n",
      "# Dumping data for table `email_templates`\n",
      "-- --------------------------------------------------\n",
      "\n",
      "INSERT INTO `email_templates` (`id`, `name`, `subject`, `help`, `body`) VALUES ('1', 'Registration Email', 'Please verify your email', 'This template is used to send Registration Verification Email, when Configuration->Registration Verification is set to YES', '&lt;div align=&quot;center&quot;&gt;\\n&lt;table cellspacing=&quot;5&quot; cellpadding=&quot;5&quot; border=&quot;0&quot; width=&quot;600&quot; style=&quot;background: none repeat scroll 0% 0% rgb(244, 244, 244); border: 1px solid rgb(102, 102, 102);&quot;&gt;\\n    &lt;tbody&gt;\\n        &lt;tr&gt;\\n            &lt;th style=&quot;background-color: rgb(204, 204, 204);&quot;&gt;Welcome [NAME]! Thanks for registering.&lt;/th&gt;\\n        &lt;/tr&gt;\\n        &lt;tr&gt;\\n            &lt;td valign=&quot;top&quot; style=&quot;text-align: left;&quot;&gt;Hello,&lt;br /&gt;\\n            &lt;br /&gt;\\n            You&#039;re now a member of [SITE_NAME].&lt;br /&gt;\\n            &lt;br /&gt;\\n            Here are your login details. Please keep them in a safe place:&lt;br /&gt;\\n            &lt;br /&gt;\\n            Username: &lt;strong&gt;[USERNAME]&lt;/strong&gt;&lt;br /&gt;\\n            Password: &lt;strong&gt;[PASSWORD]&lt;/strong&gt;         &lt;hr /&gt;\\n            The administrator of this site has requested all new accounts&lt;br /&gt;\\n            to be activated by the users who created them thus your account&lt;br /&gt;\\n            is currently inactive. To activate your account,&lt;br /&gt;\\n            please visit the link below and enter the following:&lt;hr /&gt;\\n            Token: &lt;strong&gt;[TOKEN]&lt;/strong&gt;&lt;br /&gt;\\n            Email: &lt;strong&gt;[EMAIL]&lt;/strong&gt;         &lt;hr /&gt;\\n            &lt;a href=&quot;[LINK]&quot;&gt;Click here to activate tour account&lt;/a&gt;&lt;/td&gt;\\n        &lt;/tr&gt;\\n        &lt;tr&gt;\\n            &lt;td style=&quot;text-align: left;&quot;&gt;&lt;em&gt;Thanks,&lt;br /&gt;\\n            [SITE_NAME] Team&lt;br /&gt;\\n            &lt;a href=&quot;[URL]&quot;&gt;[URL]&lt;/a&gt;&lt;/em&gt;&lt;/td&gt;\\n        &lt;/tr&gt;\\n    &lt;/tbody&gt;\\n&lt;/table&gt;\\n&lt;/div&gt;');\n",
      "INSERT INTO `email_templates` (`id`, `name`, `subject`, `help`, `body`) VALUES ('2', 'Forgot Password Email', 'Password Reset', 'This template is used for retrieving lost user password', '&lt;div align=&quot;center&quot;&gt;\\n&lt;table width=&quot;600&quot; cellspacing=&quot;5&quot; cellpadding=&quot;5&quot; border=&quot;0&quot; style=&quot;background: none repeat scroll 0% 0% rgb(244, 244, 244); border: 1px solid rgb(102, 102, 102);&quot;&gt;\\n    &lt;tbody&gt;\\n        &lt;tr&gt;\\n            &lt;th style=&quot;background-color: rgb(204, 204, 204);&quot;&gt;New password reset from [SITE_NAME]!&lt;/th&gt;\\n        &lt;/tr&gt;\\n        &lt;tr&gt;\\n            &lt;td valign=&quot;top&quot; style=&quot;text-align: left;&quot;&gt;Hello, &lt;strong&gt;[USERNAME]&lt;/strong&gt;&lt;br /&gt;\\n            &lt;br /&gt;\\n            It seems that you or someone requested a new password for you.&lt;br /&gt;\\n            We have generated a new password, as requested:&lt;br /&gt;\\n            &lt;br /&gt;\\n            Your new password: &lt;strong&gt;[PASSWORD]&lt;/strong&gt;&lt;br /&gt;\\n            &lt;br /&gt;\\n            To use the new password you need to activate it. To do this click the link provided below and login with your new password.&lt;br /&gt;\\n            &lt;a href=&quot;[LINK]&quot;&gt;[LINK]&lt;/a&gt;&lt;br /&gt;\\n            &lt;br /&gt;\\n            You can change your password after you sign in.&lt;hr /&gt;\\n            Password requested from IP: [IP]&lt;/td&gt;\\n        &lt;/tr&gt;\\n        &lt;tr&gt;\\n            &lt;td style=&quot;text-align: left;&quot;&gt;&lt;em&gt;Thanks,&lt;br /&gt;\\n            [SITE_NAME] Team&lt;br /&gt;\\n            &lt;a href=&quot;[URL]&quot;&gt;[URL]&lt;/a&gt;&lt;/em&gt;&lt;/td&gt;\\n        &lt;/tr&gt;\\n    &lt;/tbody&gt;\\n&lt;/table&gt;\\n&lt;/div&gt;');\n",
      "INSERT INTO `email_templates` (`id`, `name`, `subject`, `help`, `body`) VALUES ('3', 'Welcome Mail From Admin', 'You have been registered', 'This template is used to send welcome email, when user is added by administrator', '&lt;div align=&quot;center&quot;&gt;\\n&lt;table cellspacing=&quot;5&quot; cellpadding=&quot;5&quot; border=&quot;0&quot; width=&quot;600&quot; style=&quot;background: none repeat scroll 0% 0% rgb(244, 244, 244); border: 1px solid rgb(102, 102, 102);&quot;&gt;\\n    &lt;tbody&gt;\\n        &lt;tr&gt;\\n            &lt;th style=&quot;background-color: rgb(204, 204, 204);&quot;&gt;Welcome [NAME]! You have been Registered.&lt;/th&gt;\\n        &lt;/tr&gt;\\n        &lt;tr&gt;\\n            &lt;td style=&quot;text-align: left;&quot;&gt;Hello,&lt;br /&gt;\\n            &lt;br /&gt;\\n            You&#039;re now a member of [SITE_NAME].&lt;br /&gt;\\n            &lt;br /&gt;\\n            Here are your login details. Please keep them in a safe place:&lt;br /&gt;\\n            &lt;br /&gt;\\n            Username: &lt;strong&gt;[USERNAME]&lt;/strong&gt;&lt;br /&gt;\\n            Password: &lt;strong&gt;[PASSWORD]&lt;/strong&gt;&lt;/td&gt;\\n        &lt;/tr&gt;\\n        &lt;tr&gt;\\n            &lt;td style=&quot;text-align: left;&quot;&gt;&lt;em&gt;Thanks,&lt;br /&gt;\\n            [SITE_NAME] Team&lt;br /&gt;\\n            &lt;a href=&quot;[URL]&quot;&gt;[URL]&lt;/a&gt;&lt;/em&gt;&lt;/td&gt;\\n        &lt;/tr&gt;\\n    &lt;/tbody&gt;\\n&lt;/table&gt;\\n&lt;/div&gt;');\n",
      "INSERT INTO `email_templates` (`id`, `name`, `subject`, `help`, `body`) VALUES ('4', 'Default Newsletter', 'Newsletter', 'This is a default newsletter template', '&lt;div align=&quot;center&quot;&gt;\\n&lt;table width=&quot;600&quot; cellspacing=&quot;5&quot; cellpadding=&quot;5&quot; border=&quot;0&quot; style=&quot;background: none repeat scroll 0% 0% rgb(244, 244, 244); border: 1px solid rgb(102, 102, 102);&quot;&gt;\\n    &lt;tbody&gt;\\n        &lt;tr&gt;\\n            &lt;th style=&quot;background-color: rgb(204, 204, 204);&quot;&gt;Hello [NAME]!&lt;/th&gt;\\n        &lt;/tr&gt;\\n        &lt;tr&gt;\\n            &lt;td valign=&quot;top&quot; style=&quot;text-align: left;&quot;&gt;You are receiving this email as a part of your newsletter subscription.         &lt;hr /&gt;\\n            Here goes your newsletter content         &lt;hr /&gt;\\n            &lt;/td&gt;\\n        &lt;/tr&gt;\\n        &lt;tr&gt;\\n            &lt;td style=&quot;text-align: left;&quot;&gt;&lt;em&gt;Thanks,&lt;br /&gt;\\n            [SITE_NAME] Team&lt;br /&gt;\\n            &lt;a href=&quot;[URL]&quot;&gt;[URL]&lt;/a&gt;&lt;/em&gt;         &lt;hr /&gt;\\n            &lt;span style=&quot;font-size: 11px;&quot;&gt;&lt;em&gt;To stop receiving future newsletters please login into your account         and uncheck newsletter subscription box.&lt;/em&gt;&lt;/span&gt;&lt;/td&gt;\\n        &lt;/tr&gt;\\n    &lt;/tbody&gt;\\n&lt;/table&gt;\\n&lt;/div&gt;');\n",
      "INSERT INTO `email_templates` (`id`, `name`, `subject`, `help`, `body`) VALUES ('5', 'Transaction Completed', 'Payment Completed', 'This template is used to notify administrator on successful payment transaction', '&lt;div align=&quot;center&quot;&gt;\\n&lt;table width=&quot;600&quot; cellspacing=&quot;5&quot; cellpadding=&quot;5&quot; border=&quot;0&quot; style=&quot;background: none repeat scroll 0% 0% rgb(244, 244, 244); border: 1px solid rgb(102, 102, 102);&quot;&gt;\\n    &lt;tbody&gt;\\n        &lt;tr&gt;\\n            &lt;th style=&quot;background-color: rgb(204, 204, 204);&quot;&gt;Hello, Admin&lt;/th&gt;\\n        &lt;/tr&gt;\\n        &lt;tr&gt;\\n            &lt;td valign=&quot;top&quot; style=&quot;text-align: left;&quot;&gt;You have received new payment following:&lt;br /&gt;\\n            &lt;br /&gt;\\n            Username: &lt;strong&gt;[USERNAME]&lt;/strong&gt;&lt;br /&gt;\\n            Membership: &lt;strong&gt;[ITEMNAME]&lt;/strong&gt;&lt;br /&gt;\\n            Price: &lt;strong&gt;[PRICE]&lt;/strong&gt;&lt;br /&gt;\\n            Status: &lt;strong&gt;[STATUS] &lt;/strong&gt;&lt;br /&gt;\\r\\n            Processor: &lt;strong&gt;[PP] &lt;/strong&gt;&lt;br /&gt;\\n            IP: &lt;strong&gt;[IP] &lt;/strong&gt;&lt;/td&gt;\\n        &lt;/tr&gt;\\n        &lt;tr&gt;\\n            &lt;td valign=&quot;top&quot; style=&quot;text-align: left;&quot;&gt;&lt;em&gt;You can view this transaction from your admin panel&lt;/em&gt;&lt;/td&gt;\\n        &lt;/tr&gt;\\n    &lt;/tbody&gt;\\n&lt;/table&gt;\\n&lt;/div&gt;');\n",
      "INSERT INTO `email_templates` (`id`, `name`, `subject`, `help`, `body`) VALUES ('6', 'Transaction Suspicious', 'Suspicious Transaction', 'This template is used to notify administrator on failed/suspicious payment transaction', '&lt;div align=&quot;center&quot;&gt;\\n&lt;table width=&quot;600&quot; cellspacing=&quot;5&quot; cellpadding=&quot;5&quot; border=&quot;0&quot; style=&quot;background: none repeat scroll 0% 0% rgb(244, 244, 244); border: 1px solid rgb(102, 102, 102);&quot;&gt;\\n    &lt;tbody&gt;\\n        &lt;tr&gt;\\n            &lt;th style=&quot;background-color:#ccc&quot;&gt;Hello, Admin&lt;/th&gt;\\n        &lt;/tr&gt;\\n        &lt;tr&gt;\\n            &lt;td valign=&quot;top&quot; style=&quot;text-align:left&quot;&gt;The following transaction has been disabled due to suspicious activity:&lt;br /&gt;\\n            &lt;br /&gt;\\n            Buyer: &lt;strong&gt;[USERNAME]&lt;/strong&gt;&lt;br /&gt;\\n            Item: &lt;strong&gt;[ITEM]&lt;/strong&gt;&lt;br /&gt;\\n            Price: &lt;strong&gt;[PRICE]&lt;/strong&gt;&lt;br /&gt;\\n            Status: &lt;strong&gt;[STATUS]&lt;/strong&gt;&lt;/td&gt;\\r\\n            Processor: &lt;strong&gt;[PP] &lt;/strong&gt;&lt;br /&gt;\\n        &lt;/tr&gt;\\n        &lt;tr&gt;\\n            &lt;td style=&quot;text-align:left&quot;&gt;&lt;em&gt;Please verify this transaction is correct. If it is, please activate it in the transaction section of your site&#039;s &lt;br /&gt;\\n            administration control panel. If not, it appears that someone tried to fraudulently obtain products from your site.&lt;/em&gt;&lt;/td&gt;\\n        &lt;/tr&gt;\\n    &lt;/tbody&gt;\\n&lt;/table&gt;\\n&lt;/div&gt;');\n",
      "INSERT INTO `email_templates` (`id`, `name`, `subject`, `help`, `body`) VALUES ('7', 'Welcome Email', 'Welcome', 'This template is used to welcome newly registered user when Configuration->Registration Verification and Configuration->Auto Registration are both set to YES', '&lt;div align=&quot;center&quot;&gt;\\n&lt;table width=&quot;600&quot; cellspacing=&quot;5&quot; cellpadding=&quot;5&quot; border=&quot;0&quot; style=&quot;background: none repeat scroll 0% 0% rgb(244, 244, 244); border: 1px solid rgb(102, 102, 102);&quot;&gt;\\n    &lt;tbody&gt;\\n        &lt;tr&gt;\\n            &lt;th style=&quot;background-color: rgb(204, 204, 204);&quot;&gt;Welcome [NAME]! Thanks for registering.&lt;/th&gt;\\n        &lt;/tr&gt;\\n        &lt;tr&gt;\\n            &lt;td style=&quot;text-align: left;&quot;&gt;Hello,&lt;br /&gt;\\n            &lt;br /&gt;\\n            You&#039;re now a member of [SITE_NAME].&lt;br /&gt;\\n            &lt;br /&gt;\\n            Here are your login details. Please keep them in a safe place:&lt;br /&gt;\\n            &lt;br /&gt;\\n            Username: &lt;strong&gt;[USERNAME]&lt;/strong&gt;&lt;br /&gt;\\n            Password: &lt;strong&gt;[PASSWORD]&lt;/strong&gt;&lt;/td&gt;\\n        &lt;/tr&gt;\\n        &lt;tr&gt;\\n            &lt;td style=&quot;text-align: left;&quot;&gt;&lt;em&gt;Thanks,&lt;br /&gt;\\n            [SITE_NAME] Team&lt;br /&gt;\\n            &lt;a href=&quot;[URL]&quot;&gt;[URL]&lt;/a&gt;&lt;/em&gt;&lt;/td&gt;\\n        &lt;/tr&gt;\\n    &lt;/tbody&gt;\\n&lt;/table&gt;\\n&lt;/div&gt;');\n",
      "INSERT INTO `email_templates` (`id`, `name`, `subject`, `help`, `body`) VALUES ('8', 'Membership Expire 7 days', 'Your membership will expire in 7 days', 'This template is used to remind user that membership will expire in 7 days', '&lt;div align=&quot;center&quot;&gt;\\n&lt;table cellspacing=&quot;5&quot; cellpadding=&quot;5&quot; border=&quot;0&quot; width=&quot;600&quot; style=&quot;background: none repeat scroll 0% 0% rgb(244, 244, 244); border: 1px solid rgb(102, 102, 102);&quot;&gt;\\n    &lt;tbody&gt;\\n        &lt;tr&gt;\\n            &lt;th style=&quot;background-color: rgb(204, 204, 204);&quot;&gt;Hello, [NAME]&lt;/th&gt;\\n        &lt;/tr&gt;\\n        &lt;tr&gt;\\n            &lt;td valign=&quot;top&quot; style=&quot;text-align: left;&quot;&gt;\\n            &lt;h2 style=&quot;color: rgb(255, 0, 0);&quot;&gt;Your current membership will expire in 7 days&lt;/h2&gt;\\n            Please login to your user panel to extend or upgrade your membership.&lt;/td&gt;\\n        &lt;/tr&gt;\\n        &lt;tr&gt;\\n            &lt;td style=&quot;text-align: left;&quot;&gt;&lt;em&gt;Thanks,&lt;br /&gt;\\n            [SITE_NAME] Team&lt;br /&gt;\\n            &lt;a href=&quot;[URL]&quot;&gt;[URL]&lt;/a&gt;&lt;/em&gt;&lt;/td&gt;\\n        &lt;/tr&gt;\\n    &lt;/tbody&gt;\\n&lt;/table&gt;\\n&lt;/div&gt;');\n",
      "INSERT INTO `email_templates` (`id`, `name`, `subject`, `help`, `body`) VALUES ('9', 'Membership Expired Today', 'Your membership has expired', 'This template is used to remind user that membership had expired', '&lt;div align=&quot;center&quot;&gt;\\n&lt;table width=&quot;600&quot; cellspacing=&quot;5&quot; cellpadding=&quot;5&quot; border=&quot;0&quot; style=&quot;background: none repeat scroll 0% 0% rgb(244, 244, 244); border: 1px solid rgb(102, 102, 102);&quot;&gt;\\n    &lt;tbody&gt;\\n        &lt;tr&gt;\\n            &lt;th style=&quot;background-color: rgb(204, 204, 204);&quot;&gt;Hello, [NAME]&lt;/th&gt;\\n        &lt;/tr&gt;\\n        &lt;tr&gt;\\n            &lt;td valign=&quot;top&quot; style=&quot;text-align: left;&quot;&gt;\\n            &lt;h2 style=&quot;color: rgb(255, 0, 0);&quot;&gt;Your current membership has expired!&lt;/h2&gt;\\n            Please login to your user panel to extend or upgrade your membership.&lt;/td&gt;\\n        &lt;/tr&gt;\\n        &lt;tr&gt;\\n            &lt;td style=&quot;text-align: left;&quot;&gt;&lt;em&gt;Thanks,&lt;br /&gt;\\n            [SITE_NAME] Team&lt;br /&gt;\\n            &lt;a href=&quot;[URL]&quot;&gt;[URL]&lt;/a&gt;&lt;/em&gt;&lt;/td&gt;\\n        &lt;/tr&gt;\\n    &lt;/tbody&gt;\\n&lt;/table&gt;\\n&lt;/div&gt;');\n",
      "INSERT INTO `email_templates` (`id`, `name`, `subject`, `help`, `body`) VALUES ('10', 'Contact Request', 'Contact Inquiry', 'This template is used to send default Contact Request Form', '&lt;div align=&quot;center&quot;&gt;\\n&lt;table width=&quot;600&quot; cellspacing=&quot;5&quot; cellpadding=&quot;5&quot; border=&quot;0&quot; style=&quot;background: none repeat scroll 0% 0% rgb(244, 244, 244); border: 1px solid rgb(102, 102, 102);&quot;&gt;\\n    &lt;tbody&gt;\\n        &lt;tr&gt;\\n            &lt;th style=&quot;background-color: rgb(204, 204, 204);&quot;&gt;Hello Admin&lt;/th&gt;\\n        &lt;/tr&gt;\\n        &lt;tr&gt;\\n            &lt;td valign=&quot;top&quot; style=&quot;text-align: left;&quot;&gt;You have a new contact request:         &lt;hr /&gt;\\n            [MESSAGE]         &lt;hr /&gt;\\n            From: &lt;strong&gt;[SENDER] - [NAME]&lt;/strong&gt;&lt;br /&gt;\\n            Subject: &lt;strong&gt;[MAILSUBJECT]&lt;/strong&gt;&lt;br /&gt;\\n            Senders IP: &lt;strong&gt;[IP]&lt;/strong&gt;&lt;/td&gt;\\n        &lt;/tr&gt;\\n    &lt;/tbody&gt;\\n&lt;/table&gt;\\n&lt;/div&gt;');\n",
      "INSERT INTO `email_templates` (`id`, `name`, `subject`, `help`, `body`) VALUES ('12', 'Single Email', 'Single User Email', 'This template is used to email single user', '&lt;div align=&quot;center&quot;&gt;\\n  &lt;table width=&quot;600&quot; cellspacing=&quot;5&quot; cellpadding=&quot;5&quot; border=&quot;0&quot; style=&quot;background: none repeat scroll 0% 0% rgb(244, 244, 244); border: 1px solid rgb(102, 102, 102);&quot;&gt;\\n    &lt;tbody&gt;\\n      &lt;tr&gt;\\n        &lt;th style=&quot;background-color:#ccc&quot;&gt;Hello [NAME]&lt;/th&gt;\\n      &lt;/tr&gt;\\n      &lt;tr&gt;\\n        &lt;td valign=&quot;top&quot; style=&quot;text-align:left&quot;&gt;Your message goes here...&lt;/td&gt;\\n      &lt;/tr&gt;\\n      &lt;tr&gt;\\n        &lt;td style=&quot;text-align:left&quot;&gt;&lt;em&gt;Thanks,&lt;br /&gt;\\n          [SITE_NAME] Team&lt;br /&gt;\\n          &lt;a href=&quot;[URL]&quot;&gt;[URL]&lt;/a&gt;&lt;/em&gt;&lt;/td&gt;\\n      &lt;/tr&gt;\\n    &lt;/tbody&gt;\\n  &lt;/table&gt;\\n&lt;/div&gt;');\n",
      "INSERT INTO `email_templates` (`id`, `name`, `subject`, `help`, `body`) VALUES ('13', 'Notify Admin', 'New User Registration', 'This template is used to notify admin of new registration when Configuration->Registration Notification is set to YES', '&lt;div align=&quot;center&quot;&gt;\\n&lt;table cellspacing=&quot;5&quot; cellpadding=&quot;5&quot; border=&quot;0&quot; width=&quot;600&quot; style=&quot;background: none repeat scroll 0% 0% rgb(244, 244, 244); border: 1px solid rgb(102, 102, 102);&quot;&gt;\\n    &lt;tbody&gt;\\n        &lt;tr&gt;\\n            &lt;th style=&quot;background-color: rgb(204, 204, 204);&quot;&gt;Hello Admin&lt;/th&gt;\\n        &lt;/tr&gt;\\n        &lt;tr&gt;\\n            &lt;td valign=&quot;top&quot; style=&quot;text-align: left;&quot;&gt;You have a new user registration. You can login into your admin panel to view details:&lt;hr /&gt;\\n            Username: &lt;strong&gt;[USERNAME]&lt;/strong&gt;&lt;br /&gt;\\n            Name: &lt;strong&gt;[NAME]&lt;/strong&gt;&lt;br /&gt;\\n            IP: &lt;strong&gt;[IP]&lt;/strong&gt;&lt;/td&gt;\\n        &lt;/tr&gt;\\n    &lt;/tbody&gt;\\n&lt;/table&gt;\\n&lt;/div&gt;');\n",
      "INSERT INTO `email_templates` (`id`, `name`, `subject`, `help`, `body`) VALUES ('14', 'Registration Pending', 'Registration Verification Pending', 'This template is used to send Registration Verification Email, when Configuration->Auto Registration is set to NO', '&lt;div align=&quot;center&quot;&gt;\\n&lt;table cellspacing=&quot;5&quot; cellpadding=&quot;5&quot; border=&quot;0&quot; width=&quot;600&quot; style=&quot;background: none repeat scroll 0% 0% rgb(244, 244, 244); border: 1px solid rgb(102, 102, 102);&quot;&gt;\\n    &lt;tbody&gt;\\n        &lt;tr&gt;\\n            &lt;th style=&quot;background-color: rgb(204, 204, 204);&quot;&gt;Welcome [NAME]! Thanks for registering.&lt;/th&gt;\\n        &lt;/tr&gt;\\n        &lt;tr&gt;\\n            &lt;td valign=&quot;top&quot; style=&quot;text-align: left;&quot;&gt;Hello,&lt;br /&gt;\\n            &lt;br /&gt;\\n            You&#039;re now a member of [SITE_NAME].&lt;br /&gt;\\n            &lt;br /&gt;\\n            Here are your login details. Please keep them in a safe place:&lt;br /&gt;\\n            &lt;br /&gt;\\n            Username: &lt;strong&gt;[USERNAME]&lt;/strong&gt;&lt;br /&gt;\\n            Password: &lt;strong&gt;[PASSWORD]&lt;/strong&gt;         &lt;hr /&gt;\\n            The administrator of this site has requested all new accounts&lt;br /&gt;\\n            to be activated by the users who created them thus your account&lt;br /&gt;\\n            is currently pending verification process.&lt;/td&gt;\\n        &lt;/tr&gt;\\n        &lt;tr&gt;\\n            &lt;td style=&quot;text-align: left;&quot;&gt;&lt;em&gt;Thanks,&lt;br /&gt;\\n            [SITE_NAME] Team&lt;br /&gt;\\n            &lt;a href=&quot;[URL]&quot;&gt;[URL]&lt;/a&gt;&lt;/em&gt;&lt;/td&gt;\\n        &lt;/tr&gt;\\n    &lt;/tbody&gt;\\n&lt;/table&gt;\\n&lt;/div&gt;');\n",
      "\n",
      "\n",
      "-- --------------------------------------------------\n",
      "# -- Table structure for table `gateways`\n",
      "-- --------------------------------------------------\n",
      "DROP TABLE IF EXISTS `gateways`;\n",
      "CREATE TABLE `gateways` (\n",
      "`id` int(11) NOT NULL AUTO_INCREMENT,\n",
      "`name` varchar(255) NOT NULL,\n",
      "`displayname` varchar(255) NOT NULL,\n",
      "`dir` varchar(255) NOT NULL,\n",
      "`demo` tinyint(1) NOT NULL DEFAULT '1',\n",
      "`extra_txt` varchar(255) NOT NULL,\n",
      "`extra_txt2` varchar(255) NOT NULL,\n",
      "`extra_txt3` varchar(255) DEFAULT NULL,\n",
      "`extra` varchar(255) NOT NULL,\n",
      "`extra2` varchar(255) NOT NULL,\n",
      "`extra3` varchar(255) DEFAULT NULL,\n",
      "`is_recurring` tinyint(1) NOT NULL DEFAULT '0',\n",
      "`active` tinyint(1) NOT NULL DEFAULT '1',\n",
      "PRIMARY KEY (`id`)\n",
      ") ENGINE=MyISAM AUTO_INCREMENT=3 DEFAULT CHARSET=utf8;\n",
      "\n",
      "-- --------------------------------------------------\n",
      "# Dumping data for table `gateways`\n",
      "-- --------------------------------------------------\n",
      "\n",
      "INSERT INTO `gateways` (`id`, `name`, `displayname`, `dir`, `demo`, `extra_txt`, `extra_txt2`, `extra_txt3`, `extra`, `extra2`, `extra3`, `is_recurring`, `active`) VALUES ('1', 'paypal', 'PayPal', 'paypal', '1', 'Paypal Email Address', 'Currency Code', 'Not in Use', 'hfdeceive@hotmail.com', 'USD', '', '1', '1');\n",
      "INSERT INTO `gateways` (`id`, `name`, `displayname`, `dir`, `demo`, `extra_txt`, `extra_txt2`, `extra_txt3`, `extra`, `extra2`, `extra3`, `is_recurring`, `active`) VALUES ('2', 'moneybookers', 'MoneyBookers', 'moneybookers', '0', 'MoneyBookers Email Address', 'Currency Code', 'Secret Passphrase', 'moneybookers@address.com', 'EUR', 'mypassphrase', '1', '0');\n",
      "\n",
      "\n",
      "-- --------------------------------------------------\n",
      "# -- Table structure for table `getshells`\n",
      "-- --------------------------------------------------\n",
      "DROP TABLE IF EXISTS `getshells`;\n",
      "CREATE TABLE `getshells` (\n",
      "`URL` varchar(1000) NOT NULL,\n",
      "`online` int(1) NOT NULL DEFAULT '0',\n",
      "`lastChecked` int(10) NOT NULL DEFAULT '0'\n",
      ") ENGINE=MyISAM DEFAULT CHARSET=latin1;\n",
      "\n",
      "-- --------------------------------------------------\n",
      "# Dumping data for table `getshells`\n",
      "-- --------------------------------------------------\n",
      "\n",
      "INSERT INTO `getshells` (`URL`, `online`, `lastChecked`) VALUES ('http://www.filbanken.nu/awstats/awstats/UDP.php', '0', '0');\n",
      "INSERT INTO `getshells` (`URL`, `online`, `lastChecked`) VALUES ('http://www.indianethicalhacker.blackapplehost.com/xoep.php?', '0', '0');\n",
      "INSERT INTO `getshells` (`URL`, `online`, `lastChecked`) VALUES ('http://www.indianethicalhacker.blackapplehost.com/xoep.php ', '0', '0');\n",
      "INSERT INTO `getshells` (`URL`, `online`, `lastChecked`) VALUES ('http://www.indianethicalhacker.blackapplehost.com/xoep.php\\t', '0', '0');\n",
      "INSERT INTO `getshells` (`URL`, `online`, `lastChecked`) VALUES ('http://163.178.170.74/webdav/greenshell.php', '0', '0');\n"
     ]
    }
   ],
   "source": [
    "lines_to_read=100\n",
    "\n",
    "with open(dumpfile) as myfile:\n",
    "    firstlines=myfile.readlines()[0:lines_to_read] #put here the interval you want\n",
    "    for x in firstlines:\n",
    "        print(x.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Listing tables that have content inserted into the dump file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_tables_with_insert(dumpfile):\n",
    "    tables = []\n",
    "    with open(dumpfile, 'rb') as f:\n",
    "        for line in f:\n",
    "            line = line.decode(\"utf-8\").strip()\n",
    "            if line.lower().startswith('insert'):\n",
    "                table = re.findall(r'`(.*?)`', line)\n",
    "                tables.append(table[0])\n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 email_templates\n",
      "2 email_templates\n",
      "3 email_templates\n",
      "4 email_templates\n",
      "5 email_templates\n",
      "6 email_templates\n",
      "7 email_templates\n",
      "8 email_templates\n",
      "9 email_templates\n",
      "10 email_templates\n",
      "11 email_templates\n",
      "12 email_templates\n",
      "13 email_templates\n",
      "14 gateways\n",
      "15 gateways\n",
      "16 getshells\n",
      "17 getshells\n",
      "18 getshells\n",
      "19 getshells\n",
      "20 getshells\n",
      "21 getshells\n",
      "22 getshells\n",
      "23 getshells\n",
      "24 getshells\n",
      "25 getshells\n",
      "26 getshells\n",
      "27 getshells\n",
      "28 getshells\n",
      "29 getshells\n",
      "30 getshells\n",
      "31 getshells\n",
      "32 getshells\n",
      "33 getshells\n",
      "34 getshells\n",
      "35 getshells\n",
      "36 getshells\n",
      "37 getshells\n",
      "38 getshells\n",
      "39 getshells\n",
      "40 getshells\n",
      "41 getshells\n",
      "42 getshells\n",
      "43 getshells\n",
      "44 getshells\n",
      "45 getshells\n",
      "46 getshells\n",
      "47 getshells\n",
      "48 getshells\n",
      "49 getshells\n",
      "50 getshells\n",
      "51 getshells\n",
      "52 getshells\n",
      "53 getshells\n",
      "54 getshells\n",
      "55 getshells\n",
      "56 getshells\n",
      "57 getshells\n",
      "58 getshells\n",
      "59 getshells\n",
      "60 getshells\n",
      "61 getshells\n",
      "62 getshells\n",
      "63 getshells\n",
      "64 getshells\n",
      "65 getshells\n",
      "66 getshells\n",
      "67 getshells\n",
      "68 getshells\n",
      "69 getshells\n",
      "70 getshells\n",
      "71 getshells\n",
      "72 getshells\n",
      "73 getshells\n",
      "74 getshells\n",
      "75 getshells\n",
      "76 getshells\n",
      "77 getshells\n",
      "78 getshells\n",
      "79 getshells\n",
      "80 getshells\n",
      "81 getshells\n",
      "82 getshells\n",
      "83 getshells\n",
      "84 getshells\n",
      "85 getshells\n",
      "86 getshells\n",
      "87 getshells\n",
      "88 getshells\n",
      "89 getshells\n",
      "90 getshells\n",
      "91 getshells\n",
      "92 getshells\n",
      "93 getshells\n",
      "94 getshells\n",
      "95 getshells\n",
      "96 getshells\n",
      "97 getshells\n",
      "98 getshells\n",
      "99 getshells\n",
      "100 getshells\n",
      "101 getshells\n",
      "102 getshells\n",
      "103 getshells\n",
      "104 getshells\n",
      "105 getshells\n",
      "106 getshells\n",
      "107 getshells\n",
      "108 getshells\n",
      "109 getshells\n",
      "110 getshells\n",
      "111 getshells\n",
      "112 getshells\n",
      "113 getshells\n",
      "114 getshells\n",
      "115 getshells\n",
      "116 getshells\n",
      "117 getshells\n",
      "118 getshells\n",
      "119 getshells\n",
      "120 getshells\n",
      "121 getshells\n",
      "122 getshells\n",
      "123 getshells\n",
      "124 getshells\n",
      "125 getshells\n",
      "126 getshells\n",
      "127 getshells\n",
      "128 getshells\n",
      "129 getshells\n",
      "130 getshells\n",
      "131 getshells\n",
      "132 getshells\n",
      "133 getshells\n",
      "134 getshells\n",
      "135 getshells\n",
      "136 getshells\n",
      "137 getshells\n",
      "138 getshells\n",
      "139 getshells\n",
      "140 getshells\n",
      "141 getshells\n",
      "142 getshells\n",
      "143 getshells\n",
      "144 getshells\n",
      "145 getshells\n",
      "146 getshells\n",
      "147 getshells\n",
      "148 getshells\n",
      "149 getshells\n",
      "150 getshells\n",
      "151 getshells\n",
      "152 getshells\n",
      "153 getshells\n",
      "154 getshells\n",
      "155 getshells\n",
      "156 getshells\n",
      "157 getshells\n",
      "158 getshells\n",
      "159 getshells\n",
      "160 getshells\n",
      "161 getshells\n",
      "162 getshells\n",
      "163 getshells\n",
      "164 getshells\n",
      "165 getshells\n",
      "166 getshells\n",
      "167 getshells\n",
      "168 getshells\n",
      "169 getshells\n",
      "170 getshells\n",
      "171 getshells\n",
      "172 getshells\n",
      "173 getshells\n",
      "174 getshells\n",
      "175 getshells\n",
      "176 getshells\n",
      "177 getshells\n",
      "178 getshells\n",
      "179 getshells\n",
      "180 getshells\n",
      "181 getshells\n",
      "182 getshells\n",
      "183 getshells\n",
      "184 getshells\n",
      "185 getshells\n",
      "186 getshells\n",
      "187 getshells\n",
      "188 getshells\n",
      "189 getshells\n",
      "190 getshells\n",
      "191 getshells\n",
      "192 getshells\n",
      "193 getshells\n",
      "194 getshells\n",
      "195 getshells\n",
      "196 getshells\n",
      "197 getshells\n",
      "198 getshells\n",
      "199 getshells\n",
      "200 getshells\n",
      "201 getshells\n",
      "202 getshells\n",
      "203 getshells\n",
      "204 getshells\n",
      "205 getshells\n",
      "206 getshells\n",
      "207 getshells\n",
      "208 getshells\n",
      "209 getshells\n",
      "210 getshells\n",
      "211 getshells\n",
      "212 getshells\n",
      "213 getshells\n",
      "214 getshells\n",
      "215 getshells\n",
      "216 getshells\n",
      "217 getshells\n",
      "218 getshells\n",
      "219 getshells\n",
      "220 getshells\n",
      "221 getshells\n",
      "222 getshells\n",
      "223 getshells\n",
      "224 getshells\n",
      "225 getshells\n",
      "226 getshells\n",
      "227 getshells\n",
      "228 getshells\n",
      "229 getshells\n",
      "230 getshells\n",
      "231 getshells\n",
      "232 getshells\n",
      "233 getshells\n",
      "234 getshells\n",
      "235 getshells\n",
      "236 getshells\n",
      "237 getshells\n",
      "238 getshells\n",
      "239 getshells\n",
      "240 getshells\n",
      "241 getshells\n",
      "242 getshells\n",
      "243 getshells\n",
      "244 getshells\n",
      "245 getshells\n",
      "246 getshells\n",
      "247 getshells\n",
      "248 getshells\n",
      "249 getshells\n",
      "250 getshells\n",
      "251 getshells\n",
      "252 getshells\n",
      "253 getshells\n",
      "254 getshells\n",
      "255 getshells\n",
      "256 getshells\n",
      "257 getshells\n",
      "258 getshells\n",
      "259 getshells\n",
      "260 getshells\n",
      "261 getshells\n",
      "262 getshells\n",
      "263 getshells\n",
      "264 getshells\n",
      "265 getshells\n",
      "266 getshells\n",
      "267 getshells\n",
      "268 getshells\n",
      "269 getshells\n",
      "270 getshells\n",
      "271 getshells\n",
      "272 getshells\n",
      "273 getshells\n",
      "274 getshells\n",
      "275 getshells\n",
      "276 getshells\n",
      "277 getshells\n",
      "278 getshells\n",
      "279 getshells\n",
      "280 getshells\n",
      "281 getshells\n",
      "282 getshells\n",
      "283 getshells\n",
      "284 getshells\n",
      "285 getshells\n",
      "286 getshells\n",
      "287 getshells\n",
      "288 getshells\n",
      "289 getshells\n",
      "290 getshells\n",
      "291 getshells\n",
      "292 getshells\n",
      "293 getshells\n",
      "294 getshells\n",
      "295 getshells\n",
      "296 getshells\n",
      "297 getshells\n",
      "298 getshells\n",
      "299 getshells\n",
      "300 logs\n",
      "301 logs\n",
      "302 logs\n",
      "303 logs\n",
      "304 logs\n",
      "305 logs\n",
      "306 logs\n",
      "307 logs\n",
      "308 logs\n",
      "309 logs\n",
      "310 logs\n",
      "311 logs\n",
      "312 logs\n",
      "313 logs\n",
      "314 logs\n",
      "315 logs\n",
      "316 logs\n",
      "317 logs\n",
      "318 logs\n",
      "319 logs\n",
      "320 logs\n",
      "321 logs\n",
      "322 logs\n",
      "323 logs\n",
      "324 logs\n",
      "325 logs\n",
      "326 logs\n",
      "327 logs\n",
      "328 memberships\n",
      "329 memberships\n",
      "330 memberships\n",
      "331 memberships\n",
      "332 memberships\n",
      "333 news\n",
      "334 payments\n",
      "335 payments\n",
      "336 payments\n",
      "337 postshells\n",
      "338 postshells\n",
      "339 postshells\n",
      "340 postshells\n",
      "341 postshells\n",
      "342 postshells\n",
      "343 postshells\n",
      "344 postshells\n",
      "345 postshells\n",
      "346 postshells\n",
      "347 postshells\n",
      "348 postshells\n",
      "349 postshells\n",
      "350 postshells\n",
      "351 postshells\n",
      "352 postshells\n",
      "353 postshells\n",
      "354 postshells\n",
      "355 postshells\n",
      "356 postshells\n",
      "357 postshells\n",
      "358 postshells\n",
      "359 postshells\n",
      "360 postshells\n",
      "361 postshells\n",
      "362 postshells\n",
      "363 postshells\n",
      "364 postshells\n",
      "365 postshells\n",
      "366 postshells\n",
      "367 postshells\n",
      "368 postshells\n",
      "369 postshells\n",
      "370 postshells\n",
      "371 postshells\n",
      "372 postshells\n",
      "373 postshells\n",
      "374 postshells\n",
      "375 postshells\n",
      "376 postshells\n",
      "377 postshells\n",
      "378 postshells\n",
      "379 postshells\n",
      "380 postshells\n",
      "381 postshells\n",
      "382 postshells\n",
      "383 postshells\n",
      "384 postshells\n",
      "385 postshells\n",
      "386 postshells\n",
      "387 postshells\n",
      "388 postshells\n",
      "389 postshells\n",
      "390 postshells\n",
      "391 postshells\n",
      "392 postshells\n",
      "393 postshells\n",
      "394 postshells\n",
      "395 postshells\n",
      "396 postshells\n",
      "397 postshells\n",
      "398 postshells\n",
      "399 postshells\n",
      "400 postshells\n",
      "401 postshells\n",
      "402 postshells\n",
      "403 postshells\n",
      "404 postshells\n",
      "405 postshells\n",
      "406 postshells\n",
      "407 postshells\n",
      "408 postshells\n",
      "409 postshells\n",
      "410 postshells\n",
      "411 postshells\n",
      "412 postshells\n",
      "413 postshells\n",
      "414 postshells\n",
      "415 postshells\n",
      "416 postshells\n",
      "417 postshells\n",
      "418 postshells\n",
      "419 postshells\n",
      "420 postshells\n",
      "421 postshells\n",
      "422 postshells\n",
      "423 postshells\n",
      "424 postshells\n",
      "425 postshells\n",
      "426 postshells\n",
      "427 postshells\n",
      "428 postshells\n",
      "429 postshells\n",
      "430 postshells\n",
      "431 postshells\n",
      "432 postshells\n",
      "433 postshells\n",
      "434 postshells\n",
      "435 postshells\n",
      "436 postshells\n",
      "437 postshells\n",
      "438 postshells\n",
      "439 postshells\n",
      "440 postshells\n",
      "441 postshells\n",
      "442 postshells\n",
      "443 postshells\n",
      "444 postshells\n",
      "445 postshells\n",
      "446 postshells\n",
      "447 postshells\n",
      "448 postshells\n",
      "449 postshells\n",
      "450 postshells\n",
      "451 postshells\n",
      "452 postshells\n",
      "453 postshells\n",
      "454 postshells\n",
      "455 postshells\n",
      "456 postshells\n",
      "457 postshells\n",
      "458 postshells\n",
      "459 postshells\n",
      "460 postshells\n",
      "461 postshells\n",
      "462 postshells\n",
      "463 postshells\n",
      "464 postshells\n",
      "465 postshells\n",
      "466 postshells\n",
      "467 postshells\n",
      "468 postshells\n",
      "469 postshells\n",
      "470 postshells\n",
      "471 postshells\n",
      "472 postshells\n",
      "473 postshells\n",
      "474 postshells\n",
      "475 postshells\n",
      "476 postshells\n",
      "477 postshells\n",
      "478 postshells\n",
      "479 postshells\n",
      "480 postshells\n",
      "481 postshells\n",
      "482 postshells\n",
      "483 postshells\n",
      "484 postshells\n",
      "485 postshells\n",
      "486 postshells\n",
      "487 postshells\n",
      "488 postshells\n",
      "489 postshells\n",
      "490 postshells\n",
      "491 postshells\n",
      "492 postshells\n",
      "493 postshells\n",
      "494 postshells\n",
      "495 postshells\n",
      "496 postshells\n",
      "497 postshells\n",
      "498 postshells\n",
      "499 postshells\n",
      "500 postshells\n",
      "501 postshells\n",
      "502 postshells\n",
      "503 postshells\n",
      "504 postshells\n",
      "505 postshells\n",
      "506 postshells\n",
      "507 postshells\n",
      "508 postshells\n",
      "509 postshells\n",
      "510 postshells\n",
      "511 postshells\n",
      "512 postshells\n",
      "513 postshells\n",
      "514 postshells\n",
      "515 postshells\n",
      "516 postshells\n",
      "517 postshells\n",
      "518 postshells\n",
      "519 postshells\n",
      "520 postshells\n",
      "521 postshells\n",
      "522 postshells\n",
      "523 postshells\n",
      "524 postshells\n",
      "525 postshells\n",
      "526 postshells\n",
      "527 postshells\n",
      "528 postshells\n",
      "529 postshells\n",
      "530 postshells\n",
      "531 postshells\n",
      "532 postshells\n",
      "533 postshells\n",
      "534 postshells\n",
      "535 postshells\n",
      "536 postshells\n",
      "537 postshells\n",
      "538 settings\n",
      "539 slowloris\n",
      "540 slowloris\n",
      "541 slowloris\n",
      "542 slowloris\n",
      "543 slowloris\n",
      "544 slowloris\n",
      "545 slowloris\n",
      "546 slowloris\n",
      "547 slowloris\n",
      "548 slowloris\n",
      "549 slowloris\n",
      "550 slowloris\n",
      "551 slowloris\n",
      "552 slowloris\n",
      "553 slowloris\n",
      "554 slowloris\n",
      "555 slowloris\n",
      "556 slowloris\n",
      "557 slowloris\n",
      "558 slowloris\n",
      "559 slowloris\n",
      "560 slowloris\n",
      "561 slowloris\n",
      "562 slowloris\n",
      "563 slowloris\n",
      "564 slowloris\n",
      "565 slowloris\n",
      "566 slowloris\n",
      "567 slowloris\n",
      "568 slowloris\n",
      "569 slowloris\n",
      "570 slowloris\n",
      "571 slowloris\n",
      "572 slowloris\n",
      "573 slowloris\n",
      "574 slowloris\n",
      "575 slowloris\n",
      "576 slowloris\n",
      "577 slowloris\n",
      "578 slowloris\n",
      "579 slowloris\n",
      "580 slowloris\n",
      "581 slowloris\n",
      "582 slowloris\n",
      "583 slowloris\n",
      "584 slowloris\n",
      "585 slowloris\n",
      "586 slowloris\n",
      "587 slowloris\n",
      "588 slowloris\n",
      "589 slowloris\n",
      "590 slowloris\n",
      "591 slowloris\n",
      "592 slowloris\n",
      "593 slowloris\n",
      "594 slowloris\n",
      "595 slowloris\n",
      "596 slowloris\n",
      "597 slowloris\n",
      "598 slowloris\n",
      "599 slowloris\n",
      "600 slowloris\n",
      "601 slowloris\n",
      "602 slowloris\n",
      "603 users\n",
      "604 users\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'enumerate' object has no attribute 'uni'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c943263317a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muni\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'enumerate' object has no attribute 'uni'"
     ]
    }
   ],
   "source": [
    "tables=enumerate(list_tables_with_insert(dumpfile))\n",
    "\n",
    "for i, item in tables:\n",
    "    print(i+1,item)\n",
    "    \n",
    "print(tables.uni)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the list above looks like? \n",
    "looks well-formed but many insert operations in a same table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align='center'>======================================================================\n",
    "If NOT well-formed SQL dump file then you must first do the following:\n",
    "======================================================================</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Removing lines that are not part of the actual content to be analysed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# num_header_lines=55\n",
    "\n",
    "# columns=['id', 'username', 'action', 'ip','date','platform','hidden']\n",
    "\n",
    "# df_manual= pd.read_csv(dumpfile,\n",
    "#             delimiter=\",\",\n",
    "#             skiprows = num_header_lines,\n",
    "#             names = columns)\n",
    "# df_manual.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually analysing if first column has an SQL insert operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_manual[df_manual['id'].str.contains('INSERT INTO')].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually analysing if the last column contains ';' remaing from the end of an insert operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_manual[df_manual['hidden'].str.contains(';')].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Splitting and naming tables and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting the content of column 'action' in 6 new columns:\n",
    "# df_attacks['targetip']=df_attacks['action'].str.extract('on (?P<targetip>[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+)', expand=True) \n",
    "# df_attacks['targeturl']=df_attacks['action'].str.extract('on [A-Z]+://(?P<targeturl>[^\\s:/]+)', expand=True, flags=re.IGNORECASE)\n",
    "# df_attacks['port']=df_attacks['action'].str.extract(':(?P<targetport>[0-9]+)', expand=True)\n",
    "# df_attacks['duration']=df_attacks['action'].str.extract('for\\s+(?P<duration>[0-9]+)', expand=True)\n",
    "# df_attacks['type']=df_attacks['action'].str.extract('using\\s+([a-z]\\w{0,})', expand=True, flags=re.IGNORECASE)\n",
    "# df_attacks['vip_attack']=df_attacks['action'].str.contains('Launched a VIP stress test on', regex=True)\n",
    "# df_attacks['target_ddosprotected']=df_attacks['action'].str.contains('DDoS protected', regex=True)\n",
    "\n",
    "# Splitting the content of column 'ip' in 2 new columns\n",
    "# df_attacks['userip']=df_attacks['ip'].str.extract('(([0-9a-fA-F]*[:\\.])+[0-9a-fA-F]*)', expand=True)[0] #for  IPv4 and IPv6\n",
    "# df_attacks['country']=df_attacks['ip'].str.extract('([A-Z]\\w)', expand=True,flags=re.IGNORECASE)\n",
    "\n",
    "# Splitting the content of column 'platform' in 2 new columns\n",
    "# df_attacks['user_os']=df_attacks['platform'].str.extract('on\\s+(\\w+)', expand=True,flags=re.IGNORECASE)\n",
    "# df_attacks['user_browser']=df_attacks['platform'].str.extract('\\'(.*)\\s+on', expand=True,flags=re.IGNORECASE)\n",
    "\n",
    "# Splitting table 'attacks' to compose table 'logins'\n",
    "# df_logins=df_attacks[['username','userip','date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align='center'>======================================================================\n",
    "Additional functions\n",
    "======================================================================</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Function to read tables from wel-formed SQL database dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For well formed SQL database dumps!\n",
    "def read_inserted_table(dumpfile, target_table):\n",
    "    sio = StringIO()\n",
    "    fast_forward = True\n",
    "    with open(dumpfile, 'rb') as f:\n",
    "        for line in f:\n",
    "            line = line.decode(\"utf-8\").strip()\n",
    "            line = re.sub(\"(?!(([^']*'){2})*[^']*$)\\)\", '',line) #Step0:remove any \")\" from the content of columns\n",
    "            if line.lower().startswith('insert') and target_table in line:\n",
    "                fast_forward = False\n",
    "            if fast_forward:\n",
    "                continue\n",
    "            data = re.findall('\\([^\\)]*\\)', line) #Step1: get the content between parentesis (i.e., insert line)\n",
    "            try:\n",
    "                newline = data[0].strip('()') #Step2:remove parenthesis\n",
    "                newline=newline.replace('`','') #Step3: remove ` (usually in table names)\n",
    "                newline=re.sub(r'(?!(([^\\']*\\'){2})*[^\\']*$),', '', newline) #Step4: remove commas from the content of columns\n",
    "                newline=newline.replace('\\'','') #Step5: remove single quotes\n",
    "                newline=newline.replace(', ', ',') #Step6: remove single spaces after comma (i.e., in the beginning of a column)\n",
    "                sio.write(newline)\n",
    "                sio.write(\"\\n\")\n",
    "            except IndexError:\n",
    "                pass\n",
    "            if line.endswith(';'):\n",
    "                break\n",
    "    sio.seek(0)\n",
    "    return sio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Converter functions for formatting content of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tolowercase(text):\n",
    "    try:\n",
    "        return text.lower()\n",
    "    except AttributeError:\n",
    "        return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_singlequote(text):\n",
    "    try:\n",
    "        return text.strip('\\'')\n",
    "    except AttributeError:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timestamp2datetime(timestamp):\n",
    "    try:\n",
    "        return  pd.to_datetime(timestamp,unit='s')\n",
    "    except AttributeError:\n",
    "        return timestamp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dateformat2datetime(text):\n",
    "    try:\n",
    "        return  pd.to_datetime(text,format='%d-%m-%Y %H:%M')\n",
    "    except AttributeError:\n",
    "        return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_singlequote_and_tolowercase(text):\n",
    "    try:\n",
    "        return text.strip('\\'').lower()\n",
    "    except AttributeError:\n",
    "        return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_singlequote_and_dateformat2datetime(text):\n",
    "    try:\n",
    "        return  pd.to_datetime(text.strip('\\''),format='%d-%m-%Y %H:%M')\n",
    "    except AttributeError:\n",
    "        return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_parenthesisandsemicolon(text):\n",
    "    try:\n",
    "        return text.replace(');',\"\")\n",
    "    except AttributeError:\n",
    "        return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_insertintologs(text):\n",
    "    try:\n",
    "        return int(text.replace('INSERT INTO `logs` VALUES (',\"\"))\n",
    "    except AttributeError:\n",
    "        return text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align='center'>======================================================================\n",
    "Adapting EACH existing table\n",
    "======================================================================</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Table: 'api'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How this table looks like without modification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablename='api'\n",
    "\n",
    "pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "            delimiter=\",\",\n",
    "            error_bad_lines=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Is this table different from other previous loaded table? YES\n",
    "#### Q2: Is this table similar to any table in the generic Booter database schema? NO\n",
    "    - This table looks like: \n",
    "    - The blacklist table must to have the columns: \n",
    "    \n",
    "#### Q3: Are there modifications required? \n",
    "    - On the table name: \n",
    "        o \n",
    "    - On the column type/converter: \n",
    "        o    \n",
    "    - On the column name:\n",
    "        o  \n",
    "    - Split columns:\n",
    "        o \n",
    "    - Add required columns:\n",
    "        o "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Table: 'blacklist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How this table looks like without modification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablename='blacklist'\n",
    "\n",
    "# Only displaying the results\n",
    "pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "            delimiter=\",\",\n",
    "            error_bad_lines=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Is this table different from other previous loaded table? YES\n",
    "#### Q2: Is this table similar to any table in the generic Booter database schema? YES\n",
    "    - This table looks like: blacklist\n",
    "    - The blacklist table must to have the columns: 'id','ip','note'\n",
    "    \n",
    "#### Q3: Are there modifications required? \n",
    "    - On the table name: \n",
    "        o \n",
    "    - On the column type: \n",
    "        o    \n",
    "    - On the column name:\n",
    "        o ID => id\n",
    "        o IP => ip\n",
    "    - Split columns:\n",
    "        o \n",
    "    - Add required columns:\n",
    "        o         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying table: 'blacklist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_blacklist = pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "                                delimiter=\",\",\n",
    "                                error_bad_lines=False,)\n",
    "#                        converters = {'date':timestamp2datetime})\n",
    "\n",
    "###Changing names of columns\n",
    "df_blacklist.rename(columns = {'ID': 'id',\n",
    "                               'IP': 'ip'},\n",
    "                         inplace=True)\n",
    "\n",
    "###Creating empty columns (with \"\" [for future string] or np.nan [for future float])\n",
    "# df_blacklist['generic_column_name_missing'] = \"\"\n",
    "\n",
    "###Showing some lines after adapt the table\n",
    "df_blacklist.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Table: 'fe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How this table looks like without modification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablename='fe'\n",
    "\n",
    "pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "            delimiter=\",\",\n",
    "            error_bad_lines=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Is this table different from other previous loaded table? YES\n",
    "#### Q2: Is this table similar to any table in the generic Booter database schema? NO\n",
    "    - This table looks like: 'id','ip','note','userid','type'\n",
    "    - The blacklist table must to have the columns: 'id','ip','note','userid','type'\n",
    "    \n",
    "#### Q3: Are there modifications required? \n",
    "    - On the table name: \n",
    "        o fe => friendsenemies\n",
    "    - On the column type: \n",
    "    - On the column name:\n",
    "        o ID => id\n",
    "        o userID => userid\n",
    "    - Add required columns:\n",
    "        o \n",
    "    - Split columns:\n",
    "        o "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying table: 'fe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Loading the column converting columns with predefined functions\n",
    "df_friendsenemies = pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "                                delimiter=\",\",\n",
    "                                error_bad_lines=False,)\n",
    "#                        converters = {'date':timestamp2datetime})\n",
    "\n",
    "###Changing names of columns\n",
    "df_friendsenemies.rename(columns = {'ID': 'id',\n",
    "                                    'userID': 'userid'},\n",
    "                         inplace=True)\n",
    "\n",
    "##Creating empty columns (with \"\" [for future string] or np.nan [for future float])\n",
    "# df_friendsenemies['generic_column_name_missing'] = \"\"\n",
    "\n",
    "###Showing some lines after adapt the table\n",
    "df_friendsenemies.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4. Table: 'gateway'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How this table looks like without modification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablename='gateway'\n",
    "\n",
    "pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "            delimiter=\",\",\n",
    "            error_bad_lines=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Is this table different from other previous loaded table? YES\n",
    "#### Q2: Is this table similar to any table in the generic Booter database schema? YES\n",
    "    - This table looks like: gateways\n",
    "    - The blacklist table must to have the columns: email\n",
    "    \n",
    "#### Q3: Are there modifications required? \n",
    "    - On the table name: \n",
    "        o gatway => gateways\n",
    "    - On the column type: \n",
    "        o    \n",
    "    - On the column name:\n",
    "        o  \n",
    "    - Add required columns:\n",
    "        o \n",
    "    - Split columns:\n",
    "        o "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying table: 'gateways'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Loading the column converting columns with predefined functions\n",
    "df_gateways = pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "                                delimiter=\",\",\n",
    "                                error_bad_lines=False,)\n",
    "#                        converters = {'date':timestamp2datetime})\n",
    "\n",
    "###Changing names of columns\n",
    "# df_gateways.rename(columns = {'actual1': 'generic_column_name1',\n",
    "#                                     'actual2': 'generic_column_name2'},\n",
    "#                          inplace=True)\n",
    "\n",
    "###Creating empty columns (with \"\" [for future string] or np.nan [for future float])\n",
    "# df_gateways['generic_column_name_missing'] = \"\"\n",
    "\n",
    "###Showing some lines after adapt the table\n",
    "df_gateways.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Table: 'iplogs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How this table looks like without modification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablename='iplogs'\n",
    "\n",
    "pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "            delimiter=\",\",\n",
    "            error_bad_lines=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Is this table different from other previous loaded table? YES\n",
    "#### Q2: Is this table similar to any table in the generic Booter database schema? YES\n",
    "    - This table looks like: logins\n",
    "    - The blacklist table must to have the columns: 'id','userid','username','userip','date'\n",
    "    \n",
    "#### Q3: Are there modifications required? \n",
    "    - On the table name: \n",
    "        o iplogs => logins\n",
    "    - On the column type: \n",
    "        o date => timestamp2date \n",
    "    - On the column name:\n",
    "        o ID => id\n",
    "        o userID => userid\n",
    "        o logged => userip\n",
    "    - Add required columns:\n",
    "        o username\n",
    "    - Split columns:\n",
    "        o "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying table: 'iplogins'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Loading the column converting columns with predefined functions\n",
    "df_logins = pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "                                delimiter=\",\",\n",
    "                                error_bad_lines=False,\n",
    "                       converters = {'date':timestamp2datetime})\n",
    "\n",
    "###Changing names of columns\n",
    "df_logins.rename(columns = {'ID': 'id',\n",
    "                                    'userID': 'userid',\n",
    "                           'logged':'userip'},\n",
    "                         inplace=True)\n",
    "\n",
    "###Creating empty columns (with \"\" [for future string] or np.nan [for future float])\n",
    "df_logins['username'] = \"\"\n",
    "\n",
    "###Showing some lines after adapt the table\n",
    "df_logins.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6. Table: 'loginlogs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How this table looks like without modification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablename='loginlogs'\n",
    "\n",
    "pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "            delimiter=\",\",\n",
    "            error_bad_lines=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Is this table different from other previous loaded table? NO\n",
    "#### Q2: Is this table similar to any table in the generic Booter database schema? \n",
    "    - This table looks like: \n",
    "    - The blacklist table must to have the columns: \n",
    "    \n",
    "#### Q3: Are there modifications required? \n",
    "    - On the table name: \n",
    "        o \n",
    "    - On the column type: \n",
    "        o    \n",
    "    - On the column name:\n",
    "        o  \n",
    "    - Add required columns:\n",
    "        o \n",
    "    - Split columns:\n",
    "        o "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7. Table: 'logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How this table looks like without modification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablename='logs'\n",
    "\n",
    "pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "            delimiter=\",\",\n",
    "            error_bad_lines=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Is this table different from other previous loaded table? NO\n",
    "#### Q2: Is this table similar to any table in the generic Booter database schema? \n",
    "    - This table looks like: \n",
    "    - The blacklist table must to have the columns: \n",
    "    \n",
    "#### Q3: Are there modifications required? \n",
    "    - On the table name: \n",
    "        o \n",
    "    - On the column type: \n",
    "        o    \n",
    "    - On the column name:\n",
    "        o  \n",
    "    - Add required columns:\n",
    "        o \n",
    "    - Split columns:\n",
    "        o "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.8. Table: 'messages'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How this table looks like without modification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablename='messages'\n",
    "\n",
    "pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "            delimiter=\",\",\n",
    "            error_bad_lines=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Is this table different from other previous loaded table? YES\n",
    "#### Q2: Is this table similar to any table in the generic Booter database schema? NO\n",
    "    - This table looks like: \n",
    "    - The blacklist table must to have the columns: \n",
    "    \n",
    "#### Q3: Are there modifications required? \n",
    "    - On the table name: \n",
    "        o \n",
    "    - On the column type: \n",
    "        o    \n",
    "    - On the column name:\n",
    "        o  \n",
    "    - Add required columns:\n",
    "        o \n",
    "    - Split columns:\n",
    "        o "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.9. Table: 'news'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How this table looks like without modification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablename='news'\n",
    "\n",
    "pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "            delimiter=\",\",\n",
    "            error_bad_lines=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Is this table different from other previous loaded table? YES\n",
    "#### Q2: Is this table similar to any table in the generic Booter database schema? NO\n",
    "    - This table looks like: \n",
    "    - The blacklist table must to have the columns: \n",
    "    \n",
    "#### Q3: Are there modifications required? \n",
    "    - On the table name: \n",
    "        o \n",
    "    - On the column type: \n",
    "        o    \n",
    "    - On the column name:\n",
    "        o  \n",
    "    - Add required columns:\n",
    "        o \n",
    "    - Split columns:\n",
    "        o "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.10. Table: 'payments'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How this table looks like without modification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablename='payments'\n",
    "\n",
    "pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "            delimiter=\",\",\n",
    "            error_bad_lines=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Is this table different from other previous loaded table? YES\n",
    "#### Q2: Is this table similar to any table in the generic Booter database schema? NO\n",
    "    - This table looks like: payments\n",
    "    - The blacklist table must to have the columns: 'id','userid','username','amountpaid','paymentemail','planid','tid','date'\n",
    "    \n",
    "#### Q3: Are there modifications required? \n",
    "    - On the table name: \n",
    "        o \n",
    "    - On the column type: \n",
    "        o  date => timestamptodate  \n",
    "    - On the column name:\n",
    "        o ID => id\n",
    "        o paid => amountpaid\n",
    "        o plan => planid\n",
    "        o user => userid\n",
    "        o email => paymentemail\n",
    "    - Add required columns:\n",
    "        o username\n",
    "    - Split columns:\n",
    "        o "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying table: 'payments'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Loading the column converting columns with predefined functions\n",
    "df_payments = pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "                                delimiter=\",\",\n",
    "                                error_bad_lines=False,\n",
    "                       converters = {'date':timestamp2datetime})\n",
    "\n",
    "###Changing names of columns\n",
    "df_payments.rename(columns = {'ID': 'id',\n",
    "                              'paid': 'amountpaid',\n",
    "                             'plan': 'planid',\n",
    "                             'user':'userid',\n",
    "                             'email':'paymentemail'},\n",
    "                         inplace=True)\n",
    "\n",
    "###Creating empty columns (with \"\" [for future string] or np.nan [for future float])\n",
    "df_payments['username'] = \"\"\n",
    "\n",
    "###Showing some lines after adapt the table\n",
    "df_payments.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.11. Table: 'plans'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How this table looks like without modification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablename='plans'\n",
    "\n",
    "pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "            delimiter=\",\",\n",
    "            error_bad_lines=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Is this table different from other previous loaded table? YES\n",
    "#### Q2: Is this table similar to any table in the generic Booter database schema? YES\n",
    "    - This table looks like: plans\n",
    "    - The blacklist table must to have the columns: 'planid','planname','plandescr','price','maxboottime','concurrency'\n",
    "    \n",
    "#### Q3: Are there modifications required? \n",
    "    - On the table name: \n",
    "        o \n",
    "    - On the column type: \n",
    "\n",
    "    - On the column name:\n",
    "        o ID => planid\n",
    "        o name => planname\n",
    "        o description => plandescr\n",
    "        o mbt => maxboottime\n",
    "        o concurrents => concurrency\n",
    "    - Add required columns:\n",
    "        o \n",
    "    - Split columns:\n",
    "        o "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying table: 'plans'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Loading the column converting columns with predefined functions\n",
    "df_plans = pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "                                delimiter=\",\",\n",
    "                                error_bad_lines=False,)\n",
    "#                        converters = {'date':timestamp2datetime})\n",
    "\n",
    "###Changing names of columns\n",
    "df_plans.rename(columns = {'ID': 'planid',\n",
    "                          'name': 'planname',\n",
    "                          'description':'plandescr',\n",
    "                          'mbt': 'maxboottime',\n",
    "                          'concurrents':'concurrency'},\n",
    "                         inplace=True)\n",
    "\n",
    "###Creating empty columns (with \"\" [for future string] or np.nan [for future float])\n",
    "# df_plans['generic_column_name_missing'] = \"\"\n",
    "\n",
    "###Showing some lines after adapt the table\n",
    "df_plans.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.12. Table: 'referrals'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How this table looks like without modification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablename='referrals'\n",
    "\n",
    "pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "            delimiter=\",\",\n",
    "            error_bad_lines=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Is this table different from other previous loaded table? YES\n",
    "#### Q2: Is this table similar to any table in the generic Booter database schema? NO\n",
    "    - This table looks like: \n",
    "    - The blacklist table must to have the columns: \n",
    "    \n",
    "#### Q3: Are there modifications required? \n",
    "    - On the table name: \n",
    "        o \n",
    "    - On the column type: \n",
    "        o    \n",
    "    - On the column name:\n",
    "        o  \n",
    "    - Add required columns:\n",
    "        o \n",
    "    - Split columns:\n",
    "        o "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.13. Table: 'skype_api'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How this table looks like without modification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablename='skype_api'\n",
    "\n",
    "pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "            delimiter=\",\",\n",
    "            error_bad_lines=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Is this table different from other previous loaded table? YES\n",
    "#### Q2: Is this table similar to any table in the generic Booter database schema? NO\n",
    "    - This table looks like: \n",
    "    - The blacklist table must to have the columns: \n",
    "    \n",
    "#### Q3: Are there modifications required? \n",
    "    - On the table name: \n",
    "        o \n",
    "    - On the column type: \n",
    "        o    \n",
    "    - On the column name:\n",
    "        o  \n",
    "    - Add required columns:\n",
    "        o \n",
    "    - Split columns:\n",
    "        o "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.14. Read the raw table: 'users'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How this table looks like without modification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablename='users'\n",
    "\n",
    "pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "            delimiter=\",\",\n",
    "            error_bad_lines=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Is this table different from other previous loaded table? YES\n",
    "#### Q2: Is this table similar to any table in the generic Booter database schema? NO\n",
    "    - This table looks like: users \n",
    "    - This table must to have the columns: 'userid','username','useremail','password','expire','planid'\n",
    "    \n",
    "#### Q3: Are there modifications required? \n",
    "    - On the table name: \n",
    "        o \n",
    "    - On the column type: \n",
    "        o    \n",
    "    - On the column name:\n",
    "        o ID => userid\n",
    "        o email => useremail\n",
    "        o membership => planid\n",
    "    - Add required columns:\n",
    "        o \n",
    "    - Split columns:\n",
    "        o "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying table: 'users'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Loading the column converting columns with predefined functions\n",
    "df_users = pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "                                delimiter=\",\",\n",
    "                                error_bad_lines=False,)\n",
    "#                        converters = {'date':timestamp2datetime})\n",
    "\n",
    "###Changing names of columns\n",
    "df_users.rename(columns = {'ID': 'userid',\n",
    "                            'email': 'useremail',\n",
    "                          'membership':'planid'},\n",
    "                         inplace=True)\n",
    "\n",
    "###Creating empty columns (with \"\" [for future string] or np.nan [for future float])\n",
    "# df_users['generic_column_name_missing'] = \"\"\n",
    "\n",
    "###Showing some lines after adapt the table\n",
    "df_users.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align='center'>======================================================================\n",
    "<br>Final step of the manual part\n",
    "======================================================================</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Adding missing tables accordingly to our generic Booter database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_users=pd.DataFrame(columns=['userid','username','useremail','password','expire','plan'])      \n",
    "# df_logins=pd.DataFrame(columns=['id','userid','username','userip','date'])\n",
    "df_attacks= pd.DataFrame(columns=['id','userid','username','targetip','targeturl','duration','port','type','date'])\n",
    "# df_payments=pd.DataFrame(columns=['id','userid','username','amountpaid','paymentemail','planid','tid','date'])\n",
    "df_settings=pd.DataFrame(columns=['url','sitename','siteemail'])\n",
    "# df_gateways=pd.DataFrame(columns=['email'])\n",
    "# df_friendsenemies=pd.DataFrame(columns=['id','ip','note','userid','type'])\n",
    "# df_blacklist=pd.DataFrame(columns=['id','ip','note'])\n",
    "df_webshells=pd.DataFrame(columns=['id','url','status','lastchecked','attacktype'])\n",
    "df_servers=pd.DataFrame(columns=['id','ip'])\n",
    "# df_plans=pd.DataFrame(columns=['planid','planname','plandescr','price','maxboottime','concurrency'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!<br>\n",
    "STEP 2: Data Enrichment<br>\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Depending of the size of the data, this part can take HOURS. I tested for both small and big datasets and it worked. Be pacient. This will pay-off!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Libraries needed to retrieve information from external databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import cfscrape\n",
    "from lxml import etree\n",
    "import os.path\n",
    "import random\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Function to enrich IP addresseswith AS information and country "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# THANKS TO: team-cymru.org\n",
    "def iptoasn(iplist_teamcymruformat_filelocation,outputfile):\n",
    "    cat = subprocess.Popen(['cat', iplist_teamcymruformat_filelocation], \n",
    "                            stdout=subprocess.PIPE)\n",
    "    \n",
    "    netcat = subprocess.Popen(['netcat', 'whois.cymru.com', '43'],\n",
    "                              stdin=cat.stdout,\n",
    "                              stdout=outputfile)\n",
    "    time.sleep(3) #for some reason the poll does not work! This was the way to overcome the waiting time.\n",
    "    \n",
    "    return netcat.stdout      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Function to check if an IP address was Tor node in a given moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THANKS TO: exonerator.torproject.org\n",
    "def WasTorNode(ip, date ):\n",
    "    url=\"https://exonerator.torproject.org/?ip=\"+ip+\"&timestamp=\"+date\n",
    "    scraper = cfscrape.create_scraper()\n",
    "    scraped_html=scraper.get(url).content    \n",
    "    html_tree = etree.HTML(scraped_html)\n",
    "    result=html_tree.xpath(\"//h3[@class='panel-title']/text()\") # I was looking for <h3 class=\"panel-title\">Result is positive</h3>\n",
    "    tor_node=True if result == ['Result is positive'] else False\n",
    "    return tor_node \n",
    "# 'date' MUST BE formated as: Year-month-day (2016-03-31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Discovering the middle date of the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    try:\n",
    "        middle_date=(min(df_attacks['date'])+((max(df_attacks['date'])-min(df_attacks['date']))/2))\n",
    "        raise\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        middle_date=(min(df_logins['date'])+((max(df_logins['date'])-min(df_logins['date']))/2))\n",
    "        raise\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        middle_date=(min(df_payments['date'])+((max(df_payments['date'])-min(df_payments['date']))/2))\n",
    "        raise\n",
    "    except:\n",
    "        pass\n",
    "except Exception:\n",
    "    print(\"There is no date in the entire dataset\")\n",
    "\n",
    "date_tor_check = middle_date.strftime('%Y-%m-%d')\n",
    "date_iptoasn_lookup= str(middle_date)\n",
    "print(date_tor_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Preparing to Perform IP to ASN info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_logins['middledate']=date_iptoasn_lookup\n",
    "df_attacks['middledate']=date_iptoasn_lookup\n",
    "df_friendsenemies['middledate']=date_iptoasn_lookup\n",
    "df_blacklist['middledate']=date_iptoasn_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1  Lookup IP to ASN info of table: logins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if (os.path.exists('enrichments/logins_iptoasn_out')== False):\n",
    "    logins_iptoasn_in = open('enrichments/logins_iptoasn_in', 'w+')\n",
    "    logins_iptoasn_in.write('begin\\nverbose\\n')\n",
    "    df_logins[['userip','middledate']].drop_duplicates().to_csv(logins_iptoasn_in,header=False,index=False,sep=\"\\t\") \n",
    "    logins_iptoasn_in.write('end')\n",
    "    logins_iptoasn_in.close()\n",
    "\n",
    "    logins_iptoasn_out = open('logins_iptoasn_out', 'w+')\n",
    "    iptoasn('logins_iptoasn_in',logins_iptoasn_out)\n",
    "    logins_iptoasn_out.close()\n",
    "else:\n",
    "    print(\"You already performed the lookup for this table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_logins_iptoasn = pd.read_csv('enrichments/logins_iptoasn_out',\\\n",
    "                                skiprows=1,\\\n",
    "                             delimiter=\"\\s+\\|\\s\",\\\n",
    "                             names = ['asn', 'ip', 'bgp_prefix', 'country','registry','info_date','info_request','as_name'])\n",
    "\n",
    "df_logins_extended= pd.merge(df_logins,\n",
    "                              df_logins_iptoasn,\n",
    "                              how = 'left',\n",
    "                              left_on = 'userip',\n",
    "                              right_on = 'ip')\n",
    "\n",
    "# Changing name of columns to avoid misunderstandings\n",
    "df_logins_extended.rename(columns={'asn':'srcasn', \n",
    "                                   'ip':'srcip', \n",
    "                                   'bgp_prefix':'srcbgp_prefix', \n",
    "                                   'country':'srccountry' ,\n",
    "                                   'registry':'srcregistry',\n",
    "                                   'info_date':'srcinfo_date',\n",
    "                                   'info_request':'srcinfo_request'},\n",
    "                         inplace=True)\n",
    "\n",
    "\n",
    "df_logins_extended.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.2  Lookup IP to ASN info of table: attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if (os.path.exists('enrichments/attacks_iptoasn_out')== False):\n",
    "    attacks_iptoasn_in = open('enrichments/attacks_iptoasn_in', 'w+')\n",
    "    attacks_iptoasn_in.write('begin\\nverbose\\n')\n",
    "    df_attacks[['targetip','middledate']].drop_duplicates().to_csv(attacks_iptoasn_in,header=False,index=False,sep=\"\\t\") \n",
    "    attacks_iptoasn_in.write('end')\n",
    "    attacks_iptoasn_in.close()\n",
    "\n",
    "    attacks_iptoasn_out = open('attacks_iptoasn_out', 'w+')\n",
    "    iptoasn('attacks_iptoasn_in',attacks_iptoasn_out)\n",
    "    attacks_iptoasn_out.close()\n",
    "else:\n",
    "    print(\"You already performed the lookup for this table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_attacks_iptoasn = pd.read_csv('enrichments/attacks_iptoasn_out',\\\n",
    "                                skiprows=1,\\\n",
    "                             delimiter=\"\\s+\\|\\s\",\\\n",
    "                             names = ['asn', 'ip', 'bgp_prefix', 'country','registry','info_date','info_request','as_name'])\n",
    "\n",
    "# Merging the iptoasn with the queried column\n",
    "df_attacks_extended= pd.merge(df_attacks,\n",
    "                              df_attacks_iptoasn,\n",
    "                              how = 'left',\n",
    "                              left_on = 'targetip',\n",
    "                              right_on = 'ip')\n",
    "\n",
    "# Changing name of columns to avoid misunderstandings\n",
    "df_attacks_extended.rename(columns={'asn':'targetasn', \n",
    "                                   'ip_y':'targetip', \n",
    "                                   'bgp_prefix':'targetbgp_prefix', \n",
    "                                   'country_y':'targetcountry' ,\n",
    "                                   'registry':'targetregistry',\n",
    "                                   'info_date':'targetinfo_date',\n",
    "                                   'info_request':'targetinfo_request'},\n",
    "                         inplace=True)\n",
    "df_attacks_extended.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.3  Lookup IP to ASN info of table: friendsenemies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if (os.path.exists('enrichments/friendsenemies_iptoasn_out')== False):\n",
    "    friendsenemies_iptoasn_in = open('enrichments/friendsenemies_iptoasn_in', 'w+')\n",
    "    friendsenemies_iptoasn_in.write('begin\\nverbose\\n')\n",
    "    df_friendsenemies[['ip','middledate']].drop_duplicates().to_csv(friendsenemies_iptoasn_in,header=False,index=False,sep=\"\\t\") \n",
    "    friendsenemies_iptoasn_in.write('end')\n",
    "    friendsenemies_iptoasn_in.close()\n",
    "\n",
    "    friendsenemies_iptoasn_out = open('friendsenemies_iptoasn_out', 'w+')\n",
    "    iptoasn('friendsenemies_iptoasn_in',friendsenemies_iptoasn_out)\n",
    "    friendsenemies_iptoasn_out.close()\n",
    "else:\n",
    "    print(\"You already performed the lookup for this table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_friendsenemies_iptoasn = pd.read_csv('enrichments/friendsenemies_iptoasn_out',\\\n",
    "                                skiprows=1,\\\n",
    "                             delimiter=\"\\s+\\|\\s\",\\\n",
    "                             names = ['asn', 'ip', 'bgp_prefix', 'country','registry','info_date','info_request','as_name'])\n",
    "\n",
    "# Merging the iptoasn with the queried column\n",
    "df_friendsenemies_extended= pd.merge(df_friendsenemies,\n",
    "                              df_friendsenemies_iptoasn,\n",
    "                              how = 'left',\n",
    "                              left_on = 'ip',\n",
    "                              right_on = 'ip')\n",
    "\n",
    "# Changing name of columns to avoid misunderstandings\n",
    "df_friendsenemies_extended.rename(columns={'asn':'friendsenemiesasn', \n",
    "                                   'ip':'friendsenemiesip', \n",
    "                                   'bgp_prefix':'friendsenemiesbgp_prefix', \n",
    "                                   'country':'friendsenemiescountry' ,\n",
    "                                   'registry':'friendsenemiesregistry',\n",
    "                                   'info_date':'friendsenemiesinfo_date',\n",
    "                                   'info_request':'friendsenemiesinfo_request',\n",
    "                                   'as_name': 'friendsenemiesas_name'},\n",
    "                         inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.4  Lookup IP to ASN info of table: blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if (os.path.exists('enrichments/blacklist_iptoasn_out')== False):\n",
    "    blacklist_iptoasn_in = open('enrichments/blacklist_iptoasn_in', 'w+')\n",
    "    blacklist_iptoasn_in.write('begin\\nverbose\\n')\n",
    "    df_blacklist[['ip','middledate']].drop_duplicates().to_csv(blacklist_iptoasn_in,header=False,index=False,sep=\"\\t\") \n",
    "    blacklist_iptoasn_in.write('end')\n",
    "    blacklist_iptoasn_in.close()\n",
    "\n",
    "    blacklist_iptoasn_out = open('blacklist_iptoasn_out', 'w+')\n",
    "    iptoasn('blacklist_iptoasn_in',blacklist_iptoasn_out)\n",
    "    blacklist_iptoasn_out.close()\n",
    "else:\n",
    "    print(\"You already performed the lookup for this table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_blacklist_iptoasn = pd.read_csv('enrichments/blacklist_iptoasn_out',\\\n",
    "                                skiprows=1,\\\n",
    "                             delimiter=\"\\s+\\|\\s\",\\\n",
    "                             names = ['asn', 'ip', 'bgp_prefix', 'country','registry','info_date','info_request','as_name'])\n",
    "\n",
    "# Merging the iptoasn with the queried column\n",
    "df_blacklist_extended= pd.merge(df_blacklist,\n",
    "                              df_blacklist_iptoasn,\n",
    "                              how = 'left',\n",
    "                              left_on = 'ip',\n",
    "                              right_on = 'ip')\n",
    "\n",
    "# Changing name of columns to avoid misunderstandings\n",
    "df_blacklist_extended.rename(columns={'asn':'blacklistasn', \n",
    "                                   'ip':'blacklistip', \n",
    "                                   'bgp_prefix':'blacklistbgp_prefix', \n",
    "                                   'country':'blacklistcountry' ,\n",
    "                                   'registry':'blacklistregistry',\n",
    "                                   'info_date':'blacklistinfo_date',\n",
    "                                   'info_request':'blacklistinfo_request',\n",
    "                                   'as_name': 'blacklistas_name'},\n",
    "                         inplace=True)\n",
    "df_blacklist_extended.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.1. Check if IP was a TOR node for table: login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date_tor_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if len(df_logins['userip'].unique())<1200:\n",
    "    if (os.path.exists('enrichments/logins_torcheck')== False):\n",
    "        print(\"Note: it can take a while to finish...\")\n",
    "        logins_torcheck = open('logins_torcheck', 'w+')\n",
    "        for i in df_logins['userip'].unique():\n",
    "            wasTor=WasTorNode(i,date_tor_check)\n",
    "            print(i, wasTor, file=logins_torcheck)\n",
    "    #         print(i, wasTor) #DEBUGING =D\n",
    "            time.sleep(random.randint(1,3)) #adding some random sleep time\n",
    "            logins_torcheck.flush()\n",
    "\n",
    "        logins_torcheck.close()\n",
    "    else:\n",
    "        print(\"The IP addresses from this table were already checked.\")\n",
    "else:\n",
    "        print(\"Aborted!!! It will take more than one hour to analyse!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_logins_torcheck = pd.read_csv('enrichments/logins_torcheck',\\\n",
    "                          delimiter=\"\\s+\",\\\n",
    "                          names = ['userip', 'tor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.2. Check if IP was a TOR node for table: attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if len(df_attacks['targetip'].unique())<1200:\n",
    "    if (os.path.exists('enrichments/attacks_torcheck')== False):\n",
    "        print(\"Note: it can take a while to finish...\",len(df_attacks['targetip'].unique())*3,\"seconds (in the worst case).\")\n",
    "\n",
    "        attacks_torcheck = open('attacks_torcheck', 'w+')\n",
    "\n",
    "        for i in df_attacks['targetip'].unique():\n",
    "            wasTor=WasTorNode(i,date_tor_check)\n",
    "            print(i, wasTor, file=attacks_torcheck)\n",
    "            print(i, wasTor) #DEBUGING =D\n",
    "            time.sleep(random.randint(1,3)) #adding some random sleep time\n",
    "            attacks_torcheck.flush()\n",
    "\n",
    "        attacks_torcheck.close()\n",
    "    else:\n",
    "        print(\"The IP addresses from this table were already checked.\") \n",
    "else:\n",
    "        print(\"Aborted!!! It will take more than one hour to analyse!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_attacks_torcheck = pd.read_csv('enrichments/attacks_torcheck',\\\n",
    "                          delimiter=\"\\s+\",\\\n",
    "                          names = ['targetip', 'tor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.3. Check if IP was a TOR node for table: friendsenemies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(df_friendsenemies['ip'].unique()) <1200:\n",
    "    if (os.path.exists('enrichments/friendsenemies_torcheck')== False):\n",
    "        print(\"Note: it can take a while to finish...\",len(df_friendsenemies['ip'].unique())*3,\"seconds (in the worst case).\")\n",
    "\n",
    "        friendsenemies_torcheck = open('enrichments/friendsenemies_torcheck', 'w+')\n",
    "\n",
    "        for i in df_friendsenemies['ip'].unique():\n",
    "            wasTor=WasTorNode(i,date_tor_check)\n",
    "            print(i, wasTor, file=friendsenemies_torcheck)\n",
    "        #     print(i, wasTor) #DEBUGING =D\n",
    "            time.sleep(random.randint(1,3)) #adding some random sleep time\n",
    "            friendsenemies_torcheck.flush()\n",
    "\n",
    "        friendsenemies_torcheck.close()\n",
    "    else:\n",
    "        print(\"The IP addresses from this table were already checked.\") \n",
    "else:\n",
    "        print(\"Aborted!!! It will take more than one hour to analyse!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_friendsenemies_torcheck = pd.read_csv('enrichments/friendsenemies_torcheck',\\\n",
    "                          delimiter=\"\\s+\",\\\n",
    "                          names = ['ip', 'tor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.4. Check if IP was a TOR node for table: blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(df_blacklist['ip'].unique()) < 1200:\n",
    "    if (os.path.exists('enrichments/blacklist_torcheck')== False):\n",
    "        print(\"Note: it can take a while to finish...\",len(df_blacklist['ip'].unique())*3,\"seconds (in the worst case).\")\n",
    "\n",
    "        blacklist_torcheck = open('enrichments/blacklist_torcheck', 'w+')\n",
    "\n",
    "        for i in df_blacklist['ip'].unique():\n",
    "            wasTor=WasTorNode(i,date_tor_check)\n",
    "            print(i, wasTor, file=blacklist_torcheck)\n",
    "    #         print(i, wasTor) #DEBUGING =D\n",
    "            time.sleep(random.randint(1,3)) #adding some random sleep time\n",
    "            blacklist_torcheck.flush()\n",
    "\n",
    "        blacklist_torcheck.close()\n",
    "    else:\n",
    "        print(\"The IP addresses from this table were already checked.\") \n",
    "else:\n",
    "        print(\"Aborted!!! It will take more than one hour to analyse!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_blacklist_torcheck = pd.read_csv('enrichments/blacklist_torcheck',\\\n",
    "                          delimiter=\"\\s+\",\\\n",
    "                          names = ['ip', 'tor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Relation between Attack dates and Login dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nearestDate(base_date, date_list):\n",
    "    nearest={}\n",
    "    for date in date_list:\n",
    "        if (base_date.timestamp() - date.timestamp())>=0:\n",
    "            nearest[base_date.timestamp() - date.timestamp()]= date\n",
    "    return nearest[min(nearest.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This is the TOTAL number records to be checks!!!!\n",
    "len(df_attacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_attacks['nearestlogin']=\"\"\n",
    "df_attacks['nearestlogin']=pd.to_datetime(df_attacks['nearestlogin'])\n",
    "    \n",
    "if len(df_attacks)>0 and len(df_logins)>0:\n",
    "    #When was the last login of the user that performed attacks\n",
    "    df_attacks['nearestlogin']=\"\"\n",
    "    df_attacks['nearestlogin']=pd.to_datetime(df_attacks['nearestlogin'])\n",
    "\n",
    "    for index, row in df_attacks.head(100).iterrows():\n",
    "        intermediate_df= df_logins[df_logins['username']==row['username']]\n",
    "        nearestlogindate= nearestDate(row['date'],intermediate_df['date'])\n",
    "        df_attacks.set_value(index, 'nearestlogin', nearestlogindate)\n",
    "\n",
    "        #DEBUGGING\n",
    "        if index % 1000 == 0:\n",
    "            print(index,\": +1000 records analysed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(df_attacks['nearestlogin'].value_counts()) >1:\n",
    "    df_attacks_and_logins = pd.merge(df_attacks_extended,\n",
    "                                     df_logins_extended,\n",
    "                                     how = 'left',\n",
    "                                     left_on = ['username','nearestlogin'],\n",
    "                                     right_on = ['username','date'])\n",
    "else:\n",
    "    df_attacks_and_logins=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!<br>\n",
    "STEP 3: Automatic Analysis\n",
    "<br>\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Libraries that I use to plot figs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import *\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "plt.style.use('seaborn-muted')\n",
    "# plt.rcParams['font.family'] = 'serif'\n",
    "# plt.rcParams['font.size'] = 12\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.1. Number of records per table (part of the generic Booter database schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df_users),len(df_logins),len(df_attacks),len(df_payments),len(df_settings),len(df_gateways), len(df_friendsenemies),len(df_blacklist),len(df_webshells),len(df_servers),len(df_plans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.2. Number of users, customers, attackers, and their intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(set(df_users['userid'].unique())) > 1:\n",
    "    users_set=set(df_users['userid'].unique())\n",
    "else:\n",
    "    users_set=set(df_users['username'].unique())\n",
    "    \n",
    "if len(set(df_logins['userid'].unique())) > 1:\n",
    "    userslogin_set=set(df_logins['userid'].unique())\n",
    "else:\n",
    "    userslogin_set=set(df_logins['username'].unique())\n",
    "\n",
    "if len(set(df_payments['userid'].unique())) > 1:\n",
    "    customers_set=set(df_payments['userid'].unique())\n",
    "else:\n",
    "    customers_set=set(df_payments['username'].unique())\n",
    "\n",
    "if len(set(df_attacks['userid'].unique())) > 1:\n",
    "    attackers_set=set(df_attacks['userid'].unique())\n",
    "else:\n",
    "    attackers_set=set(df_attacks['username'].unique())\n",
    "\n",
    "intersec_customers_attacker=pd.Series(list(customers_set.intersection(attackers_set)))\n",
    "intersec_users_customers=pd.Series(list(users_set.intersection(customers_set)))\n",
    "intersec_users_attackers=pd.Series(list(users_set.intersection(attackers_set)))\n",
    "intersec_users_customers_attackers=pd.Series(list(users_set.intersection(customers_set).intersection(attackers_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(users_set),len(userslogin_set),len(customers_set),len(attackers_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4,3))\n",
    "fig.suptitle('Users, Customers & Attackers', fontsize=14)\n",
    "\n",
    "ax = plt.subplot2grid((1,1), (0,0))\n",
    "\n",
    "venn=venn3(ax=ax,subsets = {'001':len(attackers_set)-len(intersec_customers_attacker)-len(intersec_users_attackers)+len(intersec_users_customers_attackers), \n",
    "                            '010':len(customers_set)-len(intersec_users_customers)-len(intersec_customers_attacker)+len(intersec_users_customers_attackers), \n",
    "                            '011':len(intersec_customers_attacker)-len(intersec_users_customers_attackers),\n",
    "                            '100':len(users_set)-len(intersec_users_customers)-len(intersec_users_attackers)+len(intersec_users_customers_attackers),\n",
    "                            '101':len(intersec_users_attackers)-len(intersec_users_customers_attackers),\n",
    "                            '110':len(intersec_users_customers)-len(intersec_users_customers_attackers),\n",
    "                            '111':len(intersec_users_customers_attackers)},\\\n",
    "          set_labels = ('Users', 'Customers','Attackers'),\\\n",
    "          alpha=1)\n",
    "try:\n",
    "    venn.get_patch_by_id('100').set_color('#404096')\n",
    "except:\n",
    "    print(\"*Users set is empty!\")  \n",
    "    \n",
    "try:\n",
    "    venn.get_patch_by_id('110').set_color('#DEA73A')\n",
    "except:\n",
    "    print(\"*Customers set is empty!\")   \n",
    "\n",
    "try:\n",
    "    venn.get_patch_by_id('001').set_color('#D92120')\n",
    "except:\n",
    "    print(\"*Attackers set is empty!\")\n",
    "\n",
    "fig.show()\n",
    "# fig.savefig('../figs/timeseries_attacks.eps', format='eps', dpi=1200) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.3. Distribution of login times per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(df_logins['userid'].value_counts()) > 0:\n",
    "    num_distinct_logins_per_user=df_logins['userid'].value_counts()\n",
    "else:\n",
    "    num_distinct_logins_per_user=df_logins['username'].value_counts()\n",
    "\n",
    "freq_distinct_logins_per_user=num_distinct_logins_per_user.value_counts()\n",
    "cum_dist_user_logins = np.linspace(0.,1.,len(num_distinct_logins_per_user))\n",
    "cdf_user_logins = pd.Series(cum_dist_user_logins, index=num_distinct_logins_per_user.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(num_distinct_logins_per_user)>0:\n",
    "    fig = plt.figure(figsize=(6, 3))\n",
    "    fig.suptitle('* Distribution of Login Times by Users:', fontsize=14, y=1.05,x=0.35)\n",
    "    \n",
    "    #Plot CDF\n",
    "    ax1 = plt.subplot2grid((1,2), (0,0))\n",
    "    ax1 = cdf_user_logins.plot(ax=ax1,lw=2, drawstyle='steps',legend=False)\n",
    "    ax1.set_xlabel(\"# logins\")\n",
    "    ax1.set_ylabel(\"CDF of Users\")\n",
    "    ax1.set_ylim(0,1.05)\n",
    "    ax1.set_title(\"\")\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    \n",
    "    ax2 = plt.subplot2grid((1,2), (0,1))\n",
    "    freq_distinct_logins_per_user.plot(ax=ax2,kind='pie', \n",
    "                                       labels=None, \n",
    "                                       legend=False,\n",
    "                                       startangle=270,\n",
    "#                                        colors=sns.color_palette()\n",
    "                                       )\n",
    "    ax2.set_ylabel(\"\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    ####\n",
    "    # For Pie Chart Better Legend box\n",
    "    porcent = (100.*freq_distinct_logins_per_user.values)/(freq_distinct_logins_per_user.values.sum())\n",
    "    labels = ['{0} - {1:1.2f} %'.\\\n",
    "              format(i,j) for i,j in zip(freq_distinct_logins_per_user.index, porcent)]\n",
    "    # To Sort Legend (defaulf: keeps the same order)\n",
    "#     patches, labels, dummy =  zip(*sorted(zip(patches, labels, freq_distinct_logins_per_user.values),\n",
    "#                                           key=lambda x: x[2],\n",
    "#                                           reverse=True))\n",
    "    legend_show_top=10\n",
    "    ax2.legend(ax2.patches[0:legend_show_top], \n",
    "               labels[0:legend_show_top], \n",
    "               bbox_to_anchor=(1.5, 1.),\n",
    "               fontsize=10)\n",
    "    ####\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    fig.savefig('figs/login_times.eps', bbox_inches='tight',format='eps', dpi=1200)\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Number of Users that Login via TOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df_logins_torcheck[df_logins_torcheck['tor']==True]['userip'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Number of Distinct IP addresses by Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(df_logins.groupby(['userid','userip']).size().reset_index()['userid'].value_counts()) >0:\n",
    "    num_distinct_ips_per_user=df_logins.groupby(['userid','userip']).size().reset_index()['userid'].value_counts()\n",
    "else:\n",
    "    num_distinct_ips_per_user=df_logins.groupby(['username','userip']).size().reset_index()['username'].value_counts()\n",
    "    \n",
    "freq_distinct_ips_per_user=num_distinct_ips_per_user.value_counts()\n",
    "cum_dist_user_ips = np.linspace(0.,1.,len(num_distinct_ips_per_user))\n",
    "cdf_user_ips = pd.Series(cum_dist_user_ips, index=num_distinct_ips_per_user.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(num_distinct_ips_per_user)>0:\n",
    "    fig = plt.figure(figsize=(6, 3))\n",
    "    fig.suptitle('* Distribution of Distinct IP address used by Users:', fontsize=14, y=1.05, x=0.4)\n",
    "\n",
    "    ax1 = plt.subplot2grid((1,2), (0,0))\n",
    "    ax1 = cdf_user_ips.plot( ax=ax1,lw=2, drawstyle='steps',legend=False)\n",
    "    ax1.set_xlabel(\"# IPs\")\n",
    "    ax1.set_ylabel(\"CDF of Users\")\n",
    "    ax1.set_ylim(0,1.05)\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    ax2 = plt.subplot2grid((1,2), (0,1))\n",
    "    freq_distinct_ips_per_user.plot(ax=ax2,kind='pie',\n",
    "                                    labels=None,legend=False,\n",
    "                                       startangle=270,\n",
    "#                                        colors=sns.color_palette()\n",
    "                                       )\n",
    "    ax2.set_ylabel(\"\")\n",
    "    ####\n",
    "    # For Pie Chart Better Legend box\n",
    "    porcent = (100.*freq_distinct_ips_per_user.values)/(freq_distinct_ips_per_user.values.sum())\n",
    "    labels = ['{0} - {1:1.2f} %'.\\\n",
    "              format(i,j) for i,j in zip(freq_distinct_ips_per_user.index, porcent)]\n",
    "    # To Sort Legend (defaulf: keeps the same order)\n",
    "#     patches, labels, dummy =  zip(*sorted(zip(patches, labels, freq_distinct_logins_per_user.values),\n",
    "#                                           key=lambda x: x[2],\n",
    "#                                           reverse=True))\n",
    "    legend_show_top=10\n",
    "    ax2.legend(ax2.patches[0:legend_show_top], \n",
    "               labels[0:legend_show_top], \n",
    "               bbox_to_anchor=(1.5, 1.),\n",
    "               fontsize=10)\n",
    "    ####\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    fig.savefig('figs/num_ips_by_users.eps', bbox_inches='tight',format='eps', dpi=1200)\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5. Number of Payments by Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(df_payments['userid'].value_counts())>0:\n",
    "    num_distinct_payments_per_user=df_payments['userid'].value_counts()\n",
    "else:\n",
    "    num_distinct_payments_per_user=df_payments['username'].value_counts()\n",
    "\n",
    "freq_distinct_payments_per_user=num_distinct_payments_per_user.value_counts()\n",
    "cum_dist_user_payments = np.linspace(0.,1.,len(num_distinct_payments_per_user))\n",
    "cdf_user_payments = pd.Series(cum_dist_user_payments, index=num_distinct_payments_per_user.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(num_distinct_payments_per_user)>0:\n",
    "    fig = plt.figure(figsize=(6, 3))\n",
    "    fig.suptitle('* Distribution of Number of Payments by Users:', fontsize=14, y=1.05, x=0.4)\n",
    "\n",
    "    ax1 = plt.subplot2grid((1,2), (0,0))\n",
    "    ax1 = cdf_user_payments.plot( ax=ax1,lw=2, drawstyle='steps',legend=False)\n",
    "    ax1.set_xlabel(\"# Payment\")\n",
    "    ax1.set_ylabel(\"CDF of Users\")\n",
    "    ax1.set_ylim(0,1.05)\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    ax2 = plt.subplot2grid((1,2), (0,1))\n",
    "    freq_distinct_payments_per_user.plot(ax=ax2,kind='pie', \n",
    "                                         labels=None,legend=False,\n",
    "                                         startangle=270)\n",
    "    ax2.set_ylabel(\"\")\n",
    "    ####\n",
    "    # For Pie Chart Better Legend box\n",
    "    porcent = (100.*freq_distinct_payments_per_user.values)/(freq_distinct_payments_per_user.values.sum())\n",
    "    labels = ['{0} - {1:1.2f} %'.\\\n",
    "              format(i,j) for i,j in zip(freq_distinct_payments_per_user.index, porcent)]\n",
    "    # To Sort Legend (defaulf: keeps the same order)\n",
    "#     patches, labels, dummy =  zip(*sorted(zip(patches, labels, freq_distinct_logins_per_user.values),\n",
    "#                                           key=lambda x: x[2],\n",
    "#                                           reverse=True))\n",
    "    legend_show_top=10\n",
    "    ax2.legend(ax2.patches[0:legend_show_top], \n",
    "               labels[0:legend_show_top], \n",
    "               bbox_to_anchor=(1.55, 1.),\n",
    "               fontsize=10)\n",
    "    ####\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    fig.savefig('figs/payments_distribution.eps', bbox_inches='tight',format='eps', dpi=1200)\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6. Total Amount of Money Earned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(df_payments['amountpaid']) >0:\n",
    "    total_earned=df_payments['amountpaid'].values.sum()\n",
    "    'US$ {:,.2f}'.format(float(total_earned))\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7. Amount of Money Paid by Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_distinct_payments_money_per_user=df_payments['amountpaid'].value_counts()\n",
    "freq_distinct_payments_money_per_user=num_distinct_payments_money_per_user.value_counts()\n",
    "cum_dist_user_payments_money = np.linspace(0.,1.,len(num_distinct_payments_money_per_user))\n",
    "cdf_user_payments_money = pd.Series(cum_dist_user_payments_money, index=num_distinct_payments_money_per_user.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(num_distinct_payments_money_per_user)>0:\n",
    "    fig = plt.figure(figsize=(6, 3))\n",
    "    fig.suptitle('* Distribution of Money Payments by Users:', fontsize=14, y=1.05, x=0.4)\n",
    "\n",
    "    ax1 = plt.subplot2grid((1,2), (0,0))\n",
    "    ax1 = cdf_user_payments_money.plot( ax=ax1,lw=2, drawstyle='steps',legend=False)\n",
    "    ax1.set_xlabel(\"$\")\n",
    "    ax1.set_ylabel(\"CDF of Users\")\n",
    "    ax1.set_ylim(0,1.05)\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    ax2 = plt.subplot2grid((1,2), (0,1))\n",
    "    freq_distinct_payments_money_per_user.plot(ax=ax2,kind='pie', \n",
    "                                               labels=None,legend=False,\n",
    "                                               startangle=270)\n",
    "    ax2.set_ylabel(\"\")\n",
    "    ####\n",
    "    # For Pie Chart Better Legend box\n",
    "    porcent = (100.*freq_distinct_payments_money_per_user.values)/(freq_distinct_payments_money_per_user.values.sum())\n",
    "    labels = ['${0:1.2f} - {1:1.2f} %'.\\\n",
    "              format(i,j) for i,j in zip(freq_distinct_payments_money_per_user.index, porcent)]\n",
    "    # To Sort Legend (defaulf: keeps the same order)\n",
    "#     patches, labels, dummy =  zip(*sorted(zip(patches, labels, freq_distinct_logins_per_user.values),\n",
    "#                                           key=lambda x: x[2],\n",
    "#                                           reverse=True))\n",
    "    legend_show_top=10\n",
    "    ax2.legend(ax2.patches[0:legend_show_top], \n",
    "               labels[0:legend_show_top], \n",
    "               bbox_to_anchor=(1.6, 1.),\n",
    "               fontsize=10)\n",
    "    ####\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    fig.savefig('figs/distribution_amount_paid.eps', bbox_inches='tight',format='eps', dpi=1200)\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5.8. Countries from where Users Access Booters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logins_country_distribution_sorted = df_logins_iptoasn['country'].value_counts(sort=True,ascending=True)\n",
    "logins_country_distribution = df_logins_iptoasn['country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(logins_country_distribution)>0:\n",
    "    fig = plt.figure(figsize=(6, 3))\n",
    "    fig.suptitle('* Users Accessing from Countries:', fontsize=14, y=1.05, x=0.28)\n",
    "\n",
    "    ax1 = plt.subplot2grid((1,2), (0,0))\n",
    "    logins_country_distribution_sorted.plot(ax=ax1,kind='barh')\n",
    "    ax1.set_ylabel(\"# access\")\n",
    "    ax1.set_xlabel(\"Country\")\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    ax2 = plt.subplot2grid((1,2), (0,1))\n",
    "    logins_country_distribution.plot(ax=ax2,kind='pie', \n",
    "                                     labels=None,legend=False,\n",
    "                                     startangle=270)\n",
    "    ax2.set_ylabel(\"\")\n",
    "    ####\n",
    "    # For Pie Chart Better Legend box\n",
    "    porcent = (100.*logins_country_distribution.values)/(logins_country_distribution.values.sum())\n",
    "    labels = ['{0} - {1:1.2f} %'.\\\n",
    "              format(i,j) for i,j in zip(logins_country_distribution.index, porcent)]\n",
    "    # To Sort Legend (defaulf: keeps the same order)\n",
    "#     patches, labels, dummy =  zip(*sorted(zip(patches, labels, freq_distinct_logins_per_user.values),\n",
    "#                                           key=lambda x: x[2],\n",
    "#                                           reverse=True))\n",
    "    legend_show_top=10\n",
    "    ax2.legend(ax2.patches[0:legend_show_top], \n",
    "               labels[0:legend_show_top], \n",
    "               bbox_to_anchor=(1.55, 1.),\n",
    "               fontsize=10)\n",
    "    ####\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    fig.savefig('figs/user_countries.eps', bbox_inches='tight',format='eps', dpi=1200)\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.9. Countries of Blacklisted IPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blacklist_country_distribution=df_blacklist_iptoasn['country'].value_counts()\n",
    "blacklist_country_distribution_sorted=df_blacklist_iptoasn['country'].value_counts(sort=True,ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(blacklist_country_distribution)>0:\n",
    "    fig = plt.figure(figsize=(6, 3))\n",
    "    fig.suptitle('* Countries of blacklisted IPs', fontsize=14, y=1.05, x=0.28)\n",
    "\n",
    "    ax1 = plt.subplot2grid((1,2), (0,0))\n",
    "    blacklist_country_distribution_sorted.plot(ax=ax1,kind='barh')\n",
    "    ax1.set_ylabel(\"# Access\")\n",
    "    ax1.set_xlabel(\"Country\")\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    ax2 = plt.subplot2grid((1,2), (0,1))\n",
    "    blacklist_country_distribution.plot(ax=ax2,kind='pie', \n",
    "                                        labels=None,legend=False,\n",
    "                                        startangle=270)\n",
    "    ax2.set_ylabel(\"\")\n",
    "    ####\n",
    "    # For Pie Chart Better Legend box\n",
    "    porcent = (100.*blacklist_country_distribution.values)/(logins_country_distribution.values.sum())\n",
    "    labels = ['{0} - {1:1.2f} %'.\\\n",
    "              format(i,j) for i,j in zip(blacklist_country_distribution.index, porcent)]\n",
    "    # To Sort Legend (defaulf: keeps the same order)\n",
    "#     patches, labels, dummy =  zip(*sorted(zip(patches, labels, freq_distinct_logins_per_user.values),\n",
    "#                                           key=lambda x: x[2],\n",
    "#                                           reverse=True))\n",
    "    legend_show_top=10\n",
    "    ax2.legend(ax2.patches[0:legend_show_top], \n",
    "               labels[0:legend_show_top], \n",
    "               bbox_to_anchor=(1.55, 1.),\n",
    "               fontsize=10)\n",
    "    ####\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    fig.savefig('figs/blacklist_countries.eps', bbox_inches='tight',format='eps', dpi=1200)\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.10. Countries of Target IPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attacks_country_distribution=df_attacks_iptoasn['country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(attacks_country_distribution)>0:\n",
    "    fig = plt.figure(figsize=(6, 3))\n",
    "    fig.suptitle('* Countries of target IPs', fontsize=14, y=1.05, x=0.28)\n",
    "\n",
    "    ax1 = plt.subplot2grid((1,2), (0,0))\n",
    "    attacks_country_distribution.plot(ax=ax1,kind='bar')\n",
    "    ax1.set_ylabel(\"# Access\")\n",
    "    ax1.set_xlabel(\"Country\")\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    ax2 = plt.subplot2grid((1,2), (0,1))\n",
    "    attacks_country_distribution.plot(ax=ax2,kind='pie', \n",
    "                                      labels=None,legend=False,\n",
    "                                      startangle=270)\n",
    "    ax2.set_ylabel(\"\")\n",
    "    ####\n",
    "    # For Pie Chart Better Legend box\n",
    "    porcent = (100.*attacks_country_distribution.values)/(attacks_country_distribution.values.sum())\n",
    "    labels = ['{0} - {1:1.2f} %'.\\\n",
    "              format(i,j) for i,j in zip(attacks_country_distribution.index, porcent)]\n",
    "    # To Sort Legend (defaulf: keeps the same order)\n",
    "#     patches, labels, dummy =  zip(*sorted(zip(patches, labels, freq_distinct_logins_per_user.values),\n",
    "#                                           key=lambda x: x[2],\n",
    "#                                           reverse=True))\n",
    "    legend_show_top=10\n",
    "    ax2.legend(ax2.patches[0:legend_show_top], \n",
    "               labels[0:legend_show_top], \n",
    "               bbox_to_anchor=(1.55, 1.),\n",
    "               fontsize=10)\n",
    "    ####\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    fig.savefig('figs/attack_countries_distribution.eps',bbox_inches='tight', format='eps', dpi=1200) \n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.12. Attacks on Same Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_attacks_on_sametarget=df_attacks['targetip'].value_counts()\n",
    "\n",
    "freq_num_attacks_on_sametarget=num_attacks_on_sametarget.value_counts()\n",
    "cum_num_attacks_on_sametarget = np.linspace(0.,1.,len(num_attacks_on_sametarget))\n",
    "cdf_num_attacks_on_sametarget = pd.Series(cum_num_attacks_on_sametarget, index=num_attacks_on_sametarget.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(num_attacks_on_sametarget)>0:\n",
    "    fig = plt.figure(figsize=(6, 3))\n",
    "    fig.suptitle('* Attacks on Same Targets:', fontsize=14, y=1.05,x=0.28)\n",
    "\n",
    "    ax1 = plt.subplot2grid((1,2), (0,0))\n",
    "    ax1 = cdf_num_attacks_on_sametarget.plot( ax=ax1,lw=2, drawstyle='steps',legend=False)\n",
    "    ax1.set_xlabel(\"# IPs\")\n",
    "    ax1.set_ylabel(\"CDF of Users\")\n",
    "    ax1.set_ylim(0,1.05)\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    ax2 = plt.subplot2grid((1,2), (0,1))\n",
    "    freq_num_attacks_on_sametarget.plot(ax=ax2,kind='pie',\n",
    "                                        labels=None,legend=False,\n",
    "                                        startangle=270)\n",
    "    ax2.set_ylabel(\"\")\n",
    "    ####\n",
    "    # For Pie Chart Better Legend box\n",
    "    porcent = (100.*freq_num_attacks_on_sametarget.values)/(freq_num_attacks_on_sametarget.values.sum())\n",
    "    labels = ['{0} - {1:1.2f} %'.\\\n",
    "              format(i,j) for i,j in zip(freq_num_attacks_on_sametarget.index, porcent)]\n",
    "    # To Sort Legend (defaulf: keeps the same order)\n",
    "#     patches, labels, dummy =  zip(*sorted(zip(patches, labels, freq_distinct_logins_per_user.values),\n",
    "#                                           key=lambda x: x[2],\n",
    "#                                           reverse=True))\n",
    "    legend_show_top=10\n",
    "    ax2.legend(ax2.patches[0:legend_show_top], \n",
    "               labels[0:legend_show_top], \n",
    "               bbox_to_anchor=(1.55, 1.),\n",
    "               fontsize=10)\n",
    "    ####\n",
    "    fig.tight_layout()\n",
    "\n",
    "    fig.show()\n",
    "    fig.savefig('figs/attacks_on_same_target.eps', bbox_inches='tight',format='eps', dpi=1200)\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.11. Attacks per day (timeseries) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(df_attacks)>0:\n",
    "    attack_timeseries=df_attacks.set_index(['date']).groupby(pd.TimeGrouper(freq='D')).agg(['count'])['action']\n",
    "    attack_mean_perday=attack_timeseries.mean()\n",
    "    attack_median_perday=attack_timeseries.median()\n",
    "else:\n",
    "    attack_timeseries=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(attack_timeseries)>0:\n",
    "    fig = plt.figure(figsize=(6,3))\n",
    "\n",
    "    ax1 = plt.subplot2grid((1,1), (0,0), rowspan=2)\n",
    "    attack_timeseries.plot(ax=ax1,\n",
    "                           lw=2,\n",
    "                           legend=False,\n",
    "    #                        style='--'\n",
    "                          )\n",
    "\n",
    "    # X and Y Labels and Ticks\n",
    "    ax1.set_xlabel(\"Time (bin=day)\")\n",
    "    ax1.set_ylabel(\"# Attacks\")\n",
    "\n",
    "    ax1.annotate(str(int(attack_median_perday[0]))+' (median)', \n",
    "                 (min(df_attacks['date']), attack_median_perday),\n",
    "                 xytext=(350, -1), \n",
    "                 textcoords='offset points',\n",
    "                 color='black', \n",
    "                 arrowprops=dict(arrowstyle='-|>',\n",
    "                                 color='black'))\n",
    "    fig.savefig('figs/attacks_timeseries.eps', bbox_inches='tight',format='eps', dpi=1200)\n",
    "\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.12. Time Between Logins and Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(df_attacks_and_logins)>0:\n",
    "    print(\"redo\")\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.13. Who attack whom? (users on the country level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(df_attacks_extended)>0 and len(df_logins_extended)>0:\n",
    "    merged_attacks_logins = pd.merge(df_attacks_extended,\n",
    "                                  df_logins_extended,\n",
    "                                  how = 'left',\n",
    "                                  left_on = 'date',\n",
    "                                  right_on = 'date')[['targetcountry','srccountry']]\n",
    "\n",
    "    who_against_whom = merged_attacks_logins.groupby(['targetcountry','srccountry'])\\\n",
    "                            .size()\\\n",
    "                            .reset_index()\\\n",
    "                            .pivot('srccountry','targetcountry',0)\n",
    "else:\n",
    "    who_against_whom =\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(who_against_whom)>0:\n",
    "    fig = plt.figure(figsize=(8,12))\n",
    "    fig.suptitle('Countries of Target IPs', fontsize=14, y=.92)\n",
    "\n",
    "    ax1 = plt.subplot2grid((1,1), (0,0))\n",
    "    sns.set()\n",
    "    sns.heatmap(who_against_whom,\n",
    "                ax=ax1,\n",
    "#                 cmap=\"YlGnBu\",\n",
    "    #             linewidths=.5,\n",
    "    #             annot=True\n",
    "                )\n",
    "\n",
    "    ax1.set_ylabel(\"Source Country\")\n",
    "    ax1.set_xlabel(\"Attack Target Country\")\n",
    "\n",
    "    fig.show()\n",
    "    fig.savefig('figs/who_attack_whom.eps', bbox_inches='tight',format='eps', dpi=1200)\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!<br>\n",
    "FIFTH PART: Query Interface<br>\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import widgets, interact, Dropdown,HTML\n",
    "from IPython.display import display,clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search_by_userid_submit(sender):\n",
    "    clear_output()\n",
    "    print(\"Searching by id =\",search_by_userid.value)\n",
    "    \n",
    "    if len(df_users[df_users['userid']== int(search_by_userid.value)])>0:\n",
    "        display(df_users[df_logins['userid']== int(search_by_userid.value)])\n",
    "    \n",
    "    if len(df_logins_extended[df_logins_extended['userid']== int(search_by_userid.value)])>0:\n",
    "        display(df_logins_extended[df_logins_extended['userid']== int(search_by_userid.value)])\n",
    "    \n",
    "    if len(df_attacks_extended[df_attacks_extended['userid']== int(search_by_userid.value)])>0:\n",
    "        display(df_attacks_extended[df_attacks_extended['userid']== int(search_by_userid.value)])\n",
    "    \n",
    "    if len(df_payments[df_payments['userid']== int(search_by_userid.value)])>0:\n",
    "        display(df_payments[df_payments['userid']== int(search_by_userid.value)])\n",
    "\n",
    "def search_by_username_submit(sender):\n",
    "    clear_output()\n",
    "    print(\"Searching by username =\",search_by_username.value,\"\\n\")\n",
    "    \n",
    "    if len(df_users[df_users['username']== search_by_username.value])>0:\n",
    "        print(\"Table df_users:\")\n",
    "        display(df_users[df_users['username']== search_by_username.value])\n",
    "    \n",
    "    if len(df_logins_extended[df_logins_extended['username']== search_by_username.value])>0:\n",
    "        print(\"Table df_logins:\")\n",
    "        display(df_logins_extended[df_logins_extended['username']== search_by_username.value])\n",
    "\n",
    "    if len(df_attacks_extended[df_attacks_extended['username']== search_by_username.value])>0:\n",
    "        print(\"Table df_attacks:\")\n",
    "        display(df_attacks_extended[df_attacks_extended['username']== search_by_username.value])\n",
    "    \n",
    "    if len(df_payments[df_payments['username']== search_by_username.value])>0:\n",
    "        print(\"Table df_payments:\")\n",
    "        display(df_payments[df_payments['username']== search_by_username.value])\n",
    "        \n",
    "def search_by_ip_submit(sender):\n",
    "    clear_output()\n",
    "    print(\"Searching by IP address =\",str(search_by_ip.value),\"\\n\")\n",
    "    \n",
    "    if len(df_logins_extended[df_logins_extended['userip']== str(search_by_ip.value)])>0:\n",
    "        print(\"Table df_logins (as ATTACKER:\")\n",
    "        display(df_logins_extended[df_logins_extended['userip']== str(search_by_ip.value)])\n",
    "     \n",
    "    if len(df_attacks_extended[df_attacks_extended['targetip']== str(search_by_ip.value)])>0:\n",
    "        print(\"Table df_attacks (as TARGET):\")\n",
    "        display(df_attacks_extended[df_attacks_extended['targetip']== str(search_by_ip.value)])  \n",
    "      \n",
    "       \n",
    "def search_by_asn_submit(sender):\n",
    "    clear_output()\n",
    "    print(\"Searching by Autonomous System Number (ASN) =\",search_by_asn.value,\"\\n\")\n",
    "    \n",
    "    if len(df_logins_extended[df_logins_extended['srcasn']== int(search_by_asn.value)])>0:\n",
    "        print(\"Table df_logins (as ATTACKER):\")\n",
    "        display(df_logins_extended[df_logins_extended['srcasn']== int(search_by_asn.value)])\n",
    "    \n",
    "    if len(df_attacks_extended[df_attacks_extended['targetasn']== str(search_by_asn.value)])>0:\n",
    "        print(\"Table df_attacks (as TARGET):\")\n",
    "        display(df_attacks_extended[df_attacks_extended['targetasn']== str(search_by_asn.value)])\n",
    "\n",
    "    if len(df_attacks_extended[df_attacks_extended['targetasn']== str(search_by_asn.value)])>0:\n",
    "        print(\"Table df_attacks (as TARGET):\")\n",
    "        display(df_attacks_extended[df_attacks_extended['targetasn']== str(search_by_asn.value)])\n",
    "\n",
    "        \n",
    "country_list=pd.read_csv('https://raw.githubusercontent.com/datasets/country-list/master/data.csv',delimiter=\",\",error_bad_lines=False)\n",
    "def search_by_country_submit():\n",
    "    country_code=country_list[country_list['Name']==search_by_country.value]['Code'].values[0]\n",
    "    clear_output()\n",
    "    print(\"Searching by Country =\",search_by_country.value,\"\\n\")\n",
    "    \n",
    "    if len(df_logins_extended[df_logins_extended['srccountry']== country_code])>0:\n",
    "        print(\"Table df_logins (as ATTACKER):\")\n",
    "        display(df_logins_extended[df_logins_extended['srccountry']== country_code])\n",
    "    \n",
    "    if len(df_attacks_extended[df_attacks_extended['targetcountry']== country_code])>0:\n",
    "        print(\"Table df_attacks (as TARGET):\")\n",
    "        display(df_attacks_extended[df_attacks_extended['targetcountry']== country_code])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(HTML('<h3>*Search by (only) one field per time:</h3>'))\n",
    "\n",
    "search_by_username = widgets.Text(description=\"username\")\n",
    "display(search_by_username)\n",
    "search_by_username.on_submit(search_by_username_submit)\n",
    "\n",
    "search_by_userid = widgets.Text(description=\"userid\")\n",
    "display(search_by_userid)\n",
    "search_by_userid.on_submit(search_by_userid_submit)\n",
    "\n",
    "search_by_ip = widgets.Text(description=\"IP\")\n",
    "display(search_by_ip)\n",
    "search_by_ip.on_submit(search_by_ip_submit)\n",
    "\n",
    "search_by_asn = widgets.Text(description=\"ASN\")\n",
    "display(search_by_asn)\n",
    "search_by_asn.on_submit(search_by_asn_submit)\n",
    "\n",
    "search_by_country = Dropdown(description=\"Country\", options=country_list['Name'].tolist())\n",
    "search_by_country.on_trait_change(search_by_country_submit, name=\"value\")\n",
    "display(search_by_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
