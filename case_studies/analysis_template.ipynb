{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of: \n",
    "#### File originally retrieved from: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!<br>\n",
    "FIRST PART: Pre-Analysis<br>\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Declaring the database dump file location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dumpfile='<filename>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Taking a look on the raw dump file (first 100 lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lines_to_read=100\n",
    "\n",
    "with open(dumpfile) as myfile:\n",
    "    firstlines=myfile.readlines()[0:lines_to_read] #put here the interval you want\n",
    "    for x in firstlines:\n",
    "        print(x.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!<br>\n",
    "SECOND PART: Adapt to Our Generic Database Schema<br>\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0. Used Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from io import StringIO\n",
    "import re\n",
    "\n",
    "from IPython.display import display,clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Listing tables that have content inserted into the dump file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_tables_with_insert(dumpfile):\n",
    "    tables = []\n",
    "    with open(dumpfile, 'rb') as f:\n",
    "        for line in f:\n",
    "            line = line.decode(\"utf-8\").strip()\n",
    "            if line.lower().startswith('insert'):\n",
    "                table = re.findall(r'`(.*?)`', line)\n",
    "                tables.append(table[0])\n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tables=enumerate(list_tables_with_insert(dumpfile))\n",
    "\n",
    "for i, item in tables:\n",
    "    print(i+1,item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Read tables, one by one, and perform required adaptations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_inserted_table(dumpfile, target_table):\n",
    "    sio = StringIO()\n",
    "    fast_forward = True\n",
    "    with open(dumpfile, 'rb') as f:\n",
    "        for line in f:\n",
    "            line = line.decode(\"utf-8\").strip()\n",
    "            line = re.sub(\"(?!(([^']*'){2})*[^']*$)\\)\", '',line) #Step0:remove any \")\" from the content of columns\n",
    "            if line.lower().startswith('insert') and target_table in line:\n",
    "                fast_forward = False\n",
    "            if fast_forward:\n",
    "                continue\n",
    "            data = re.findall('\\([^\\)]*\\)', line) #Step1: get the content between parentesis (i.e., insert line)\n",
    "            try:\n",
    "                newline = data[0].strip('()') #Step2:remove parenthesis\n",
    "                newline=newline.replace('`','') #Step3: remove ` (usually in table names)\n",
    "                newline=re.sub(r'(?!(([^\\']*\\'){2})*[^\\']*$),', '', newline) #Step4: remove commas from the content of columns\n",
    "                newline=newline.replace('\\'','') #Step5: remove single quotes\n",
    "                newline=newline.replace(', ', ',') #Step6: remove single spaces after comma (i.e., in the beginning of a column)\n",
    "                sio.write(newline)\n",
    "                sio.write(\"\\n\")\n",
    "            except IndexError:\n",
    "                pass\n",
    "            if line.endswith(';'):\n",
    "                break\n",
    "    sio.seek(0)\n",
    "    return sio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.0. Converter functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tolowercase(text):\n",
    "    try:\n",
    "        return text.lower()\n",
    "    except AttributeError:\n",
    "        return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timestamp2datetime(timestamp):\n",
    "    try:\n",
    "        return  pd.to_datetime(timestamp,unit='s')\n",
    "    except AttributeError:\n",
    "        return timestamp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.1. Read the raw table: 'tablename'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tablename='<tablename>'\n",
    "\n",
    "pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "            delimiter=\",\",\n",
    "            error_bad_lines=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Is this table different from other previous loaded table? YES|NO\n",
    "#### Q2: Is this table similar to any table in the generic Booter database schema? YES|NO\n",
    "    - This table looks like: \n",
    "    - The blacklist table must to have the columns: \n",
    "#### Q3: Are there modifications required? YES|NO\n",
    "    - On the table name: \n",
    "        o =>\n",
    "    - On the column type: \n",
    "        o =>\n",
    "    - On the column name:\n",
    "        o => \n",
    "    - Add required columns:\n",
    "        o "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.1.X Loading the modified table: 'tablename'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_<generictable> = pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "                                delimiter=\",\",\n",
    "                                error_bad_lines=False,)\n",
    "#                        converters = {'date':timestamp2datetime})\n",
    "\n",
    "df_<generictable>.rename(columns = {'actual1': 'generic_column_name1',\n",
    "                                    'actual2': 'generic_column_name2'},\n",
    "                         inplace=True)\n",
    "\n",
    "#Creating empty columns (with \"\" [for future string] or np.nan [for future float])\n",
    "df_<generictable>['status'] = np.nan\n",
    "df_<generictable>['lastchecked'] = np.nan\n",
    "df_<generictable>['attacktype'] = np.nan\n",
    "\n",
    "df_payments.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Adding missing accordingly to our generic Booter database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_attacks= pd.DataFrame(columns=['id','userid','targeturl','targetip','duration','port','date'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!<br>\n",
    "THIRD PART: Data Enrichment<br>\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0. Needed Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "import cfscrape\n",
    "from lxml import etree\n",
    "\n",
    "import os.path\n",
    "\n",
    "import random\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Lookup functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# THANKS TO: team-cymru.org\n",
    "def iptoasn(iplist_teamcymruformat_filelocation,outputfile):\n",
    "    cat = subprocess.Popen(['cat', iplist_teamcymruformat_filelocation], \n",
    "                            stdout=subprocess.PIPE)\n",
    "    \n",
    "    netcat = subprocess.Popen(['netcat', 'whois.cymru.com', '43'],\n",
    "                              stdin=cat.stdout,\n",
    "                              stdout=outputfile)\n",
    "    time.sleep(3) #for some reason the poll does not work! This was the way to overcome the waiting time.\n",
    "    \n",
    "    return netcat.stdout      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THANKS TO: exonerator.torproject.org\n",
    "def WasTorNode(ip, date ):\n",
    "    url=\"https://exonerator.torproject.org/?ip=\"+ip+\"&timestamp=\"+date\n",
    "    scraper = cfscrape.create_scraper()\n",
    "    scraped_html=scraper.get(url).content    \n",
    "    html_tree = etree.HTML(scraped_html)\n",
    "    result=html_tree.xpath(\"//h3[@class='panel-title']/text()\") # I was looking for <h3 class=\"panel-title\">Result is positive</h3>\n",
    "    tor_node=True if result == ['Result is positive'] else False\n",
    "    return tor_node \n",
    "# 'date' MUST BE formated as: Year-month-day (2016-03-31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Discovering the middle date of the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    try:\n",
    "        middle_date=(min(df_attacks['date'])+((max(df_attacks['date'])-min(df_attacks['date']))/2))\n",
    "        raise\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        middle_date=(min(df_logins['date'])+((max(df_logins['date'])-min(df_logins['date']))/2))\n",
    "        raise\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        middle_date=(min(df_payments['date'])+((max(df_payments['date'])-min(df_payments['date']))/2))\n",
    "        raise\n",
    "    except:\n",
    "        pass\n",
    "except Exception:\n",
    "    print(\"There is no date in the entire dataset\")\n",
    "\n",
    "date_tor_check = middle_date.strftime('%Y-%m-%d')\n",
    "date_iptoasn_lookup= str(middle_date)\n",
    "print(date_tor_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Preparing to Perform IP to ASN info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_logins['middledate']=date_iptoasn_lookup\n",
    "df_attacks['middledate']=date_iptoasn_lookup\n",
    "df_friendsenemies['middledate']=date_iptoasn_lookup\n",
    "df_blacklist['middledate']=date_iptoasn_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.1  Lookup IP to ASN info of table: logins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if (os.path.exists('logins_iptoasn_out')== False):\n",
    "    logins_iptoasn_in = open('logins_iptoasn_in', 'w+')\n",
    "    logins_iptoasn_in.write('begin\\nverbose\\n')\n",
    "    df_logins[['userip','middledate']].drop_duplicates().to_csv(logins_iptoasn_in,header=False,index=False,sep=\"\\t\") \n",
    "    logins_iptoasn_in.write('end')\n",
    "    logins_iptoasn_in.close()\n",
    "\n",
    "    logins_iptoasn_out = open('logins_iptoasn_out', 'w+')\n",
    "    iptoasn('logins_iptoasn_in',logins_iptoasn_out)\n",
    "    logins_iptoasn_out.close()\n",
    "else:\n",
    "    print(\"You already performed the lookup for this table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_logins_iptoasn = pd.read_csv('logins_iptoasn_out',\\\n",
    "                                skiprows=1,\\\n",
    "                             delimiter=\"\\s+\\|\\s\",\\\n",
    "                             names = ['asn', 'ip', 'bgp_prefix', 'country','registry','info_date','info_request','as_name'])\n",
    "df_logins_iptoasn.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.2  Lookup IP to ASN info of table: attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if (os.path.exists('attacks_iptoasn_out')== False):\n",
    "    attacks_iptoasn_in = open('attacks_iptoasn_in', 'w+')\n",
    "    attacks_iptoasn_in.write('begin\\nverbose\\n')\n",
    "    df_attacks[['targetip','middledate']].drop_duplicates().to_csv(attacks_iptoasn_in,header=False,index=False,sep=\"\\t\") \n",
    "    attacks_iptoasn_in.write('end')\n",
    "    attacks_iptoasn_in.close()\n",
    "\n",
    "    attacks_iptoasn_out = open('attacks_iptoasn_out', 'w+')\n",
    "    iptoasn('attacks_iptoasn_in',attacks_iptoasn_out)\n",
    "    attacks_iptoasn_out.close()\n",
    "else:\n",
    "    print(\"You already performed the lookup for this table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_attacks_iptoasn = pd.read_csv('attacks_iptoasn_out',\\\n",
    "                                skiprows=1,\\\n",
    "                             delimiter=\"\\s+\\|\\s\",\\\n",
    "                             names = ['asn', 'ip', 'bgp_prefix', 'country','registry','info_date','info_request','as_name'])\n",
    "df_attacks_iptoasn.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.3  Lookup IP to ASN info of table: friendsenemies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if (os.path.exists('friendsenemies_iptoasn_out')== False):\n",
    "    friendsenemies_iptoasn_in = open('friendsenemies_iptoasn_in', 'w+')\n",
    "    friendsenemies_iptoasn_in.write('begin\\nverbose\\n')\n",
    "    df_friendsenemies[['ip','middledate']].drop_duplicates().to_csv(friendsenemies_iptoasn_in,header=False,index=False,sep=\"\\t\") \n",
    "    friendsenemies_iptoasn_in.write('end')\n",
    "    friendsenemies_iptoasn_in.close()\n",
    "\n",
    "    friendsenemies_iptoasn_out = open('friendsenemies_iptoasn_out', 'w+')\n",
    "    iptoasn('friendsenemies_iptoasn_in',friendsenemies_iptoasn_out)\n",
    "    friendsenemies_iptoasn_out.close()\n",
    "else:\n",
    "    print(\"You already performed the lookup for this table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_friendsenemies_iptoasn = pd.read_csv('logins_iptoasn_out',\\\n",
    "                                skiprows=1,\\\n",
    "                             delimiter=\"\\s+\\|\\s\",\\\n",
    "                             names = ['asn', 'ip', 'bgp_prefix', 'country','registry','info_date','info_request','as_name'])\n",
    "df_friendsenemies_iptoasn.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.4  Lookup IP to ASN info of table: blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if (os.path.exists('blacklist_iptoasn_out')== False):\n",
    "    blacklist_iptoasn_in = open('blacklist_iptoasn_in', 'w+')\n",
    "    blacklist_iptoasn_in.write('begin\\nverbose\\n')\n",
    "    df_blacklist[['ip','middledate']].drop_duplicates().to_csv(blacklist_iptoasn_in,header=False,index=False,sep=\"\\t\") \n",
    "    blacklist_iptoasn_in.write('end')\n",
    "    blacklist_iptoasn_in.close()\n",
    "\n",
    "    blacklist_iptoasn_out = open('blacklist_iptoasn_out', 'w+')\n",
    "    iptoasn('blacklist_iptoasn_in',blacklist_iptoasn_out)\n",
    "    blacklist_iptoasn_out.close()\n",
    "else:\n",
    "    print(\"You already performed the lookup for this table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_blacklist_iptoasn = pd.read_csv('blacklist_iptoasn_out',\\\n",
    "                                skiprows=1,\\\n",
    "                             delimiter=\"\\s+\\|\\s\",\\\n",
    "                             names = ['asn', 'ip', 'bgp_prefix', 'country','registry','info_date','info_request','as_name'])\n",
    "df_blacklist_iptoasn.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5.1. Check if IP was a TOR node for table: login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date_tor_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if (os.path.exists('logins_torcheck')== False):\n",
    "    print(\"Note: it can take a while to finish...\")\n",
    "    logins_torcheck = open('logins_torcheck', 'w+')\n",
    "    for i in df_logins['userip'].unique():\n",
    "        wasTor=WasTorNode(i,date_tor_check)\n",
    "        print(i, wasTor, file=logins_torcheck)\n",
    "#         print(i, wasTor) #DEBUGING =D\n",
    "        time.sleep(random.randint(1,3)) #adding some random sleep time\n",
    "        logins_torcheck.flush()\n",
    "\n",
    "    logins_torcheck.close()\n",
    "else:\n",
    "    print(\"The IP addresses from this table were already checked.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_logins_torcheck = pd.read_csv('logins_torcheck',\\\n",
    "                          delimiter=\"\\s+\",\\\n",
    "                          names = ['userip', 'tor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5.2. Check if IP was a TOR node for table: attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if (os.path.exists('attacks_torcheck')== False):\n",
    "    print(\"Note: it can take a while to finish...\")\n",
    "\n",
    "    attacks_torcheck = open('attacks_torcheck', 'w+')\n",
    "\n",
    "    for i in df_attacks['targetip'].unique():\n",
    "        wasTor=WasTorNode(i,date_tor_check)\n",
    "        print(i, wasTor, file=attacks_torcheck)\n",
    "    #     print(i, wasTor) #DEBUGING =D\n",
    "        time.sleep(random.randint(1,3)) #adding some random sleep time\n",
    "        attacks_torcheck.flush()\n",
    "\n",
    "    attacks_torcheck.close()\n",
    "else:\n",
    "    print(\"The IP addresses from this table were already checked.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_attacks_torcheck = pd.read_csv('attacks_torcheck',\\\n",
    "                          delimiter=\"\\s+\",\\\n",
    "                          names = ['targetip', 'tor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5.3. Check if IP was a TOR node for table: friendsenemies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if (os.path.exists('friendsenemies_torcheck')== False):\n",
    "    print(\"Note: it can take a while to finish...\")\n",
    "\n",
    "    friendsenemies_torcheck = open('friendsenemies_torcheck', 'w+')\n",
    "\n",
    "    for i in df_friendsenemies['ip'].unique():\n",
    "        wasTor=WasTorNode(i,date_tor_check)\n",
    "        print(i, wasTor, file=friendsenemies_torcheck)\n",
    "    #     print(i, wasTor) #DEBUGING =D\n",
    "        time.sleep(random.randint(1,3)) #adding some random sleep time\n",
    "        friendsenemies_torcheck.flush()\n",
    "\n",
    "    friendsenemies_torcheck.close()\n",
    "else:\n",
    "    print(\"The IP addresses from this table were already checked.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_friendsenemies_torcheck = pd.read_csv('friendsenemies_torcheck',\\\n",
    "                          delimiter=\"\\s+\",\\\n",
    "                          names = ['ip', 'tor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5.4. Check if IP was a TOR node for table: blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if (os.path.exists('blacklist_torcheck')== False):\n",
    "    print(\"Note: it can take a while to finish...\")\n",
    "    \n",
    "    blacklist_torcheck = open('blacklist_torcheck', 'w+')\n",
    "\n",
    "    for i in df_blacklist['ip'].unique():\n",
    "        wasTor=WasTorNode(i,date_tor_check)\n",
    "        print(i, wasTor, file=blacklist_torcheck)\n",
    "#         print(i, wasTor) #DEBUGING =D\n",
    "        time.sleep(random.randint(1,3)) #adding some random sleep time\n",
    "        blacklist_torcheck.flush()\n",
    "\n",
    "    blacklist_torcheck.close()\n",
    "else:\n",
    "    print(\"The IP addresses from this table were already checked.\")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_blacklist_torcheck = pd.read_csv('blacklist_torcheck',\\\n",
    "                          delimiter=\"\\s+\",\\\n",
    "                          names = ['ip', 'tor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!<br>\n",
    "FOURTH PART: Automatic Analysis\n",
    "<br>\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0. Used Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import *\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('seaborn-muted')\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Number of users, customers, attackers, and their intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users_set=set(df_users['userid'].unique())\n",
    "customers_set=set(df_payments['userid'].unique())\n",
    "attackers_set=set(df_attacks['userid'].unique())\n",
    "\n",
    "intersec_customers_attacker=pd.Series(list(customers_set.intersection(attackers_set)))\n",
    "intersec_users_customers=pd.Series(list(users_set.intersection(customers_set)))\n",
    "intersec_users_attackers=pd.Series(list(users_set.intersection(attackers_set)))\n",
    "\n",
    "intersec_users_customers_attackers=pd.Series(list(users_set.intersection(customers_set).intersection(attackers_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4,3))\n",
    "fig.suptitle('Users, Customers & Attackers', fontsize=16)\n",
    "\n",
    "ax = plt.subplot2grid((1,1), (0,0))\n",
    "\n",
    "venn=venn3(ax=ax,subsets = {'001':len(attackers_set)-len(intersec_customers_attacker)-len(intersec_users_attackers)+len(intersec_users_customers_attackers), \n",
    "                            '010':len(customers_set)-len(intersec_users_customers)-len(intersec_customers_attacker)+len(intersec_users_customers_attackers), \n",
    "                            '011':len(intersec_customers_attacker)-len(intersec_users_customers_attackers),\n",
    "                            '100':len(users_set)-len(intersec_users_customers)-len(intersec_users_attackers)+len(intersec_users_customers_attackers),\n",
    "                            '101':len(intersec_users_attackers)-len(intersec_users_customers_attackers),\n",
    "                            '110':len(intersec_users_customers)-len(intersec_users_customers_attackers),\n",
    "                            '111':len(intersec_users_customers_attackers)},\\\n",
    "          set_labels = ('Users', 'Customers','Attackers'),\\\n",
    "          alpha=1)\n",
    "try:\n",
    "    venn.get_patch_by_id('100').set_color('#404096')\n",
    "except:\n",
    "    print(\"*Users set is empty!\")  \n",
    "    \n",
    "try:\n",
    "    venn.get_patch_by_id('110').set_color('#DEA73A')\n",
    "except:\n",
    "    print(\"*Customers set is empty!\")   \n",
    "\n",
    "try:\n",
    "    venn.get_patch_by_id('001').set_color('#D92120')\n",
    "except:\n",
    "    print(\"*Attackers set is empty!\")\n",
    "\n",
    "fig.show()\n",
    "# fig.savefig('../figs/timeseries_attacks.eps', format='eps', dpi=1200) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Distribution of login times per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_distinct_logins_per_user=df_logins['userid'].value_counts()\n",
    "freq_distinct_logins_per_user=num_distinct_logins_per_user.value_counts()\n",
    "cum_dist_user_logins = np.linspace(0.,1.,len(num_distinct_logins_per_user))\n",
    "cdf_user_logins = pd.Series(cum_dist_user_logins, index=num_distinct_logins_per_user.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(num_distinct_logins_per_user)>0:\n",
    "    fig = plt.figure()\n",
    "    fig = plt.figure(figsize=(7, 3))\n",
    "    fig.suptitle('Distribution of Login Times by Users', fontsize=16)\n",
    "\n",
    "    #Plot CDF\n",
    "    ax1 = plt.subplot2grid((1,2), (0,0))\n",
    "    ax1 = cdf_user_logins.plot(ax=ax1,lw=2, drawstyle='steps',legend=False)\n",
    "    ax1.set_xlabel(\"# logins\")\n",
    "    ax1.set_ylabel(\"CDF of Users\")\n",
    "    ax1.set_ylim(0,1.05)\n",
    "\n",
    "    ax2 = plt.subplot2grid((1,2), (0,1))\n",
    "    freq_distinct_logins_per_user.plot(ax=ax2,kind='pie', autopct='%1.1f%%', startangle=270, fontsize=10)\n",
    "    ax2.set_ylabel(\"\")\n",
    "\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")\n",
    "# fig.savefig('../figs/cdf_users_attacks.eps', bbox_inches='tight',format='eps', dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Distribution of IP addresses per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_distinct_ips_per_user=df_logins.groupby(['userid','userip']).size().reset_index()['userid'].value_counts()\n",
    "freq_distinct_ips_per_user=num_distinct_ips_per_user.value_counts()\n",
    "cum_dist_user_ips = np.linspace(0.,1.,len(num_distinct_ips_per_user))\n",
    "cdf_user_ips = pd.Series(cum_dist_user_ips, index=num_distinct_ips_per_user.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(num_distinct_ips_per_user)>0:\n",
    "    fig = plt.figure()\n",
    "    fig = plt.figure(figsize=(7, 3))\n",
    "    fig.suptitle('Distribution of Distinct IP address used by Users', fontsize=16)\n",
    "\n",
    "    ax1 = plt.subplot2grid((1,2), (0,0))\n",
    "    ax1 = cdf_user_ips.plot( ax=ax1,lw=2, drawstyle='steps',legend=False)\n",
    "    ax1.set_xlabel(\"# IPs\")\n",
    "    ax1.set_ylabel(\"CDF of Users\")\n",
    "    ax1.set_ylim(0,1.05)\n",
    "\n",
    "    ax2 = plt.subplot2grid((1,2), (0,1))\n",
    "    freq_distinct_ips_per_user.plot(ax=ax2,kind='pie', autopct='%1.1f%%', startangle=270, fontsize=10)\n",
    "    ax2.set_ylabel(\"\")\n",
    "\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")\n",
    "# fig.savefig('../figs/cdf_users_attacks.eps', bbox_inches='tight',format='eps', dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Distribution of Number of Payments by Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_distinct_payments_per_user=df_payments['userid'].value_counts()\n",
    "freq_distinct_payments_per_user=num_distinct_payments_per_user.value_counts()\n",
    "cum_dist_user_payments = np.linspace(0.,1.,len(num_distinct_payments_per_user))\n",
    "cdf_user_payments = pd.Series(cum_dist_user_payments, index=num_distinct_payments_per_user.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(num_distinct_payments_per_user)>0:\n",
    "    fig = plt.figure()\n",
    "    fig = plt.figure(figsize=(7, 3))\n",
    "    fig.suptitle('Distribution of Number of Payments by Users', fontsize=16)\n",
    "\n",
    "    ax1 = plt.subplot2grid((1,2), (0,0))\n",
    "    ax1 = cdf_user_payments.plot( ax=ax1,lw=2, drawstyle='steps',legend=False)\n",
    "    ax1.set_xlabel(\"# Payment\")\n",
    "    ax1.set_ylabel(\"CDF of Users\")\n",
    "    ax1.set_ylim(0,1.05)\n",
    "\n",
    "    ax2 = plt.subplot2grid((1,2), (0,1))\n",
    "    freq_distinct_payments_per_user.plot(ax=ax2,kind='pie', autopct='%1.1f%%', startangle=270, fontsize=10)\n",
    "    ax2.set_ylabel(\"\")\n",
    "\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")\n",
    "# fig.savefig('../figs/cdf_users_attacks.eps', bbox_inches='tight',format='eps', dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5. Total amount of money earned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_earned=df_payments['amountpaid'].values.sum()\n",
    "'US$ {:,.2f}'.format(float(total_earned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6. Distribution of the money paid by Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_distinct_payments_money_per_user=df_payments['amountpaid'].value_counts()\n",
    "freq_distinct_payments_money_per_user=num_distinct_payments_money_per_user.value_counts()\n",
    "cum_dist_user_payments_money = np.linspace(0.,1.,len(num_distinct_payments_money_per_user))\n",
    "cdf_user_payments_money = pd.Series(cum_dist_user_payments_money, index=num_distinct_payments_money_per_user.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(num_distinct_payments_money_per_user)>0:\n",
    "    fig = plt.figure()\n",
    "    fig = plt.figure(figsize=(7, 3))\n",
    "    fig.suptitle('Distribution of Money Payments by Users', fontsize=16)\n",
    "\n",
    "    ax1 = plt.subplot2grid((1,2), (0,0))\n",
    "    ax1 = cdf_user_payments_money.plot( ax=ax1,lw=2, drawstyle='steps',legend=False)\n",
    "    ax1.set_xlabel(\"US$\")\n",
    "    ax1.set_ylabel(\"CDF of Users\")\n",
    "    ax1.set_ylim(0,1.05)\n",
    "\n",
    "    ax2 = plt.subplot2grid((1,2), (0,1))\n",
    "    freq_distinct_payments_money_per_user.plot(ax=ax2,kind='pie', autopct='%1.1f%%', startangle=270, fontsize=10)\n",
    "    ax2.set_ylabel(\"\")\n",
    "\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")\n",
    "# fig.savefig('../figs/cdf_users_attacks.eps', bbox_inches='tight',format='eps', dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5.7. Distribution of Countries that users access Booters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logins_country_distribution=df_logins_iptoasn['country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(logins_country_distribution)>0:\n",
    "    fig = plt.figure()\n",
    "    fig = plt.figure(figsize=(7, 3))\n",
    "    fig.suptitle('Users Accessing from Countries', fontsize=16)\n",
    "\n",
    "    ax1 = plt.subplot2grid((1,2), (0,0))\n",
    "    logins_country_distribution.plot(ax=ax1,kind='bar')\n",
    "    ax1.set_ylabel(\"\")\n",
    "\n",
    "    ax2 = plt.subplot2grid((1,2), (0,1))\n",
    "    logins_country_distribution.plot(ax=ax2,kind='pie', autopct='%1.1f%%', startangle=270, fontsize=10)\n",
    "    ax2.set_ylabel(\"\")\n",
    "\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8. Countries of blacklisted IPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blacklist_country_distribution=df_blacklist_iptoasn['country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(blacklist_country_distribution)>0:\n",
    "    fig = plt.figure()\n",
    "    fig = plt.figure(figsize=(7, 3))\n",
    "    fig.suptitle('Countries of blacklisted IPs', fontsize=16)\n",
    "\n",
    "    ax1 = plt.subplot2grid((1,2), (0,0))\n",
    "    blacklist_country_distribution.plot(ax=ax1,kind='bar')\n",
    "    ax1.set_ylabel(\"\")\n",
    "\n",
    "    ax2 = plt.subplot2grid((1,2), (0,1))\n",
    "    blacklist_country_distribution.plot(ax=ax2,kind='pie', autopct='%1.1f%%', startangle=270, fontsize=10)\n",
    "    ax2.set_ylabel(\"\")\n",
    "\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.9. Countries of target IPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attacks_country_distribution=df_attacks_iptoasn['country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(attacks_country_distribution)>0:\n",
    "    fig = plt.figure()\n",
    "    fig = plt.figure(figsize=(7, 3))\n",
    "    fig.suptitle('Countries of target IPs', fontsize=16)\n",
    "\n",
    "    ax1 = plt.subplot2grid((1,2), (0,0))\n",
    "    attacks_country_distribution.plot(ax=ax1,kind='bar')\n",
    "    ax1.set_ylabel(\"\")\n",
    "\n",
    "    ax2 = plt.subplot2grid((1,2), (0,1))\n",
    "    attacks_country_distribution.plot(ax=ax2,kind='pie', autopct='%1.1f%%', startangle=270, fontsize=10)\n",
    "    ax2.set_ylabel(\"\")\n",
    "\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!<br>\n",
    "FIFTH PART: Query Interface<br>\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import widgets, interact\n",
    "from IPython.display import display,clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5.1. Search user by name or email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def searchfor_username_submit(sender):\n",
    "    clear_output()\n",
    "    print(\"Searching for \\\"\",searchfor_username.value,\"\\\" in df_users:\")\n",
    "    display(df_users[df_users['username']== searchfor_username.value.lower()])\n",
    "\n",
    "def searchfor_useremail_submit(sender):\n",
    "    clear_output()\n",
    "    print(\"Searching for \\\"\",searchfor_useremail.value,\"\\\" in df_users:\")\n",
    "    display(df_users[df_users['useremail']== searchfor_useremail.value])\n",
    "    print(\"Searching for \\\"\",searchfor_useremail.value,\"\\\" in df_payment:\")\n",
    "    display(df_payments[df_payments['paymentemail']== searchfor_useremail.value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "searchfor_username = widgets.Text(description=\"username\")\n",
    "display(searchfor_username)\n",
    "searchfor_username.on_submit(searchfor_username_submit)\n",
    "\n",
    "searchfor_useremail = widgets.Text(description=\"email\")\n",
    "display(searchfor_useremail)\n",
    "searchfor_useremail.on_submit(searchfor_useremail_submit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Search attacks by username or userid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def searchfor_attacks_userid_submit(sender):\n",
    "    clear_output()\n",
    "    print(\"Searching for \\\"\",searchfor_attacks_userid.value,\"\\\" in df_attacks:\")\n",
    "    display(df_attacks[df_attacks['userid']== searchfor_attacks_userid.value])\n",
    "\n",
    "def searchfor_attacks_username_submit(sender):\n",
    "    clear_output()\n",
    "    print(\"Searching for \\\"\",searchfor_attacks_username.value,\"\\\" in df_attacks:\")\n",
    "    userid=df_users[df_users['username']==searchfor_attacks_username.value.lower()]['userid'].iloc[0]\n",
    "    display(df_attacks[df_attacks['userid']== userid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "searchfor_attacks_username = widgets.Text(description=\"username\",)\n",
    "display(searchfor_attacks_username)\n",
    "searchfor_attacks_username.on_submit(searchfor_attacks_username_submit)\n",
    "\n",
    "searchfor_attacks_userid = widgets.Text(description=\"userid\")\n",
    "display(searchfor_attacks_userid)\n",
    "searchfor_attacks_userid.on_submit(searchfor_attacks_userid_submit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Search IP address by userid or username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search_ips_by_userid_submit(sender):\n",
    "    clear_output()\n",
    "    print(\"Searching for \\\"\",search_ips_by_userid.value,\"\\\" in df_logins:\")\n",
    "    display(df_logins[df_logins['userid']== int(search_ips_by_userid.value)])\n",
    "\n",
    "def search_ips_by_username_submit(sender):\n",
    "    clear_output()\n",
    "    print(\"Searching for \\\"\",search_ips_by_username.value,\"\\\" in df_logins:\")\n",
    "    userid=df_users[df_users['username']==search_ips_by_username.value.lower()]['userid'].iloc[0]\n",
    "    display(df_logins[df_logins['userid']== userid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "search_ips_by_username = widgets.Text(description=\"username\")\n",
    "display(search_ips_by_username)\n",
    "search_ips_by_username.on_submit(search_ips_by_username_submit)\n",
    "\n",
    "search_ips_by_userid = widgets.Text(description=\"userid\")\n",
    "display(search_ips_by_userid)\n",
    "search_ips_by_userid.on_submit(search_ips_by_userid_submit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
