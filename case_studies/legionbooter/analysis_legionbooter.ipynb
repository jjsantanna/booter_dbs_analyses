{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of: legionbooter\n",
    "#### File originally retrieved from: http://4lz5rmnkd6f63tmm.onion/db/legion_2013_08_16.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumpfile='legion_2013_08_16.sql'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Libraries that I use to analyse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from io import StringIO\n",
    "import re\n",
    "\n",
    "from IPython.display import display,clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Functions loaded!!!!\n"
     ]
    }
   ],
   "source": [
    "%run '../additional_functions.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!<br>\n",
    "STEP 1: Adaptation to our Booter database schema<br>\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading the first 'N' (100) lines of the input Booter database file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- MySQL dump 10.13  Distrib 5.5.31, for Linux (x86_64)\n",
      "--\n",
      "-- Host: localhost    Database: legion\n",
      "-- ------------------------------------------------------\n",
      "-- Server version\t5.5.31\n",
      "\n",
      "/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;\n",
      "/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;\n",
      "/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;\n",
      "/*!40101 SET NAMES utf8 */;\n",
      "/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;\n",
      "/*!40103 SET TIME_ZONE='+00:00' */;\n",
      "/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;\n",
      "/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;\n",
      "/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;\n",
      "/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;\n",
      "\n",
      "--\n",
      "-- Table structure for table `boots_list`\n",
      "--\n",
      "\n",
      "DROP TABLE IF EXISTS `boots_list`;\n",
      "/*!40101 SET @saved_cs_client     = @@character_set_client */;\n",
      "/*!40101 SET character_set_client = utf8 */;\n",
      "CREATE TABLE `boots_list` (\n",
      "`id` int(255) NOT NULL AUTO_INCREMENT,\n",
      "`user` int(255) NOT NULL,\n",
      "`user_name` varchar(255) CHARACTER SET utf8 NOT NULL,\n",
      "`host` varchar(255) NOT NULL,\n",
      "`port` int(6) NOT NULL,\n",
      "`method` int(255) NOT NULL,\n",
      "`methodName` varchar(255) CHARACTER SET utf8 NOT NULL,\n",
      "`api_id` int(255) NOT NULL,\n",
      "`startTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,\n",
      "`bootTime` int(100) NOT NULL,\n",
      "`stopped` tinyint(1) NOT NULL,\n",
      "`created_at` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00',\n",
      "`updated_at` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00',\n",
      "PRIMARY KEY (`id`)\n",
      ") ENGINE=InnoDB AUTO_INCREMENT=38249 DEFAULT CHARSET=latin1;\n",
      "/*!40101 SET character_set_client = @saved_cs_client */;\n",
      "\n",
      "--\n",
      "-- Dumping data for table `boots_list`\n",
      "--\n",
      "\n",
      "LOCK TABLES `boots_list` WRITE;\n",
      "/*!40000 ALTER TABLE `boots_list` DISABLE KEYS */;\n",
      "INSERT INTO `boots_list` VALUES (1,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:13:14',100,1,'2013-04-04 16:13:14','2013-04-04 16:13:21'),\n",
      "(2,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:13:16',100,1,'2013-04-04 16:13:16','2013-04-04 16:13:21'),\n",
      "(3,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:13:17',100,1,'2013-04-04 16:13:17','2013-04-04 16:13:20'),\n",
      "(4,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:13:54',100,1,'2013-04-04 16:13:54','2013-04-04 16:14:55'),\n",
      "(5,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:13:57',100,1,'2013-04-04 16:13:57','2013-04-04 16:14:54'),\n",
      "(6,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:13:58',100,1,'2013-04-04 16:13:58','2013-04-04 16:14:53'),\n",
      "(7,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:14:39',100,1,'2013-04-04 16:14:39','2013-04-04 16:14:53'),\n",
      "(8,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:15:03',100,1,'2013-04-04 16:15:03','2013-04-04 16:15:13'),\n",
      "(9,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:15:04',100,1,'2013-04-04 16:15:04','2013-04-04 16:15:11'),\n",
      "(10,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:15:07',100,1,'2013-04-04 16:15:07','2013-04-04 16:15:09'),\n",
      "(11,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:16:41',100,0,'2013-04-04 16:16:41','2013-04-04 16:16:41'),\n",
      "(12,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:18:49',100,0,'2013-04-04 16:18:49','2013-04-04 16:18:49'),\n",
      "(13,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:18:52',100,0,'2013-04-04 16:18:52','2013-04-04 16:18:52'),\n",
      "(14,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:18:54',100,0,'2013-04-04 16:18:54','2013-04-04 16:18:54'),\n",
      "(15,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:19:19',100,0,'2013-04-04 16:19:19','2013-04-04 16:19:19'),\n",
      "(16,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:19:48',100,0,'2013-04-04 16:19:48','2013-04-04 16:19:48'),\n",
      "(17,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:19:54',100,0,'2013-04-04 16:19:54','2013-04-04 16:19:54'),\n",
      "(18,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:19:55',100,0,'2013-04-04 16:19:55','2013-04-04 16:19:55'),\n",
      "(19,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:20:24',100,0,'2013-04-04 16:20:24','2013-04-04 16:20:24'),\n",
      "(20,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:20:26',100,0,'2013-04-04 16:20:26','2013-04-04 16:20:26'),\n",
      "(21,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:20:27',100,0,'2013-04-04 16:20:27','2013-04-04 16:20:27'),\n",
      "(22,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:20:29',100,0,'2013-04-04 16:20:29','2013-04-04 16:20:29'),\n",
      "(23,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:23:10',100,0,'2013-04-04 16:23:10','2013-04-04 16:23:10'),\n",
      "(24,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:23:11',100,0,'2013-04-04 16:23:11','2013-04-04 16:23:11'),\n",
      "(25,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:23:12',100,0,'2013-04-04 16:23:12','2013-04-04 16:23:12'),\n",
      "(26,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:24:49',100,0,'2013-04-04 16:24:49','2013-04-04 16:24:49'),\n",
      "(27,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:24:51',100,0,'2013-04-04 16:24:51','2013-04-04 16:24:51'),\n",
      "(28,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:24:52',100,0,'2013-04-04 16:24:52','2013-04-04 16:24:52'),\n",
      "(29,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:25:18',100,0,'2013-04-04 16:25:18','2013-04-04 16:25:18'),\n",
      "(30,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:25:25',100,0,'2013-04-04 16:25:25','2013-04-04 16:25:25'),\n",
      "(31,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:25:27',100,0,'2013-04-04 16:25:27','2013-04-04 16:25:27'),\n",
      "(32,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:25:29',100,0,'2013-04-04 16:25:29','2013-04-04 16:25:29'),\n",
      "(33,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:26:17',100,0,'2013-04-04 16:26:17','2013-04-04 16:26:17'),\n",
      "(34,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:26:18',100,0,'2013-04-04 16:26:18','2013-04-04 16:26:18'),\n",
      "(35,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:26:20',100,0,'2013-04-04 16:26:20','2013-04-04 16:26:20'),\n",
      "(36,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:27:26',100,0,'2013-04-04 16:27:26','2013-04-04 16:27:26'),\n",
      "(37,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:27:43',100,0,'2013-04-04 16:27:43','2013-04-04 16:27:43'),\n",
      "(38,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:27:45',100,0,'2013-04-04 16:27:45','2013-04-04 16:27:45'),\n",
      "(39,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:27:57',100,0,'2013-04-04 16:27:57','2013-04-04 16:27:57'),\n",
      "(40,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:27:59',100,0,'2013-04-04 16:27:59','2013-04-04 16:27:59'),\n",
      "(41,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:29:20',100,0,'2013-04-04 16:29:20','2013-04-04 16:29:20'),\n",
      "(42,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:29:22',100,0,'2013-04-04 16:29:22','2013-04-04 16:29:22'),\n",
      "(43,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:29:23',100,0,'2013-04-04 16:29:23','2013-04-04 16:29:23'),\n",
      "(44,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:29:24',100,0,'2013-04-04 16:29:24','2013-04-04 16:29:24'),\n",
      "(45,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:29:25',100,0,'2013-04-04 16:29:25','2013-04-04 16:29:25'),\n",
      "(46,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:29:27',100,0,'2013-04-04 16:29:27','2013-04-04 16:29:27'),\n",
      "(47,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:31:12',100,1,'2013-04-04 16:31:12','2013-04-04 16:32:44'),\n",
      "(48,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:31:14',100,1,'2013-04-04 16:31:14','2013-04-04 16:32:43'),\n",
      "(49,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:32:46',100,1,'2013-04-04 16:32:46','2013-04-04 16:32:51'),\n",
      "(50,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:32:48',100,1,'2013-04-04 16:32:48','2013-04-04 16:32:50'),\n",
      "(51,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:37:02',100,0,'2013-04-04 16:37:02','2013-04-04 16:37:02'),\n",
      "(52,1,'Manu','localhost',21,1,'POST',0,'2013-04-04 16:37:04',100,0,'2013-04-04 16:37:04','2013-04-04 16:37:04'),\n"
     ]
    }
   ],
   "source": [
    "lines_to_read=100\n",
    "\n",
    "with open(dumpfile) as myfile:\n",
    "    firstlines=myfile.readlines()[0:lines_to_read] #put here the interval you want\n",
    "    for x in firstlines:\n",
    "        print(x.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Listing tables that have content inserted into the dump file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "boots_list               6\n",
       "friends_ennemies_list    1\n",
       "general_logs             1\n",
       "getip_logs               1\n",
       "laravel_migrations       1\n",
       "methods_apis             1\n",
       "methods_blacklist        1\n",
       "news                     1\n",
       "settings                 1\n",
       "stress_methods           1\n",
       "tickets                  1\n",
       "tickets_responses        1\n",
       "tools_blacklist          1\n",
       "users                    6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pd.Series(list_tables_with_insert(dumpfile)).value_counts()).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the list above looks like? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align='center'>======================================================================<br>\n",
    "If NOT well-formed SQL dump file then you must first do the following:<br>\n",
    "======================================================================</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Removing lines that are not part of the actual content to be analysed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Splitting and naming tables and columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align='center'>======================================================================\n",
    "Adapting EACH existing table\n",
    "======================================================================</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Table: 'boots_list'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How this table looks like without modification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>1.1</th>\n",
       "      <th>Manu</th>\n",
       "      <th>localhost</th>\n",
       "      <th>21</th>\n",
       "      <th>1.2</th>\n",
       "      <th>POST</th>\n",
       "      <th>0</th>\n",
       "      <th>2013-04-04 16:13:14</th>\n",
       "      <th>100</th>\n",
       "      <th>1.3</th>\n",
       "      <th>2013-04-04 16:13:14.1</th>\n",
       "      <th>2013-04-04 16:13:21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Manu</td>\n",
       "      <td>localhost</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>POST</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-04-04 16:13:16</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 16:13:16</td>\n",
       "      <td>2013-04-04 16:13:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Manu</td>\n",
       "      <td>localhost</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>POST</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-04-04 16:13:17</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 16:13:17</td>\n",
       "      <td>2013-04-04 16:13:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Manu</td>\n",
       "      <td>localhost</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>POST</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-04-04 16:13:54</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 16:13:54</td>\n",
       "      <td>2013-04-04 16:14:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Manu</td>\n",
       "      <td>localhost</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>POST</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-04-04 16:13:57</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 16:13:57</td>\n",
       "      <td>2013-04-04 16:14:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>Manu</td>\n",
       "      <td>localhost</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>POST</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-04-04 16:13:58</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 16:13:58</td>\n",
       "      <td>2013-04-04 16:14:53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  1.1  Manu  localhost  21  1.2  POST  0  2013-04-04 16:13:14  100  1.3  \\\n",
       "0  2    1  Manu  localhost  21    1  POST  0  2013-04-04 16:13:16  100    1   \n",
       "1  3    1  Manu  localhost  21    1  POST  0  2013-04-04 16:13:17  100    1   \n",
       "2  4    1  Manu  localhost  21    1  POST  0  2013-04-04 16:13:54  100    1   \n",
       "3  5    1  Manu  localhost  21    1  POST  0  2013-04-04 16:13:57  100    1   \n",
       "4  6    1  Manu  localhost  21    1  POST  0  2013-04-04 16:13:58  100    1   \n",
       "\n",
       "  2013-04-04 16:13:14.1  2013-04-04 16:13:21  \n",
       "0   2013-04-04 16:13:16  2013-04-04 16:13:21  \n",
       "1   2013-04-04 16:13:17  2013-04-04 16:13:20  \n",
       "2   2013-04-04 16:13:54  2013-04-04 16:14:55  \n",
       "3   2013-04-04 16:13:57  2013-04-04 16:14:54  \n",
       "4   2013-04-04 16:13:58  2013-04-04 16:14:53  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tablename='boots_list'\n",
    "\n",
    "pd.read_csv(read_inserted_table2(dumpfile, tablename),\n",
    "            delimiter=\",\",\n",
    "            error_bad_lines=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Is this table different from other previous loaded table? YES\n",
    "#### Q2: Is this table similar to any table in the generic Booter database schema? YES\n",
    "    - This table looks like: 'attacks'\n",
    "#### Q3: Are there required modifications? \n",
    "    - On the table name: \n",
    "        o boots_list -> attacks\n",
    "    - On the column:\n",
    "        o \n",
    "    - On the column name:\n",
    "        o  id\n",
    "        o \n",
    "- username\n",
    "- targetip\n",
    "- targeturl\n",
    "- duration\n",
    "- port\n",
    "- type\n",
    "- date\n",
    "    - Split columns:\n",
    "        o \n",
    "    - Add required columns:\n",
    "        o password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying table: 'users'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>username</th>\n",
       "      <th>password</th>\n",
       "      <th>useremail</th>\n",
       "      <th>rank</th>\n",
       "      <th>planid</th>\n",
       "      <th>expire</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>534</td>\n",
       "      <td>Jayyy</td>\n",
       "      <td></td>\n",
       "      <td>jtheboss95@aol.com</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>535</td>\n",
       "      <td>Kolearcher</td>\n",
       "      <td></td>\n",
       "      <td>kolearcher@aol.com</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>Brilliance</td>\n",
       "      <td></td>\n",
       "      <td>lester.c@sky.com</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>2017-10-22 01:30:40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>RikkoHF</td>\n",
       "      <td></td>\n",
       "      <td>mail@mail1122.com</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>2017-10-20 18:30:45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>known389</td>\n",
       "      <td></td>\n",
       "      <td>412413253@emailgo.de</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>2012-10-05 19:58:34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>23</td>\n",
       "      <td>Fico97</td>\n",
       "      <td></td>\n",
       "      <td>andreaficarra@hotmail.it</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2012-10-28 20:22:10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>24</td>\n",
       "      <td>poor</td>\n",
       "      <td></td>\n",
       "      <td>cmdprompt@mail.com</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-10-28 22:53:53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>25</td>\n",
       "      <td>lulwut</td>\n",
       "      <td></td>\n",
       "      <td>skyhd15@hotmail.co.uk</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-10-29 00:17:55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>26</td>\n",
       "      <td>Rayyan</td>\n",
       "      <td></td>\n",
       "      <td>rayyanasian@gmail.com</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-10-06 00:18:25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>27</td>\n",
       "      <td>wooshlager</td>\n",
       "      <td></td>\n",
       "      <td>wooshlager@hotmail.com</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2012-11-07 02:15:39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userid    username password                 useremail  rank  planid  \\\n",
       "0     534       Jayyy                 jtheboss95@aol.com     1       0   \n",
       "1     535  Kolearcher                 kolearcher@aol.com     0       0   \n",
       "2      19  Brilliance                   lester.c@sky.com     1      22   \n",
       "3      20     RikkoHF                  mail@mail1122.com     0      22   \n",
       "4      21    known389               412413253@emailgo.de     0      21   \n",
       "5      23      Fico97           andreaficarra@hotmail.it     0      11   \n",
       "6      24        poor                 cmdprompt@mail.com     0       0   \n",
       "7      25      lulwut              skyhd15@hotmail.co.uk     0       0   \n",
       "8      26      Rayyan              rayyanasian@gmail.com     0       0   \n",
       "9      27  wooshlager             wooshlager@hotmail.com     0      11   \n",
       "\n",
       "               expire  status  \n",
       "0 1970-01-01 00:00:00       0  \n",
       "1 1970-01-01 00:00:00       0  \n",
       "2 2017-10-22 01:30:40       0  \n",
       "3 2017-10-20 18:30:45       0  \n",
       "4 2012-10-05 19:58:34       0  \n",
       "5 2012-10-28 20:22:10       0  \n",
       "6 2012-10-28 22:53:53       0  \n",
       "7 2012-10-29 00:17:55       0  \n",
       "8 2012-10-06 00:18:25       0  \n",
       "9 2012-11-07 02:15:39       0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users = pd.read_csv(read_inserted_table1(dumpfile, tablename),\n",
    "                       delimiter=\",\",\n",
    "                       error_bad_lines=False)\n",
    "\n",
    "df_users['expire']=timestamp2datetime(df_users['expire'])\n",
    "\n",
    "###Changing names of columns\n",
    "df_users.rename(columns = {'ID': 'userid',\n",
    "                               'email': 'useremail',\n",
    "                              'membership': 'planid'},\n",
    "                         inplace=True)\n",
    "\n",
    "###Creating empty columns (with \"\" [for future string] or np.nan [for future float])\n",
    "df_users['password'] = \"\"\n",
    "\n",
    "###Showing some lines after adapt the table\n",
    "df_users.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align='center'>======================================================================\n",
    "<br>Final step of the manual part\n",
    "======================================================================</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Adding missing tables accordingly to our generic Booter database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_users=pd.DataFrame(columns=['userid','username','useremail','password','expire','plan'])      \n",
    "df_logins=pd.DataFrame(columns=['id','userid','username','userip','date'])\n",
    "df_attacks= pd.DataFrame(columns=['id','userid','username','targetip','targeturl','duration','port','type','date'])\n",
    "df_payments=pd.DataFrame(columns=['id','userid','username','amountpaid','paymentemail','planid','tid','date'])\n",
    "df_settings=pd.DataFrame(columns=['url','sitename','siteemail'])\n",
    "df_gateways=pd.DataFrame(columns=['email'])\n",
    "df_friendsenemies=pd.DataFrame(columns=['id','ip','note','userid','type'])\n",
    "df_blacklist=pd.DataFrame(columns=['id','ip','note'])\n",
    "df_webshells=pd.DataFrame(columns=['id','url','status','lastchecked','attacktype'])\n",
    "df_servers=pd.DataFrame(columns=['id','ip'])\n",
    "df_plans=pd.DataFrame(columns=['planid','planname','plandescr','price','maxboottime','concurrency'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!<br>\n",
    "STEP 2: Data Enrichment<br>\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Depending of the size of the data, this part can take HOURS. I tested for both small and big datasets and it worked. Be pacient. This will pay-off!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Libraries needed to retrieve information from external databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import cfscrape\n",
    "from lxml import etree\n",
    "import os.path\n",
    "import random\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Function to enrich IP addresseswith AS information and country "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THANKS TO: team-cymru.org\n",
    "def iptoasn(iplist_teamcymruformat_filelocation,outputfile):\n",
    "    cat = subprocess.Popen(['cat', iplist_teamcymruformat_filelocation], \n",
    "                            stdout=subprocess.PIPE)\n",
    "    \n",
    "    netcat = subprocess.Popen(['netcat', 'whois.cymru.com', '43'],\n",
    "                              stdin=cat.stdout,\n",
    "                              stdout=outputfile)\n",
    "    time.sleep(3) #for some reason the poll does not work! This was the way to overcome the waiting time.\n",
    "    \n",
    "    return netcat.stdout      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Function to check if an IP address was Tor node in a given moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THANKS TO: exonerator.torproject.org\n",
    "def WasTorNode(ip, date ):\n",
    "    url=\"https://exonerator.torproject.org/?ip=\"+ip+\"&timestamp=\"+date\n",
    "    scraper = cfscrape.create_scraper()\n",
    "    scraped_html=scraper.get(url).content    \n",
    "    html_tree = etree.HTML(scraped_html)\n",
    "    result=html_tree.xpath(\"//h3[@class='panel-title']/text()\") # I was looking for <h3 class=\"panel-title\">Result is positive</h3>\n",
    "    tor_node=True if result == ['Result is positive'] else False\n",
    "    return tor_node \n",
    "# 'date' MUST BE formated as: Year-month-day (2016-03-31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Discovering the middle date of the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    try:\n",
    "        middle_date=(min(df_attacks['date'])+((max(df_attacks['date'])-min(df_attacks['date']))/2))\n",
    "        raise\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        middle_date=(min(df_logins['date'])+((max(df_logins['date'])-min(df_logins['date']))/2))\n",
    "        raise\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        middle_date=(min(df_payments['date'])+((max(df_payments['date'])-min(df_payments['date']))/2))\n",
    "        raise\n",
    "    except:\n",
    "        pass\n",
    "except Exception:\n",
    "    print(\"There is no date in the entire dataset\")\n",
    "\n",
    "date_tor_check = middle_date.strftime('%Y-%m-%d')\n",
    "date_iptoasn_lookup= str(middle_date)\n",
    "print(date_tor_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Preparing to Perform IP to ASN info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_logins['middledate']=date_iptoasn_lookup\n",
    "df_attacks['middledate']=date_iptoasn_lookup\n",
    "df_friendsenemies['middledate']=date_iptoasn_lookup\n",
    "df_blacklist['middledate']=date_iptoasn_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1  Lookup IP to ASN info of table: logins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (os.path.exists('enrichments/logins_iptoasn_out')== False):\n",
    "    logins_iptoasn_in = open('enrichments/logins_iptoasn_in', 'w+')\n",
    "    logins_iptoasn_in.write('begin\\nverbose\\n')\n",
    "    df_logins[['userip','middledate']].drop_duplicates().to_csv(logins_iptoasn_in,header=False,index=False,sep=\"\\t\") \n",
    "    logins_iptoasn_in.write('end')\n",
    "    logins_iptoasn_in.close()\n",
    "\n",
    "    logins_iptoasn_out = open('logins_iptoasn_out', 'w+')\n",
    "    iptoasn('logins_iptoasn_in',logins_iptoasn_out)\n",
    "    logins_iptoasn_out.close()\n",
    "else:\n",
    "    print(\"You already performed the lookup for this table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_logins_iptoasn = pd.read_csv('enrichments/logins_iptoasn_out',\\\n",
    "                                skiprows=1,\\\n",
    "                             delimiter=\"\\s+\\|\\s\",\\\n",
    "                             names = ['asn', 'ip', 'bgp_prefix', 'country','registry','info_date','info_request','as_name'])\n",
    "\n",
    "df_logins_extended= pd.merge(df_logins,\n",
    "                              df_logins_iptoasn,\n",
    "                              how = 'left',\n",
    "                              left_on = 'userip',\n",
    "                              right_on = 'ip')\n",
    "\n",
    "# Changing name of columns to avoid misunderstandings\n",
    "df_logins_extended.rename(columns={'asn':'srcasn', \n",
    "                                   'ip':'srcip', \n",
    "                                   'bgp_prefix':'srcbgp_prefix', \n",
    "                                   'country':'srccountry' ,\n",
    "                                   'registry':'srcregistry',\n",
    "                                   'info_date':'srcinfo_date',\n",
    "                                   'info_request':'srcinfo_request'},\n",
    "                         inplace=True)\n",
    "\n",
    "\n",
    "df_logins_extended.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.2  Lookup IP to ASN info of table: attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (os.path.exists('enrichments/attacks_iptoasn_out')== False):\n",
    "    attacks_iptoasn_in = open('enrichments/attacks_iptoasn_in', 'w+')\n",
    "    attacks_iptoasn_in.write('begin\\nverbose\\n')\n",
    "    df_attacks[['targetip','middledate']].drop_duplicates().to_csv(attacks_iptoasn_in,header=False,index=False,sep=\"\\t\") \n",
    "    attacks_iptoasn_in.write('end')\n",
    "    attacks_iptoasn_in.close()\n",
    "\n",
    "    attacks_iptoasn_out = open('attacks_iptoasn_out', 'w+')\n",
    "    iptoasn('attacks_iptoasn_in',attacks_iptoasn_out)\n",
    "    attacks_iptoasn_out.close()\n",
    "else:\n",
    "    print(\"You already performed the lookup for this table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attacks_iptoasn = pd.read_csv('enrichments/attacks_iptoasn_out',\\\n",
    "                                skiprows=1,\\\n",
    "                             delimiter=\"\\s+\\|\\s\",\\\n",
    "                             names = ['asn', 'ip', 'bgp_prefix', 'country','registry','info_date','info_request','as_name'])\n",
    "\n",
    "# Merging the iptoasn with the queried column\n",
    "df_attacks_extended= pd.merge(df_attacks,\n",
    "                              df_attacks_iptoasn,\n",
    "                              how = 'left',\n",
    "                              left_on = 'targetip',\n",
    "                              right_on = 'ip')\n",
    "\n",
    "# Changing name of columns to avoid misunderstandings\n",
    "df_attacks_extended.rename(columns={'asn':'targetasn', \n",
    "                                   'ip_y':'targetip', \n",
    "                                   'bgp_prefix':'targetbgp_prefix', \n",
    "                                   'country_y':'targetcountry' ,\n",
    "                                   'registry':'targetregistry',\n",
    "                                   'info_date':'targetinfo_date',\n",
    "                                   'info_request':'targetinfo_request'},\n",
    "                         inplace=True)\n",
    "df_attacks_extended.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.3  Lookup IP to ASN info of table: friendsenemies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (os.path.exists('enrichments/friendsenemies_iptoasn_out')== False):\n",
    "    friendsenemies_iptoasn_in = open('enrichments/friendsenemies_iptoasn_in', 'w+')\n",
    "    friendsenemies_iptoasn_in.write('begin\\nverbose\\n')\n",
    "    df_friendsenemies[['ip','middledate']].drop_duplicates().to_csv(friendsenemies_iptoasn_in,header=False,index=False,sep=\"\\t\") \n",
    "    friendsenemies_iptoasn_in.write('end')\n",
    "    friendsenemies_iptoasn_in.close()\n",
    "\n",
    "    friendsenemies_iptoasn_out = open('friendsenemies_iptoasn_out', 'w+')\n",
    "    iptoasn('friendsenemies_iptoasn_in',friendsenemies_iptoasn_out)\n",
    "    friendsenemies_iptoasn_out.close()\n",
    "else:\n",
    "    print(\"You already performed the lookup for this table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_friendsenemies_iptoasn = pd.read_csv('enrichments/friendsenemies_iptoasn_out',\\\n",
    "                                skiprows=1,\\\n",
    "                             delimiter=\"\\s+\\|\\s\",\\\n",
    "                             names = ['asn', 'ip', 'bgp_prefix', 'country','registry','info_date','info_request','as_name'])\n",
    "\n",
    "# Merging the iptoasn with the queried column\n",
    "df_friendsenemies_extended= pd.merge(df_friendsenemies,\n",
    "                              df_friendsenemies_iptoasn,\n",
    "                              how = 'left',\n",
    "                              left_on = 'ip',\n",
    "                              right_on = 'ip')\n",
    "\n",
    "# Changing name of columns to avoid misunderstandings\n",
    "df_friendsenemies_extended.rename(columns={'asn':'friendsenemiesasn', \n",
    "                                   'ip':'friendsenemiesip', \n",
    "                                   'bgp_prefix':'friendsenemiesbgp_prefix', \n",
    "                                   'country':'friendsenemiescountry' ,\n",
    "                                   'registry':'friendsenemiesregistry',\n",
    "                                   'info_date':'friendsenemiesinfo_date',\n",
    "                                   'info_request':'friendsenemiesinfo_request',\n",
    "                                   'as_name': 'friendsenemiesas_name'},\n",
    "                         inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.4  Lookup IP to ASN info of table: blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (os.path.exists('enrichments/blacklist_iptoasn_out')== False):\n",
    "    blacklist_iptoasn_in = open('enrichments/blacklist_iptoasn_in', 'w+')\n",
    "    blacklist_iptoasn_in.write('begin\\nverbose\\n')\n",
    "    df_blacklist[['ip','middledate']].drop_duplicates().to_csv(blacklist_iptoasn_in,header=False,index=False,sep=\"\\t\") \n",
    "    blacklist_iptoasn_in.write('end')\n",
    "    blacklist_iptoasn_in.close()\n",
    "\n",
    "    blacklist_iptoasn_out = open('blacklist_iptoasn_out', 'w+')\n",
    "    iptoasn('blacklist_iptoasn_in',blacklist_iptoasn_out)\n",
    "    blacklist_iptoasn_out.close()\n",
    "else:\n",
    "    print(\"You already performed the lookup for this table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blacklist_iptoasn = pd.read_csv('enrichments/blacklist_iptoasn_out',\\\n",
    "                                skiprows=1,\\\n",
    "                             delimiter=\"\\s+\\|\\s\",\\\n",
    "                             names = ['asn', 'ip', 'bgp_prefix', 'country','registry','info_date','info_request','as_name'])\n",
    "\n",
    "# Merging the iptoasn with the queried column\n",
    "df_blacklist_extended= pd.merge(df_blacklist,\n",
    "                              df_blacklist_iptoasn,\n",
    "                              how = 'left',\n",
    "                              left_on = 'ip',\n",
    "                              right_on = 'ip')\n",
    "\n",
    "# Changing name of columns to avoid misunderstandings\n",
    "df_blacklist_extended.rename(columns={'asn':'blacklistasn', \n",
    "                                   'ip':'blacklistip', \n",
    "                                   'bgp_prefix':'blacklistbgp_prefix', \n",
    "                                   'country':'blacklistcountry' ,\n",
    "                                   'registry':'blacklistregistry',\n",
    "                                   'info_date':'blacklistinfo_date',\n",
    "                                   'info_request':'blacklistinfo_request',\n",
    "                                   'as_name': 'blacklistas_name'},\n",
    "                         inplace=True)\n",
    "df_blacklist_extended.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.1. Check if IP was a TOR node for table: login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_tor_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if len(df_logins['userip'].unique())<1200:\n",
    "    if (os.path.exists('enrichments/logins_torcheck')== False):\n",
    "        print(\"Note: it can take a while to finish...\")\n",
    "        logins_torcheck = open('logins_torcheck', 'w+')\n",
    "        for i in df_logins['userip'].unique():\n",
    "            wasTor=WasTorNode(i,date_tor_check)\n",
    "            print(i, wasTor, file=logins_torcheck)\n",
    "    #         print(i, wasTor) #DEBUGING =D\n",
    "            time.sleep(random.randint(1,3)) #adding some random sleep time\n",
    "            logins_torcheck.flush()\n",
    "\n",
    "        logins_torcheck.close()\n",
    "    else:\n",
    "        print(\"The IP addresses from this table were already checked.\")\n",
    "else:\n",
    "        print(\"Aborted!!! It will take more than one hour to analyse!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_logins_torcheck = pd.read_csv('enrichments/logins_torcheck',\\\n",
    "                          delimiter=\"\\s+\",\\\n",
    "                          names = ['userip', 'tor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.2. Check if IP was a TOR node for table: attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if len(df_attacks['targetip'].unique())<1200:\n",
    "    if (os.path.exists('enrichments/attacks_torcheck')== False):\n",
    "        print(\"Note: it can take a while to finish...\",len(df_attacks['targetip'].unique())*3,\"seconds (in the worst case).\")\n",
    "\n",
    "        attacks_torcheck = open('attacks_torcheck', 'w+')\n",
    "\n",
    "        for i in df_attacks['targetip'].unique():\n",
    "            wasTor=WasTorNode(i,date_tor_check)\n",
    "            print(i, wasTor, file=attacks_torcheck)\n",
    "            print(i, wasTor) #DEBUGING =D\n",
    "            time.sleep(random.randint(1,3)) #adding some random sleep time\n",
    "            attacks_torcheck.flush()\n",
    "\n",
    "        attacks_torcheck.close()\n",
    "    else:\n",
    "        print(\"The IP addresses from this table were already checked.\") \n",
    "else:\n",
    "        print(\"Aborted!!! It will take more than one hour to analyse!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_attacks_torcheck = pd.read_csv('enrichments/attacks_torcheck',\\\n",
    "                          delimiter=\"\\s+\",\\\n",
    "                          names = ['targetip', 'tor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.3. Check if IP was a TOR node for table: friendsenemies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_friendsenemies['ip'].unique()) <1200:\n",
    "    if (os.path.exists('enrichments/friendsenemies_torcheck')== False):\n",
    "        print(\"Note: it can take a while to finish...\",len(df_friendsenemies['ip'].unique())*3,\"seconds (in the worst case).\")\n",
    "\n",
    "        friendsenemies_torcheck = open('enrichments/friendsenemies_torcheck', 'w+')\n",
    "\n",
    "        for i in df_friendsenemies['ip'].unique():\n",
    "            wasTor=WasTorNode(i,date_tor_check)\n",
    "            print(i, wasTor, file=friendsenemies_torcheck)\n",
    "        #     print(i, wasTor) #DEBUGING =D\n",
    "            time.sleep(random.randint(1,3)) #adding some random sleep time\n",
    "            friendsenemies_torcheck.flush()\n",
    "\n",
    "        friendsenemies_torcheck.close()\n",
    "    else:\n",
    "        print(\"The IP addresses from this table were already checked.\") \n",
    "else:\n",
    "        print(\"Aborted!!! It will take more than one hour to analyse!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_friendsenemies_torcheck = pd.read_csv('enrichments/friendsenemies_torcheck',\\\n",
    "                          delimiter=\"\\s+\",\\\n",
    "                          names = ['ip', 'tor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.4. Check if IP was a TOR node for table: blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_blacklist['ip'].unique()) < 1200:\n",
    "    if (os.path.exists('enrichments/blacklist_torcheck')== False):\n",
    "        print(\"Note: it can take a while to finish...\",len(df_blacklist['ip'].unique())*3,\"seconds (in the worst case).\")\n",
    "\n",
    "        blacklist_torcheck = open('enrichments/blacklist_torcheck', 'w+')\n",
    "\n",
    "        for i in df_blacklist['ip'].unique():\n",
    "            wasTor=WasTorNode(i,date_tor_check)\n",
    "            print(i, wasTor, file=blacklist_torcheck)\n",
    "    #         print(i, wasTor) #DEBUGING =D\n",
    "            time.sleep(random.randint(1,3)) #adding some random sleep time\n",
    "            blacklist_torcheck.flush()\n",
    "\n",
    "        blacklist_torcheck.close()\n",
    "    else:\n",
    "        print(\"The IP addresses from this table were already checked.\") \n",
    "else:\n",
    "        print(\"Aborted!!! It will take more than one hour to analyse!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_blacklist_torcheck = pd.read_csv('enrichments/blacklist_torcheck',\\\n",
    "                          delimiter=\"\\s+\",\\\n",
    "                          names = ['ip', 'tor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Relation between Attack dates and Login dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nearestDate(base_date, date_list):\n",
    "    nearest={}\n",
    "    for date in date_list:\n",
    "        if (base_date.timestamp() - date.timestamp())>=0:\n",
    "            nearest[base_date.timestamp() - date.timestamp()]= date\n",
    "    return nearest[min(nearest.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the TOTAL number records to be checks!!!!\n",
    "len(df_attacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_attacks['nearestlogin']=\"\"\n",
    "df_attacks['nearestlogin']=pd.to_datetime(df_attacks['nearestlogin'])\n",
    "    \n",
    "if len(df_attacks)>0 and len(df_logins)>0:\n",
    "    #When was the last login of the user that performed attacks\n",
    "    df_attacks['nearestlogin']=\"\"\n",
    "    df_attacks['nearestlogin']=pd.to_datetime(df_attacks['nearestlogin'])\n",
    "\n",
    "    for index, row in df_attacks.head(100).iterrows():\n",
    "        intermediate_df= df_logins[df_logins['username']==row['username']]\n",
    "        nearestlogindate= nearestDate(row['date'],intermediate_df['date'])\n",
    "        df_attacks.set_value(index, 'nearestlogin', nearestlogindate)\n",
    "\n",
    "        #DEBUGGING\n",
    "        if index % 1000 == 0:\n",
    "            print(index,\": +1000 records analysed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_attacks['nearestlogin'].value_counts()) >1:\n",
    "    df_attacks_and_logins = pd.merge(df_attacks_extended,\n",
    "                                     df_logins_extended,\n",
    "                                     how = 'left',\n",
    "                                     left_on = ['username','nearestlogin'],\n",
    "                                     right_on = ['username','date'])\n",
    "else:\n",
    "    df_attacks_and_logins=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!<br>\n",
    "STEP 3: Automatic Analysis\n",
    "<br>\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Libraries that I use to plot figs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import *\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "plt.style.use('seaborn-muted')\n",
    "# plt.rcParams['font.family'] = 'serif'\n",
    "# plt.rcParams['font.size'] = 12\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.1. Number of records per table (part of the generic Booter database schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_users),len(df_logins),len(df_attacks),len(df_payments),len(df_settings),len(df_gateways), len(df_friendsenemies),len(df_blacklist),len(df_webshells),len(df_servers),len(df_plans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.2. Number of users, customers, attackers, and their intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(set(df_users['userid'].unique())) > 1:\n",
    "    users_set=set(df_users['userid'].unique())\n",
    "else:\n",
    "    users_set=set(df_users['username'].unique())\n",
    "    \n",
    "if len(set(df_logins['userid'].unique())) > 1:\n",
    "    userslogin_set=set(df_logins['userid'].unique())\n",
    "else:\n",
    "    userslogin_set=set(df_logins['username'].unique())\n",
    "\n",
    "if len(set(df_payments['userid'].unique())) > 1:\n",
    "    customers_set=set(df_payments['userid'].unique())\n",
    "else:\n",
    "    customers_set=set(df_payments['username'].unique())\n",
    "\n",
    "if len(set(df_attacks['userid'].unique())) > 1:\n",
    "    attackers_set=set(df_attacks['userid'].unique())\n",
    "else:\n",
    "    attackers_set=set(df_attacks['username'].unique())\n",
    "\n",
    "intersec_customers_attacker=pd.Series(list(customers_set.intersection(attackers_set)))\n",
    "intersec_users_customers=pd.Series(list(users_set.intersection(customers_set)))\n",
    "intersec_users_attackers=pd.Series(list(users_set.intersection(attackers_set)))\n",
    "intersec_users_customers_attackers=pd.Series(list(users_set.intersection(customers_set).intersection(attackers_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(users_set),len(userslogin_set),len(customers_set),len(attackers_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4,3))\n",
    "fig.suptitle('Users, Customers & Attackers', fontsize=14)\n",
    "\n",
    "ax = plt.subplot2grid((1,1), (0,0))\n",
    "\n",
    "venn=venn3(ax=ax,subsets = {'001':len(attackers_set)-len(intersec_customers_attacker)-len(intersec_users_attackers)+len(intersec_users_customers_attackers), \n",
    "                            '010':len(customers_set)-len(intersec_users_customers)-len(intersec_customers_attacker)+len(intersec_users_customers_attackers), \n",
    "                            '011':len(intersec_customers_attacker)-len(intersec_users_customers_attackers),\n",
    "                            '100':len(users_set)-len(intersec_users_customers)-len(intersec_users_attackers)+len(intersec_users_customers_attackers),\n",
    "                            '101':len(intersec_users_attackers)-len(intersec_users_customers_attackers),\n",
    "                            '110':len(intersec_users_customers)-len(intersec_users_customers_attackers),\n",
    "                            '111':len(intersec_users_customers_attackers)},\\\n",
    "          set_labels = ('Users', 'Customers','Attackers'),\\\n",
    "          alpha=1)\n",
    "try:\n",
    "    venn.get_patch_by_id('100').set_color('#404096')\n",
    "except:\n",
    "    print(\"*Users set is empty!\")  \n",
    "    \n",
    "try:\n",
    "    venn.get_patch_by_id('110').set_color('#DEA73A')\n",
    "except:\n",
    "    print(\"*Customers set is empty!\")   \n",
    "\n",
    "try:\n",
    "    venn.get_patch_by_id('001').set_color('#D92120')\n",
    "except:\n",
    "    print(\"*Attackers set is empty!\")\n",
    "\n",
    "fig.show()\n",
    "# fig.savefig('../figs/timeseries_attacks.eps', format='eps', dpi=1200) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.3. Distribution of login times per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_logins['userid'].value_counts()) > 0:\n",
    "    num_distinct_logins_per_user=df_logins['userid'].value_counts()\n",
    "else:\n",
    "    num_distinct_logins_per_user=df_logins['username'].value_counts()\n",
    "\n",
    "freq_distinct_logins_per_user=num_distinct_logins_per_user.value_counts()\n",
    "cum_dist_user_logins = np.linspace(0.,1.,len(num_distinct_logins_per_user))\n",
    "cdf_user_logins = pd.Series(cum_dist_user_logins, index=num_distinct_logins_per_user.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(num_distinct_logins_per_user)>0:\n",
    "    fig = plt.figure(figsize=(6, 3))\n",
    "    fig.suptitle('* Distribution of Login Times by Users:', fontsize=14, y=1.05,x=0.35)\n",
    "    \n",
    "    #Plot CDF\n",
    "    ax1 = plt.subplot2grid((1,2), (0,0))\n",
    "    ax1 = cdf_user_logins.plot(ax=ax1,lw=2, drawstyle='steps',legend=False)\n",
    "    ax1.set_xlabel(\"# logins\")\n",
    "    ax1.set_ylabel(\"CDF of Users\")\n",
    "    ax1.set_ylim(0,1.05)\n",
    "    ax1.set_title(\"\")\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    \n",
    "    ax2 = plt.subplot2grid((1,2), (0,1))\n",
    "    freq_distinct_logins_per_user.plot(ax=ax2,kind='pie', \n",
    "                                       labels=None, \n",
    "                                       legend=False,\n",
    "                                       startangle=270,\n",
    "#                                        colors=sns.color_palette()\n",
    "                                       )\n",
    "    ax2.set_ylabel(\"\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    ####\n",
    "    # For Pie Chart Better Legend box\n",
    "    porcent = (100.*freq_distinct_logins_per_user.values)/(freq_distinct_logins_per_user.values.sum())\n",
    "    labels = ['{0} - {1:1.2f} %'.\\\n",
    "              format(i,j) for i,j in zip(freq_distinct_logins_per_user.index, porcent)]\n",
    "    # To Sort Legend (defaulf: keeps the same order)\n",
    "#     patches, labels, dummy =  zip(*sorted(zip(patches, labels, freq_distinct_logins_per_user.values),\n",
    "#                                           key=lambda x: x[2],\n",
    "#                                           reverse=True))\n",
    "    legend_show_top=10\n",
    "    ax2.legend(ax2.patches[0:legend_show_top], \n",
    "               labels[0:legend_show_top], \n",
    "               bbox_to_anchor=(1.5, 1.),\n",
    "               fontsize=10)\n",
    "    ####\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    fig.savefig('figs/login_times.eps', bbox_inches='tight',format='eps', dpi=1200)\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Number of Users that Login via TOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_logins_torcheck[df_logins_torcheck['tor']==True]['userip'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Number of Distinct IP addresses by Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_logins.groupby(['userid','userip']).size().reset_index()['userid'].value_counts()) >0:\n",
    "    num_distinct_ips_per_user=df_logins.groupby(['userid','userip']).size().reset_index()['userid'].value_counts()\n",
    "else:\n",
    "    num_distinct_ips_per_user=df_logins.groupby(['username','userip']).size().reset_index()['username'].value_counts()\n",
    "    \n",
    "freq_distinct_ips_per_user=num_distinct_ips_per_user.value_counts()\n",
    "cum_dist_user_ips = np.linspace(0.,1.,len(num_distinct_ips_per_user))\n",
    "cdf_user_ips = pd.Series(cum_dist_user_ips, index=num_distinct_ips_per_user.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(num_distinct_ips_per_user)>0:\n",
    "    fig = plt.figure(figsize=(6, 3))\n",
    "    fig.suptitle('* Distribution of Distinct IP address used by Users:', fontsize=14, y=1.05, x=0.4)\n",
    "\n",
    "    ax1 = plt.subplot2grid((1,2), (0,0))\n",
    "    ax1 = cdf_user_ips.plot( ax=ax1,lw=2, drawstyle='steps',legend=False)\n",
    "    ax1.set_xlabel(\"# IPs\")\n",
    "    ax1.set_ylabel(\"CDF of Users\")\n",
    "    ax1.set_ylim(0,1.05)\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    ax2 = plt.subplot2grid((1,2), (0,1))\n",
    "    freq_distinct_ips_per_user.plot(ax=ax2,kind='pie',\n",
    "                                    labels=None,legend=False,\n",
    "                                       startangle=270,\n",
    "#                                        colors=sns.color_palette()\n",
    "                                       )\n",
    "    ax2.set_ylabel(\"\")\n",
    "    ####\n",
    "    # For Pie Chart Better Legend box\n",
    "    porcent = (100.*freq_distinct_ips_per_user.values)/(freq_distinct_ips_per_user.values.sum())\n",
    "    labels = ['{0} - {1:1.2f} %'.\\\n",
    "              format(i,j) for i,j in zip(freq_distinct_ips_per_user.index, porcent)]\n",
    "    # To Sort Legend (defaulf: keeps the same order)\n",
    "#     patches, labels, dummy =  zip(*sorted(zip(patches, labels, freq_distinct_logins_per_user.values),\n",
    "#                                           key=lambda x: x[2],\n",
    "#                                           reverse=True))\n",
    "    legend_show_top=10\n",
    "    ax2.legend(ax2.patches[0:legend_show_top], \n",
    "               labels[0:legend_show_top], \n",
    "               bbox_to_anchor=(1.5, 1.),\n",
    "               fontsize=10)\n",
    "    ####\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    fig.savefig('figs/num_ips_by_users.eps', bbox_inches='tight',format='eps', dpi=1200)\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5. Number of Payments by Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_payments['userid'].value_counts())>0:\n",
    "    num_distinct_payments_per_user=df_payments['userid'].value_counts()\n",
    "else:\n",
    "    num_distinct_payments_per_user=df_payments['username'].value_counts()\n",
    "\n",
    "freq_distinct_payments_per_user=num_distinct_payments_per_user.value_counts()\n",
    "cum_dist_user_payments = np.linspace(0.,1.,len(num_distinct_payments_per_user))\n",
    "cdf_user_payments = pd.Series(cum_dist_user_payments, index=num_distinct_payments_per_user.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(num_distinct_payments_per_user)>0:\n",
    "    fig = plt.figure(figsize=(6, 3))\n",
    "    fig.suptitle('* Distribution of Number of Payments by Users:', fontsize=14, y=1.05, x=0.4)\n",
    "\n",
    "    ax1 = plt.subplot2grid((1,2), (0,0))\n",
    "    ax1 = cdf_user_payments.plot( ax=ax1,lw=2, drawstyle='steps',legend=False)\n",
    "    ax1.set_xlabel(\"# Payment\")\n",
    "    ax1.set_ylabel(\"CDF of Users\")\n",
    "    ax1.set_ylim(0,1.05)\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    ax2 = plt.subplot2grid((1,2), (0,1))\n",
    "    freq_distinct_payments_per_user.plot(ax=ax2,kind='pie', \n",
    "                                         labels=None,legend=False,\n",
    "                                         startangle=270)\n",
    "    ax2.set_ylabel(\"\")\n",
    "    ####\n",
    "    # For Pie Chart Better Legend box\n",
    "    porcent = (100.*freq_distinct_payments_per_user.values)/(freq_distinct_payments_per_user.values.sum())\n",
    "    labels = ['{0} - {1:1.2f} %'.\\\n",
    "              format(i,j) for i,j in zip(freq_distinct_payments_per_user.index, porcent)]\n",
    "    # To Sort Legend (defaulf: keeps the same order)\n",
    "#     patches, labels, dummy =  zip(*sorted(zip(patches, labels, freq_distinct_logins_per_user.values),\n",
    "#                                           key=lambda x: x[2],\n",
    "#                                           reverse=True))\n",
    "    legend_show_top=10\n",
    "    ax2.legend(ax2.patches[0:legend_show_top], \n",
    "               labels[0:legend_show_top], \n",
    "               bbox_to_anchor=(1.55, 1.),\n",
    "               fontsize=10)\n",
    "    ####\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    fig.savefig('figs/payments_distribution.eps', bbox_inches='tight',format='eps', dpi=1200)\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6. Total Amount of Money Earned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_payments['amountpaid']) >0:\n",
    "    total_earned=df_payments['amountpaid'].values.sum()\n",
    "    'US$ {:,.2f}'.format(float(total_earned))\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7. Amount of Money Paid by Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_distinct_payments_money_per_user=df_payments['amountpaid'].value_counts()\n",
    "freq_distinct_payments_money_per_user=num_distinct_payments_money_per_user.value_counts()\n",
    "cum_dist_user_payments_money = np.linspace(0.,1.,len(num_distinct_payments_money_per_user))\n",
    "cdf_user_payments_money = pd.Series(cum_dist_user_payments_money, index=num_distinct_payments_money_per_user.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(num_distinct_payments_money_per_user)>0:\n",
    "    fig = plt.figure(figsize=(6, 3))\n",
    "    fig.suptitle('* Distribution of Money Payments by Users:', fontsize=14, y=1.05, x=0.4)\n",
    "\n",
    "    ax1 = plt.subplot2grid((1,2), (0,0))\n",
    "    ax1 = cdf_user_payments_money.plot( ax=ax1,lw=2, drawstyle='steps',legend=False)\n",
    "    ax1.set_xlabel(\"$\")\n",
    "    ax1.set_ylabel(\"CDF of Users\")\n",
    "    ax1.set_ylim(0,1.05)\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    ax2 = plt.subplot2grid((1,2), (0,1))\n",
    "    freq_distinct_payments_money_per_user.plot(ax=ax2,kind='pie', \n",
    "                                               labels=None,legend=False,\n",
    "                                               startangle=270)\n",
    "    ax2.set_ylabel(\"\")\n",
    "    ####\n",
    "    # For Pie Chart Better Legend box\n",
    "    porcent = (100.*freq_distinct_payments_money_per_user.values)/(freq_distinct_payments_money_per_user.values.sum())\n",
    "    labels = ['${0:1.2f} - {1:1.2f} %'.\\\n",
    "              format(i,j) for i,j in zip(freq_distinct_payments_money_per_user.index, porcent)]\n",
    "    # To Sort Legend (defaulf: keeps the same order)\n",
    "#     patches, labels, dummy =  zip(*sorted(zip(patches, labels, freq_distinct_logins_per_user.values),\n",
    "#                                           key=lambda x: x[2],\n",
    "#                                           reverse=True))\n",
    "    legend_show_top=10\n",
    "    ax2.legend(ax2.patches[0:legend_show_top], \n",
    "               labels[0:legend_show_top], \n",
    "               bbox_to_anchor=(1.6, 1.),\n",
    "               fontsize=10)\n",
    "    ####\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    fig.savefig('figs/distribution_amount_paid.eps', bbox_inches='tight',format='eps', dpi=1200)\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8. Countries from where Users Access Booters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logins_country_distribution_sorted = df_logins_iptoasn['country'].value_counts(sort=True,ascending=True)\n",
    "logins_country_distribution = df_logins_iptoasn['country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(logins_country_distribution)>0:\n",
    "    fig = plt.figure(figsize=(6, 3))\n",
    "    fig.suptitle('* Users Accessing from Countries:', fontsize=14, y=1.05, x=0.28)\n",
    "\n",
    "    ax1 = plt.subplot2grid((1,2), (0,0))\n",
    "    logins_country_distribution_sorted.plot(ax=ax1,kind='barh')\n",
    "    ax1.set_ylabel(\"# access\")\n",
    "    ax1.set_xlabel(\"Country\")\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    ax2 = plt.subplot2grid((1,2), (0,1))\n",
    "    logins_country_distribution.plot(ax=ax2,kind='pie', \n",
    "                                     labels=None,legend=False,\n",
    "                                     startangle=270)\n",
    "    ax2.set_ylabel(\"\")\n",
    "    ####\n",
    "    # For Pie Chart Better Legend box\n",
    "    porcent = (100.*logins_country_distribution.values)/(logins_country_distribution.values.sum())\n",
    "    labels = ['{0} - {1:1.2f} %'.\\\n",
    "              format(i,j) for i,j in zip(logins_country_distribution.index, porcent)]\n",
    "    # To Sort Legend (defaulf: keeps the same order)\n",
    "#     patches, labels, dummy =  zip(*sorted(zip(patches, labels, freq_distinct_logins_per_user.values),\n",
    "#                                           key=lambda x: x[2],\n",
    "#                                           reverse=True))\n",
    "    legend_show_top=10\n",
    "    ax2.legend(ax2.patches[0:legend_show_top], \n",
    "               labels[0:legend_show_top], \n",
    "               bbox_to_anchor=(1.55, 1.),\n",
    "               fontsize=10)\n",
    "    ####\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    fig.savefig('figs/user_countries.eps', bbox_inches='tight',format='eps', dpi=1200)\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.9. Countries of Blacklisted IPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blacklist_country_distribution=df_blacklist_iptoasn['country'].value_counts()\n",
    "blacklist_country_distribution_sorted=df_blacklist_iptoasn['country'].value_counts(sort=True,ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(blacklist_country_distribution)>0:\n",
    "    fig = plt.figure(figsize=(6, 3))\n",
    "    fig.suptitle('* Countries of blacklisted IPs', fontsize=14, y=1.05, x=0.28)\n",
    "\n",
    "    ax1 = plt.subplot2grid((1,2), (0,0))\n",
    "    blacklist_country_distribution_sorted.plot(ax=ax1,kind='barh')\n",
    "    ax1.set_ylabel(\"# Access\")\n",
    "    ax1.set_xlabel(\"Country\")\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    ax2 = plt.subplot2grid((1,2), (0,1))\n",
    "    blacklist_country_distribution.plot(ax=ax2,kind='pie', \n",
    "                                        labels=None,legend=False,\n",
    "                                        startangle=270)\n",
    "    ax2.set_ylabel(\"\")\n",
    "    ####\n",
    "    # For Pie Chart Better Legend box\n",
    "    porcent = (100.*blacklist_country_distribution.values)/(logins_country_distribution.values.sum())\n",
    "    labels = ['{0} - {1:1.2f} %'.\\\n",
    "              format(i,j) for i,j in zip(blacklist_country_distribution.index, porcent)]\n",
    "    # To Sort Legend (defaulf: keeps the same order)\n",
    "#     patches, labels, dummy =  zip(*sorted(zip(patches, labels, freq_distinct_logins_per_user.values),\n",
    "#                                           key=lambda x: x[2],\n",
    "#                                           reverse=True))\n",
    "    legend_show_top=10\n",
    "    ax2.legend(ax2.patches[0:legend_show_top], \n",
    "               labels[0:legend_show_top], \n",
    "               bbox_to_anchor=(1.55, 1.),\n",
    "               fontsize=10)\n",
    "    ####\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    fig.savefig('figs/blacklist_countries.eps', bbox_inches='tight',format='eps', dpi=1200)\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.10. Countries of Target IPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attacks_country_distribution=df_attacks_iptoasn['country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(attacks_country_distribution)>0:\n",
    "    fig = plt.figure(figsize=(6, 3))\n",
    "    fig.suptitle('* Countries of target IPs', fontsize=14, y=1.05, x=0.28)\n",
    "\n",
    "    ax1 = plt.subplot2grid((1,2), (0,0))\n",
    "    attacks_country_distribution.plot(ax=ax1,kind='bar')\n",
    "    ax1.set_ylabel(\"# Access\")\n",
    "    ax1.set_xlabel(\"Country\")\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    ax2 = plt.subplot2grid((1,2), (0,1))\n",
    "    attacks_country_distribution.plot(ax=ax2,kind='pie', \n",
    "                                      labels=None,legend=False,\n",
    "                                      startangle=270)\n",
    "    ax2.set_ylabel(\"\")\n",
    "    ####\n",
    "    # For Pie Chart Better Legend box\n",
    "    porcent = (100.*attacks_country_distribution.values)/(attacks_country_distribution.values.sum())\n",
    "    labels = ['{0} - {1:1.2f} %'.\\\n",
    "              format(i,j) for i,j in zip(attacks_country_distribution.index, porcent)]\n",
    "    # To Sort Legend (defaulf: keeps the same order)\n",
    "#     patches, labels, dummy =  zip(*sorted(zip(patches, labels, freq_distinct_logins_per_user.values),\n",
    "#                                           key=lambda x: x[2],\n",
    "#                                           reverse=True))\n",
    "    legend_show_top=10\n",
    "    ax2.legend(ax2.patches[0:legend_show_top], \n",
    "               labels[0:legend_show_top], \n",
    "               bbox_to_anchor=(1.55, 1.),\n",
    "               fontsize=10)\n",
    "    ####\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    fig.savefig('figs/attack_countries_distribution.eps',bbox_inches='tight', format='eps', dpi=1200) \n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.12. Attacks on Same Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_attacks_on_sametarget=df_attacks['targetip'].value_counts()\n",
    "\n",
    "freq_num_attacks_on_sametarget=num_attacks_on_sametarget.value_counts()\n",
    "cum_num_attacks_on_sametarget = np.linspace(0.,1.,len(num_attacks_on_sametarget))\n",
    "cdf_num_attacks_on_sametarget = pd.Series(cum_num_attacks_on_sametarget, index=num_attacks_on_sametarget.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(num_attacks_on_sametarget)>0:\n",
    "    fig = plt.figure(figsize=(6, 3))\n",
    "    fig.suptitle('* Attacks on Same Targets:', fontsize=14, y=1.05,x=0.28)\n",
    "\n",
    "    ax1 = plt.subplot2grid((1,2), (0,0))\n",
    "    ax1 = cdf_num_attacks_on_sametarget.plot( ax=ax1,lw=2, drawstyle='steps',legend=False)\n",
    "    ax1.set_xlabel(\"# IPs\")\n",
    "    ax1.set_ylabel(\"CDF of Users\")\n",
    "    ax1.set_ylim(0,1.05)\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    ax2 = plt.subplot2grid((1,2), (0,1))\n",
    "    freq_num_attacks_on_sametarget.plot(ax=ax2,kind='pie',\n",
    "                                        labels=None,legend=False,\n",
    "                                        startangle=270)\n",
    "    ax2.set_ylabel(\"\")\n",
    "    ####\n",
    "    # For Pie Chart Better Legend box\n",
    "    porcent = (100.*freq_num_attacks_on_sametarget.values)/(freq_num_attacks_on_sametarget.values.sum())\n",
    "    labels = ['{0} - {1:1.2f} %'.\\\n",
    "              format(i,j) for i,j in zip(freq_num_attacks_on_sametarget.index, porcent)]\n",
    "    # To Sort Legend (defaulf: keeps the same order)\n",
    "#     patches, labels, dummy =  zip(*sorted(zip(patches, labels, freq_distinct_logins_per_user.values),\n",
    "#                                           key=lambda x: x[2],\n",
    "#                                           reverse=True))\n",
    "    legend_show_top=10\n",
    "    ax2.legend(ax2.patches[0:legend_show_top], \n",
    "               labels[0:legend_show_top], \n",
    "               bbox_to_anchor=(1.55, 1.),\n",
    "               fontsize=10)\n",
    "    ####\n",
    "    fig.tight_layout()\n",
    "\n",
    "    fig.show()\n",
    "    fig.savefig('figs/attacks_on_same_target.eps', bbox_inches='tight',format='eps', dpi=1200)\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.11. Attacks per day (timeseries) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_attacks)>0:\n",
    "    attack_timeseries=df_attacks.set_index(['date']).groupby(pd.TimeGrouper(freq='D')).agg(['count'])['action']\n",
    "    attack_mean_perday=attack_timeseries.mean()\n",
    "    attack_median_perday=attack_timeseries.median()\n",
    "else:\n",
    "    attack_timeseries=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(attack_timeseries)>0:\n",
    "    fig = plt.figure(figsize=(6,3))\n",
    "\n",
    "    ax1 = plt.subplot2grid((1,1), (0,0), rowspan=2)\n",
    "    attack_timeseries.plot(ax=ax1,\n",
    "                           lw=2,\n",
    "                           legend=False,\n",
    "    #                        style='--'\n",
    "                          )\n",
    "\n",
    "    # X and Y Labels and Ticks\n",
    "    ax1.set_xlabel(\"Time (bin=day)\")\n",
    "    ax1.set_ylabel(\"# Attacks\")\n",
    "\n",
    "    ax1.annotate(str(int(attack_median_perday[0]))+' (median)', \n",
    "                 (min(df_attacks['date']), attack_median_perday),\n",
    "                 xytext=(350, -1), \n",
    "                 textcoords='offset points',\n",
    "                 color='black', \n",
    "                 arrowprops=dict(arrowstyle='-|>',\n",
    "                                 color='black'))\n",
    "    fig.savefig('figs/attacks_timeseries.eps', bbox_inches='tight',format='eps', dpi=1200)\n",
    "\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.12. Time Between Logins and Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_attacks_and_logins)>0:\n",
    "    print(\"redo\")\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.13. Who attack whom? (users on the country level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_attacks_extended)>0 and len(df_logins_extended)>0:\n",
    "    merged_attacks_logins = pd.merge(df_attacks_extended,\n",
    "                                  df_logins_extended,\n",
    "                                  how = 'left',\n",
    "                                  left_on = 'date',\n",
    "                                  right_on = 'date')[['targetcountry','srccountry']]\n",
    "\n",
    "    who_against_whom = merged_attacks_logins.groupby(['targetcountry','srccountry'])\\\n",
    "                            .size()\\\n",
    "                            .reset_index()\\\n",
    "                            .pivot('srccountry','targetcountry',0)\n",
    "else:\n",
    "    who_against_whom =\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(who_against_whom)>0:\n",
    "    fig = plt.figure(figsize=(8,12))\n",
    "    fig.suptitle('Countries of Target IPs', fontsize=14, y=.92)\n",
    "\n",
    "    ax1 = plt.subplot2grid((1,1), (0,0))\n",
    "    sns.set()\n",
    "    sns.heatmap(who_against_whom,\n",
    "                ax=ax1,\n",
    "#                 cmap=\"YlGnBu\",\n",
    "    #             linewidths=.5,\n",
    "    #             annot=True\n",
    "                )\n",
    "\n",
    "    ax1.set_ylabel(\"Source Country\")\n",
    "    ax1.set_xlabel(\"Attack Target Country\")\n",
    "\n",
    "    fig.show()\n",
    "    fig.savefig('figs/who_attack_whom.eps', bbox_inches='tight',format='eps', dpi=1200)\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!<br>\n",
    "FIFTH PART: Query Interface<br>\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import widgets, interact, Dropdown,HTML\n",
    "from IPython.display import display,clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_userid_submit(sender):\n",
    "    clear_output()\n",
    "    print(\"Searching by id =\",search_by_userid.value)\n",
    "    \n",
    "    if len(df_users[df_users['userid']== int(search_by_userid.value)])>0:\n",
    "        display(df_users[df_logins['userid']== int(search_by_userid.value)])\n",
    "    \n",
    "    if len(df_logins_extended[df_logins_extended['userid']== int(search_by_userid.value)])>0:\n",
    "        display(df_logins_extended[df_logins_extended['userid']== int(search_by_userid.value)])\n",
    "    \n",
    "    if len(df_attacks_extended[df_attacks_extended['userid']== int(search_by_userid.value)])>0:\n",
    "        display(df_attacks_extended[df_attacks_extended['userid']== int(search_by_userid.value)])\n",
    "    \n",
    "    if len(df_payments[df_payments['userid']== int(search_by_userid.value)])>0:\n",
    "        display(df_payments[df_payments['userid']== int(search_by_userid.value)])\n",
    "\n",
    "def search_by_username_submit(sender):\n",
    "    clear_output()\n",
    "    print(\"Searching by username =\",search_by_username.value,\"\\n\")\n",
    "    \n",
    "    if len(df_users[df_users['username']== search_by_username.value])>0:\n",
    "        print(\"Table df_users:\")\n",
    "        display(df_users[df_users['username']== search_by_username.value])\n",
    "    \n",
    "    if len(df_logins_extended[df_logins_extended['username']== search_by_username.value])>0:\n",
    "        print(\"Table df_logins:\")\n",
    "        display(df_logins_extended[df_logins_extended['username']== search_by_username.value])\n",
    "\n",
    "    if len(df_attacks_extended[df_attacks_extended['username']== search_by_username.value])>0:\n",
    "        print(\"Table df_attacks:\")\n",
    "        display(df_attacks_extended[df_attacks_extended['username']== search_by_username.value])\n",
    "    \n",
    "    if len(df_payments[df_payments['username']== search_by_username.value])>0:\n",
    "        print(\"Table df_payments:\")\n",
    "        display(df_payments[df_payments['username']== search_by_username.value])\n",
    "        \n",
    "def search_by_ip_submit(sender):\n",
    "    clear_output()\n",
    "    print(\"Searching by IP address =\",str(search_by_ip.value),\"\\n\")\n",
    "    \n",
    "    if len(df_logins_extended[df_logins_extended['userip']== str(search_by_ip.value)])>0:\n",
    "        print(\"Table df_logins (as ATTACKER:\")\n",
    "        display(df_logins_extended[df_logins_extended['userip']== str(search_by_ip.value)])\n",
    "     \n",
    "    if len(df_attacks_extended[df_attacks_extended['targetip']== str(search_by_ip.value)])>0:\n",
    "        print(\"Table df_attacks (as TARGET):\")\n",
    "        display(df_attacks_extended[df_attacks_extended['targetip']== str(search_by_ip.value)])  \n",
    "      \n",
    "       \n",
    "def search_by_asn_submit(sender):\n",
    "    clear_output()\n",
    "    print(\"Searching by Autonomous System Number (ASN) =\",search_by_asn.value,\"\\n\")\n",
    "    \n",
    "    if len(df_logins_extended[df_logins_extended['srcasn']== int(search_by_asn.value)])>0:\n",
    "        print(\"Table df_logins (as ATTACKER):\")\n",
    "        display(df_logins_extended[df_logins_extended['srcasn']== int(search_by_asn.value)])\n",
    "    \n",
    "    if len(df_attacks_extended[df_attacks_extended['targetasn']== str(search_by_asn.value)])>0:\n",
    "        print(\"Table df_attacks (as TARGET):\")\n",
    "        display(df_attacks_extended[df_attacks_extended['targetasn']== str(search_by_asn.value)])\n",
    "\n",
    "    if len(df_attacks_extended[df_attacks_extended['targetasn']== str(search_by_asn.value)])>0:\n",
    "        print(\"Table df_attacks (as TARGET):\")\n",
    "        display(df_attacks_extended[df_attacks_extended['targetasn']== str(search_by_asn.value)])\n",
    "\n",
    "        \n",
    "country_list=pd.read_csv('https://raw.githubusercontent.com/datasets/country-list/master/data.csv',delimiter=\",\",error_bad_lines=False)\n",
    "def search_by_country_submit():\n",
    "    country_code=country_list[country_list['Name']==search_by_country.value]['Code'].values[0]\n",
    "    clear_output()\n",
    "    print(\"Searching by Country =\",search_by_country.value,\"\\n\")\n",
    "    \n",
    "    if len(df_logins_extended[df_logins_extended['srccountry']== country_code])>0:\n",
    "        print(\"Table df_logins (as ATTACKER):\")\n",
    "        display(df_logins_extended[df_logins_extended['srccountry']== country_code])\n",
    "    \n",
    "    if len(df_attacks_extended[df_attacks_extended['targetcountry']== country_code])>0:\n",
    "        print(\"Table df_attacks (as TARGET):\")\n",
    "        display(df_attacks_extended[df_attacks_extended['targetcountry']== country_code])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML('<h3>*Search by (only) one field per time:</h3>'))\n",
    "\n",
    "search_by_username = widgets.Text(description=\"username\")\n",
    "display(search_by_username)\n",
    "search_by_username.on_submit(search_by_username_submit)\n",
    "\n",
    "search_by_userid = widgets.Text(description=\"userid\")\n",
    "display(search_by_userid)\n",
    "search_by_userid.on_submit(search_by_userid_submit)\n",
    "\n",
    "search_by_ip = widgets.Text(description=\"IP\")\n",
    "display(search_by_ip)\n",
    "search_by_ip.on_submit(search_by_ip_submit)\n",
    "\n",
    "search_by_asn = widgets.Text(description=\"ASN\")\n",
    "display(search_by_asn)\n",
    "search_by_asn.on_submit(search_by_asn_submit)\n",
    "\n",
    "search_by_country = Dropdown(description=\"Country\", options=country_list['Name'].tolist())\n",
    "search_by_country.on_trait_change(search_by_country_submit, name=\"value\")\n",
    "display(search_by_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
