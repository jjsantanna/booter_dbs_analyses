{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!<br>\n",
    "Brief explanation on our Booter database analysis methodology: <br>\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of: Bootertw\n",
    "#### File originally retrieved from: http://krebsonsecurity.com/wp-content/uploads/2013/03/booter.7z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Libraries that I use to analyse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from io import StringIO\n",
    "import re\n",
    "\n",
    "from IPython.display import display,clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!<br>\n",
    "STEP 0: Reading an input Booter database file<br>\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dumpfile='booter.sql'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!<br>\n",
    "STEP 1: Adaptation to our Booter database schema<br>\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading the first 'N' (100) lines of the input Booter database file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- phpMyAdmin SQL Dump\n",
      "-- version 3.5.2\n",
      "-- http://www.phpmyadmin.net\n",
      "--\n",
      "-- Host: localhost\n",
      "-- Generation Time: Mar 15, 2013 at 07:46 AM\n",
      "-- Server version: 5.1.66-0+squeeze1\n",
      "-- PHP Version: 5.3.20-1~dotdeb.0\n",
      "\n",
      "SET SQL_MODE=\"NO_AUTO_VALUE_ON_ZERO\";\n",
      "SET time_zone = \"+00:00\";\n",
      "\n",
      "\n",
      "/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;\n",
      "/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;\n",
      "/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;\n",
      "/*!40101 SET NAMES utf8 */;\n",
      "\n",
      "--\n",
      "-- Database: `booter`\n",
      "--\n",
      "\n",
      "-- --------------------------------------------------------\n",
      "\n",
      "--\n",
      "-- Table structure for table `adminlogs`\n",
      "--\n",
      "\n",
      "CREATE TABLE IF NOT EXISTS `adminlogs` (\n",
      "`id` int(11) NOT NULL AUTO_INCREMENT,\n",
      "`action` text NOT NULL,\n",
      "`user` int(11) NOT NULL,\n",
      "`time` int(11) NOT NULL,\n",
      "PRIMARY KEY (`id`)\n",
      ") ENGINE=InnoDB  DEFAULT CHARSET=latin1 AUTO_INCREMENT=205 ;\n",
      "\n",
      "--\n",
      "-- Dumping data for table `adminlogs`\n",
      "--\n",
      "\n",
      "INSERT INTO `adminlogs` (`id`, `action`, `user`, `time`) VALUES\n",
      "(78, 'Added news post: Downtime 24 January', 1, 1358991550),\n",
      "(79, 'Added new user with username: sultan', 1, 1358994930),\n",
      "(80, 'Added new user with username: Awesome7', 1, 1358997207),\n",
      "(81, 'Added new user with username: voxility1', 1, 1358997271),\n",
      "(82, 'Added new user with username: Hotdog', 1, 1358997619),\n",
      "(83, 'Added new user with username: noob1337', 1, 1358998904),\n",
      "(84, 'Added new user with username: thebawsjc', 1, 1358999382),\n",
      "(85, 'Added new user with username: secac', 1, 1359020488),\n",
      "(86, 'Added new user with username: kim', 1, 1359022781),\n",
      "(87, 'Added new user with username: sealbay', 1, 1359025043),\n",
      "(88, 'Added new user with username: iWiZ13', 1, 1359025747),\n",
      "(89, 'Added new user with username: Tyrantelf', 1, 1359065876),\n",
      "(90, 'Added new user with username: anpapyalar', 1, 1359066060),\n",
      "(91, 'Added new user with username: espen', 1, 1359066180),\n",
      "(92, 'Added new user with username: arrogance', 66, 1359068893),\n",
      "(93, 'Added new user with username: Ghostxx', 1, 1359099169),\n",
      "(94, 'Added new user with username: Burcs', 1, 1359100419),\n",
      "(95, 'Added new user with username: Mailk', 1, 1359100933),\n",
      "(96, 'Added new user with username: mailk2', 1, 1359101172),\n",
      "(97, 'Added new user with username: nidhish', 1, 1359103703),\n",
      "(98, 'Added new user with username: newacc2', 1, 1359103856),\n",
      "(99, 'Added new user with username: trialx82', 1, 1359105533),\n",
      "(100, 'Added new user with username: brandon', 1, 1359107639),\n",
      "(101, 'Added new user with username: trialx81', 1, 1359107695),\n",
      "(102, 'Added new user with username: Exploitz', 1, 1359109975),\n",
      "(103, 'Added new user with username: peterx', 1, 1359115936),\n",
      "(104, 'Added new user with username: martinx', 1, 1359116468),\n",
      "(105, 'Added new user with username: jimmybrand19', 1, 1359117760),\n",
      "(106, 'Added new user with username: bobokidd', 1, 1359120694),\n",
      "(107, 'Added new user with username: Lateuser8x', 1, 1359121645),\n",
      "(108, 'Added new user with username: arcane', 1, 1359124010),\n",
      "(109, 'Added new user with username: blackawesome', 1, 1359124774),\n",
      "(110, 'Added new user with username: Supx', 1, 1359125331),\n",
      "(111, 'Added new user with username: paingod', 1, 1359126365),\n",
      "(112, 'Added new user with username: jhorne4x', 1, 1359129246),\n",
      "(113, 'Added new user with username: sluh', 1, 1359129308),\n",
      "(114, 'Added new user with username: trialx72', 1, 1359131572),\n",
      "(115, 'Added new user with username: James', 66, 1359139989),\n",
      "(116, 'Added new user with username: Broomout', 1, 1359142701),\n",
      "(117, 'Added new user with username: vimiono', 66, 1359145635),\n",
      "(118, 'Added new user with username: Awesomeguy', 1, 1359146501),\n",
      "(119, 'Added new user with username: countonme', 1, 1359147983),\n",
      "(120, 'Added new user with username: Montanarc', 1, 1359152405),\n",
      "(121, 'Added new user with username: andre', 66, 1359208818),\n",
      "(122, 'Added new user with username: wickd', 2, 1359320678),\n",
      "(123, 'Added new user with username: moldjelly', 2, 1359324026),\n",
      "(124, 'Added new user with username: hdfsi', 2, 1359439353),\n",
      "(125, 'Added new user with username: haze', 1, 1359633939),\n",
      "(126, 'Added new user with username: starfall', 1, 1359634215),\n",
      "(127, 'Added new user with username: CRAZY', 1, 1359646485),\n",
      "(128, 'Added new user with username: arco', 2, 1359731609),\n",
      "(129, 'Added new user with username: Muscab123', 1, 1359882046),\n",
      "(130, 'Added new user with username: x21', 1, 1359898750),\n",
      "(131, 'Added new user with username: baba1234', 1, 1359967561),\n",
      "(132, 'Added new user with username: thatguy', 1, 1360060952),\n",
      "(133, 'Added new user with username: saice', 1, 1360156600),\n",
      "(134, 'Added new user with username: markxx', 1, 1360203265),\n",
      "(135, 'Added new user with username: Asbjaarn', 1, 1360488365),\n",
      "(136, 'Added new user with username: heartx', 1, 1360581992),\n"
     ]
    }
   ],
   "source": [
    "lines_to_read=100\n",
    "\n",
    "with open(dumpfile) as myfile:\n",
    "    firstlines=myfile.readlines()[0:lines_to_read] #put here the interval you want\n",
    "    for x in firstlines:\n",
    "        print(x.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Listing tables that have content inserted into the dump file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_tables_with_insert(dumpfile):\n",
    "    tables = []\n",
    "    with open(dumpfile, 'rb') as f:\n",
    "        for line in f:\n",
    "            line = line.decode(\"utf-8\").strip()\n",
    "            if line.lower().startswith('insert'):\n",
    "                table = re.findall(r'`(.*?)`', line)\n",
    "                tables.append(table[0])\n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 adminlogs\n",
      "2 attacks\n",
      "3 attacks\n",
      "4 attacks\n",
      "5 attacks\n",
      "6 attacks\n",
      "7 attacks\n",
      "8 attacks\n",
      "9 attacks\n",
      "10 attacks\n",
      "11 attacks\n",
      "12 attacks\n",
      "13 attacks\n",
      "14 attacks\n",
      "15 attacks\n",
      "16 attacks\n",
      "17 attacks\n",
      "18 attacks\n",
      "19 attacks\n",
      "20 attacks\n",
      "21 attacks\n",
      "22 attacks\n",
      "23 attacks\n",
      "24 attacks\n",
      "25 attacks\n",
      "26 attacks\n",
      "27 attacks\n",
      "28 attacks\n",
      "29 attacks\n",
      "30 attacks\n",
      "31 attacks\n",
      "32 attacks\n",
      "33 attacks\n",
      "34 attacks\n",
      "35 attacks\n",
      "36 attacks\n",
      "37 attacks\n",
      "38 attacks\n",
      "39 attacks\n",
      "40 attacks\n",
      "41 attacks\n",
      "42 attacks\n",
      "43 attacks\n",
      "44 attacks\n",
      "45 attacks\n",
      "46 attacks\n",
      "47 attacks\n",
      "48 attacks\n",
      "49 attacks\n",
      "50 attacks\n",
      "51 attacks\n",
      "52 attacks\n",
      "53 attacks\n",
      "54 attacks\n",
      "55 attacks\n",
      "56 attacks\n",
      "57 attacks\n",
      "58 attacks\n",
      "59 attacks\n",
      "60 attacks\n",
      "61 attacks\n",
      "62 attacks\n",
      "63 attacks\n",
      "64 attacks\n",
      "65 attacks\n",
      "66 attacks\n",
      "67 attacks\n",
      "68 attacks\n",
      "69 attacks\n",
      "70 attacks\n",
      "71 attacks\n",
      "72 attacks\n",
      "73 attacks\n",
      "74 attacks\n",
      "75 attacks\n",
      "76 attacks\n",
      "77 attacks\n",
      "78 attacks\n",
      "79 attacks\n",
      "80 attacks\n",
      "81 attacks\n",
      "82 attacks\n",
      "83 attacks\n",
      "84 attacks\n",
      "85 attacks\n",
      "86 attacks\n",
      "87 attacks\n",
      "88 attacks\n",
      "89 attacks\n",
      "90 attacks\n",
      "91 attacks\n",
      "92 attacks\n",
      "93 attacks\n",
      "94 attacks\n",
      "95 attacks\n",
      "96 attacks\n",
      "97 attacks\n",
      "98 attacks\n",
      "99 attacks\n",
      "100 attacks\n",
      "101 attacks\n",
      "102 attacks\n",
      "103 attacks\n",
      "104 attacks\n",
      "105 attacks\n",
      "106 attacks\n",
      "107 attacks\n",
      "108 attacks\n",
      "109 attacks\n",
      "110 attacks\n",
      "111 attacks\n",
      "112 attacks\n",
      "113 attacks\n",
      "114 attacks\n",
      "115 attacks\n",
      "116 attacks\n",
      "117 attacks\n",
      "118 attacks\n",
      "119 attacks\n",
      "120 attacks\n",
      "121 attacks\n",
      "122 attacks\n",
      "123 attacks\n",
      "124 friendsandenemies\n",
      "125 giftcodes\n",
      "126 logins\n",
      "127 logins\n",
      "128 logins\n",
      "129 logins\n",
      "130 logins\n",
      "131 logins\n",
      "132 logins\n",
      "133 logins\n",
      "134 logins\n",
      "135 logins\n",
      "136 logins\n",
      "137 logins\n",
      "138 logins\n",
      "139 logins\n",
      "140 logins\n",
      "141 logins\n",
      "142 logins\n",
      "143 logins\n",
      "144 logins\n",
      "145 logins\n",
      "146 logins\n",
      "147 logins\n",
      "148 logins\n",
      "149 logins\n",
      "150 logins\n",
      "151 logins\n",
      "152 logins\n",
      "153 logins\n",
      "154 logins\n",
      "155 logins\n",
      "156 logins\n",
      "157 logins\n",
      "158 logins\n",
      "159 logins\n",
      "160 logins\n",
      "161 logins\n",
      "162 logins\n",
      "163 logins\n",
      "164 logins\n",
      "165 logins\n",
      "166 logins\n",
      "167 logins\n",
      "168 logins\n",
      "169 logins\n",
      "170 logins\n",
      "171 logins\n",
      "172 logins\n",
      "173 logins\n",
      "174 logins\n",
      "175 logins\n",
      "176 logins\n",
      "177 logins\n",
      "178 logins\n",
      "179 logins\n",
      "180 logins\n",
      "181 logins\n",
      "182 logins\n",
      "183 logins\n",
      "184 logins\n",
      "185 logins\n",
      "186 logins\n",
      "187 logins\n",
      "188 logins\n",
      "189 logins\n",
      "190 logins\n",
      "191 logins\n",
      "192 logins\n",
      "193 logins\n",
      "194 logins\n",
      "195 logins\n",
      "196 logins\n",
      "197 logins\n",
      "198 logins\n",
      "199 logins\n",
      "200 logins\n",
      "201 logins\n",
      "202 logins\n",
      "203 logins\n",
      "204 logins\n",
      "205 logins\n",
      "206 logins\n",
      "207 logins\n",
      "208 logins\n",
      "209 logins\n",
      "210 logins\n",
      "211 logins\n",
      "212 logins\n",
      "213 logins\n",
      "214 logins\n",
      "215 logins\n",
      "216 logins\n",
      "217 news\n",
      "218 paymentlogs\n",
      "219 servers\n",
      "220 settings\n",
      "221 tickets\n",
      "222 ticket_replies\n",
      "223 toverify\n",
      "224 users\n",
      "225 users\n"
     ]
    }
   ],
   "source": [
    "tables=enumerate(list_tables_with_insert(dumpfile))\n",
    "\n",
    "for i, item in tables:\n",
    "    print(i+1,item)\n",
    "    \n",
    "print(tables.uni)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the list above looks like? \n",
    "looks well-formed but many insert operations in a same table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align='center'>======================================================================\n",
    "If NOT well-formed SQL dump file then you must first do the following:\n",
    "======================================================================</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Removing lines that are not part of the actual content to be analysed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# num_header_lines=55\n",
    "\n",
    "# columns=['id', 'username', 'action', 'ip','date','platform','hidden']\n",
    "\n",
    "# df_manual= pd.read_csv(dumpfile,\n",
    "#             delimiter=\",\",\n",
    "#             skiprows = num_header_lines,\n",
    "#             names = columns)\n",
    "# df_manual.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually analysing if first column has an SQL insert operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_manual[df_manual['id'].str.contains('INSERT INTO')].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually analysing if the last column contains ';' remaing from the end of an insert operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_manual[df_manual['hidden'].str.contains(';')].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Splitting and naming tables and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting the content of column 'action' in 6 new columns:\n",
    "# df_attacks['targetip']=df_attacks['action'].str.extract('on (?P<targetip>[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+)', expand=True) \n",
    "# df_attacks['targeturl']=df_attacks['action'].str.extract('on [A-Z]+://(?P<targeturl>[^\\s:/]+)', expand=True, flags=re.IGNORECASE)\n",
    "# df_attacks['port']=df_attacks['action'].str.extract(':(?P<targetport>[0-9]+)', expand=True)\n",
    "# df_attacks['duration']=df_attacks['action'].str.extract('for\\s+(?P<duration>[0-9]+)', expand=True)\n",
    "# df_attacks['type']=df_attacks['action'].str.extract('using\\s+([a-z]\\w{0,})', expand=True, flags=re.IGNORECASE)\n",
    "# df_attacks['vip_attack']=df_attacks['action'].str.contains('Launched a VIP stress test on', regex=True)\n",
    "# df_attacks['target_ddosprotected']=df_attacks['action'].str.contains('DDoS protected', regex=True)\n",
    "\n",
    "# Splitting the content of column 'ip' in 2 new columns\n",
    "# df_attacks['userip']=df_attacks['ip'].str.extract('(([0-9a-fA-F]*[:\\.])+[0-9a-fA-F]*)', expand=True)[0] #for  IPv4 and IPv6\n",
    "# df_attacks['country']=df_attacks['ip'].str.extract('([A-Z]\\w)', expand=True,flags=re.IGNORECASE)\n",
    "\n",
    "# Splitting the content of column 'platform' in 2 new columns\n",
    "# df_attacks['user_os']=df_attacks['platform'].str.extract('on\\s+(\\w+)', expand=True,flags=re.IGNORECASE)\n",
    "# df_attacks['user_browser']=df_attacks['platform'].str.extract('\\'(.*)\\s+on', expand=True,flags=re.IGNORECASE)\n",
    "\n",
    "# Splitting table 'attacks' to compose table 'logins'\n",
    "# df_logins=df_attacks[['username','userip','date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align='center'>======================================================================\n",
    "Additional functions\n",
    "======================================================================</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Function to read tables from wel-formed SQL database dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For well formed SQL database dumps!\n",
    "def read_inserted_table(dumpfile, target_table):\n",
    "    sio = StringIO()\n",
    "    fast_forward = True\n",
    "    with open(dumpfile, 'rb') as f:\n",
    "        for line in f:\n",
    "            line = line.decode(\"utf-8\").strip()\n",
    "            line = re.sub(\"(?!(([^']*'){2})*[^']*$)\\)\", '',line) #Step0:remove any \")\" from the content of columns\n",
    "            if line.lower().startswith('insert') and target_table in line:\n",
    "                fast_forward = False\n",
    "            if fast_forward:\n",
    "                continue\n",
    "            data = re.findall('\\([^\\)]*\\)', line) #Step1: get the content between parentesis (i.e., insert line)\n",
    "            try:\n",
    "                newline = data[0].strip('()') #Step2:remove parenthesis\n",
    "                newline=newline.replace('`','') #Step3: remove ` (usually in table names)\n",
    "                newline=re.sub(r'(?!(([^\\']*\\'){2})*[^\\']*$),', '', newline) #Step4: remove commas from the content of columns\n",
    "                newline=newline.replace('\\'','') #Step5: remove single quotes\n",
    "                newline=newline.replace(', ', ',') #Step6: remove single spaces after comma (i.e., in the beginning of a column)\n",
    "                sio.write(newline)\n",
    "                sio.write(\"\\n\")\n",
    "            except IndexError:\n",
    "                pass\n",
    "            if line.endswith(';'):\n",
    "                break\n",
    "    sio.seek(0)\n",
    "    return sio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Converter functions for formatting content of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tolowercase(text):\n",
    "    try:\n",
    "        return text.lower()\n",
    "    except AttributeError:\n",
    "        return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_singlequote(text):\n",
    "    try:\n",
    "        return text.strip('\\'')\n",
    "    except AttributeError:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timestamp2datetime(timestamp):\n",
    "    try:\n",
    "        return  pd.to_datetime(timestamp,unit='s')\n",
    "    except AttributeError:\n",
    "        return timestamp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dateformat2datetime(text):\n",
    "    try:\n",
    "        return  pd.to_datetime(text,format='%d-%m-%Y %H:%M')\n",
    "    except AttributeError:\n",
    "        return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_singlequote_and_tolowercase(text):\n",
    "    try:\n",
    "        return text.strip('\\'').lower()\n",
    "    except AttributeError:\n",
    "        return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_singlequote_and_dateformat2datetime(text):\n",
    "    try:\n",
    "        return  pd.to_datetime(text.strip('\\''),format='%d-%m-%Y %H:%M')\n",
    "    except AttributeError:\n",
    "        return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_parenthesisandsemicolon(text):\n",
    "    try:\n",
    "        return text.replace(');',\"\")\n",
    "    except AttributeError:\n",
    "        return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_insertintologs(text):\n",
    "    try:\n",
    "        return int(text.replace('INSERT INTO `logs` VALUES (',\"\"))\n",
    "    except AttributeError:\n",
    "        return text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align='center'>======================================================================\n",
    "Adapting EACH existing table\n",
    "======================================================================</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Table: 'api'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How this table looks like without modification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablename='api'\n",
    "\n",
    "pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "            delimiter=\",\",\n",
    "            error_bad_lines=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Is this table different from other previous loaded table? YES\n",
    "#### Q2: Is this table similar to any table in the generic Booter database schema? NO\n",
    "    - This table looks like: \n",
    "    - The blacklist table must to have the columns: \n",
    "    \n",
    "#### Q3: Are there modifications required? \n",
    "    - On the table name: \n",
    "        o \n",
    "    - On the column type/converter: \n",
    "        o    \n",
    "    - On the column name:\n",
    "        o  \n",
    "    - Split columns:\n",
    "        o \n",
    "    - Add required columns:\n",
    "        o "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Table: 'blacklist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How this table looks like without modification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablename='blacklist'\n",
    "\n",
    "# Only displaying the results\n",
    "pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "            delimiter=\",\",\n",
    "            error_bad_lines=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Is this table different from other previous loaded table? YES\n",
    "#### Q2: Is this table similar to any table in the generic Booter database schema? YES\n",
    "    - This table looks like: blacklist\n",
    "    - The blacklist table must to have the columns: 'id','ip','note'\n",
    "    \n",
    "#### Q3: Are there modifications required? \n",
    "    - On the table name: \n",
    "        o \n",
    "    - On the column type: \n",
    "        o    \n",
    "    - On the column name:\n",
    "        o ID => id\n",
    "        o IP => ip\n",
    "    - Split columns:\n",
    "        o \n",
    "    - Add required columns:\n",
    "        o         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying table: 'blacklist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_blacklist = pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "                                delimiter=\",\",\n",
    "                                error_bad_lines=False,)\n",
    "#                        converters = {'date':timestamp2datetime})\n",
    "\n",
    "###Changing names of columns\n",
    "df_blacklist.rename(columns = {'ID': 'id',\n",
    "                               'IP': 'ip'},\n",
    "                         inplace=True)\n",
    "\n",
    "###Creating empty columns (with \"\" [for future string] or np.nan [for future float])\n",
    "# df_blacklist['generic_column_name_missing'] = \"\"\n",
    "\n",
    "###Showing some lines after adapt the table\n",
    "df_blacklist.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Table: 'fe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How this table looks like without modification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablename='fe'\n",
    "\n",
    "pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "            delimiter=\",\",\n",
    "            error_bad_lines=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Is this table different from other previous loaded table? YES\n",
    "#### Q2: Is this table similar to any table in the generic Booter database schema? NO\n",
    "    - This table looks like: 'id','ip','note','userid','type'\n",
    "    - The blacklist table must to have the columns: 'id','ip','note','userid','type'\n",
    "    \n",
    "#### Q3: Are there modifications required? \n",
    "    - On the table name: \n",
    "        o fe => friendsenemies\n",
    "    - On the column type: \n",
    "    - On the column name:\n",
    "        o ID => id\n",
    "        o userID => userid\n",
    "    - Add required columns:\n",
    "        o \n",
    "    - Split columns:\n",
    "        o "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying table: 'fe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Loading the column converting columns with predefined functions\n",
    "df_friendsenemies = pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "                                delimiter=\",\",\n",
    "                                error_bad_lines=False,)\n",
    "#                        converters = {'date':timestamp2datetime})\n",
    "\n",
    "###Changing names of columns\n",
    "df_friendsenemies.rename(columns = {'ID': 'id',\n",
    "                                    'userID': 'userid'},\n",
    "                         inplace=True)\n",
    "\n",
    "##Creating empty columns (with \"\" [for future string] or np.nan [for future float])\n",
    "# df_friendsenemies['generic_column_name_missing'] = \"\"\n",
    "\n",
    "###Showing some lines after adapt the table\n",
    "df_friendsenemies.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4. Table: 'gateway'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How this table looks like without modification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablename='gateway'\n",
    "\n",
    "pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "            delimiter=\",\",\n",
    "            error_bad_lines=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Is this table different from other previous loaded table? YES\n",
    "#### Q2: Is this table similar to any table in the generic Booter database schema? YES\n",
    "    - This table looks like: gateways\n",
    "    - The blacklist table must to have the columns: email\n",
    "    \n",
    "#### Q3: Are there modifications required? \n",
    "    - On the table name: \n",
    "        o gatway => gateways\n",
    "    - On the column type: \n",
    "        o    \n",
    "    - On the column name:\n",
    "        o  \n",
    "    - Add required columns:\n",
    "        o \n",
    "    - Split columns:\n",
    "        o "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying table: 'gateways'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Loading the column converting columns with predefined functions\n",
    "df_gateways = pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "                                delimiter=\",\",\n",
    "                                error_bad_lines=False,)\n",
    "#                        converters = {'date':timestamp2datetime})\n",
    "\n",
    "###Changing names of columns\n",
    "# df_gateways.rename(columns = {'actual1': 'generic_column_name1',\n",
    "#                                     'actual2': 'generic_column_name2'},\n",
    "#                          inplace=True)\n",
    "\n",
    "###Creating empty columns (with \"\" [for future string] or np.nan [for future float])\n",
    "# df_gateways['generic_column_name_missing'] = \"\"\n",
    "\n",
    "###Showing some lines after adapt the table\n",
    "df_gateways.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Table: 'iplogs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How this table looks like without modification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablename='iplogs'\n",
    "\n",
    "pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "            delimiter=\",\",\n",
    "            error_bad_lines=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Is this table different from other previous loaded table? YES\n",
    "#### Q2: Is this table similar to any table in the generic Booter database schema? YES\n",
    "    - This table looks like: logins\n",
    "    - The blacklist table must to have the columns: 'id','userid','username','userip','date'\n",
    "    \n",
    "#### Q3: Are there modifications required? \n",
    "    - On the table name: \n",
    "        o iplogs => logins\n",
    "    - On the column type: \n",
    "        o date => timestamp2date \n",
    "    - On the column name:\n",
    "        o ID => id\n",
    "        o userID => userid\n",
    "        o logged => userip\n",
    "    - Add required columns:\n",
    "        o username\n",
    "    - Split columns:\n",
    "        o "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying table: 'iplogins'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Loading the column converting columns with predefined functions\n",
    "df_logins = pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "                                delimiter=\",\",\n",
    "                                error_bad_lines=False,\n",
    "                       converters = {'date':timestamp2datetime})\n",
    "\n",
    "###Changing names of columns\n",
    "df_logins.rename(columns = {'ID': 'id',\n",
    "                                    'userID': 'userid',\n",
    "                           'logged':'userip'},\n",
    "                         inplace=True)\n",
    "\n",
    "###Creating empty columns (with \"\" [for future string] or np.nan [for future float])\n",
    "df_logins['username'] = \"\"\n",
    "\n",
    "###Showing some lines after adapt the table\n",
    "df_logins.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6. Table: 'loginlogs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How this table looks like without modification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablename='loginlogs'\n",
    "\n",
    "pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "            delimiter=\",\",\n",
    "            error_bad_lines=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Is this table different from other previous loaded table? NO\n",
    "#### Q2: Is this table similar to any table in the generic Booter database schema? \n",
    "    - This table looks like: \n",
    "    - The blacklist table must to have the columns: \n",
    "    \n",
    "#### Q3: Are there modifications required? \n",
    "    - On the table name: \n",
    "        o \n",
    "    - On the column type: \n",
    "        o    \n",
    "    - On the column name:\n",
    "        o  \n",
    "    - Add required columns:\n",
    "        o \n",
    "    - Split columns:\n",
    "        o "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7. Table: 'logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How this table looks like without modification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablename='logs'\n",
    "\n",
    "pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "            delimiter=\",\",\n",
    "            error_bad_lines=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Is this table different from other previous loaded table? NO\n",
    "#### Q2: Is this table similar to any table in the generic Booter database schema? \n",
    "    - This table looks like: \n",
    "    - The blacklist table must to have the columns: \n",
    "    \n",
    "#### Q3: Are there modifications required? \n",
    "    - On the table name: \n",
    "        o \n",
    "    - On the column type: \n",
    "        o    \n",
    "    - On the column name:\n",
    "        o  \n",
    "    - Add required columns:\n",
    "        o \n",
    "    - Split columns:\n",
    "        o "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.8. Table: 'messages'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How this table looks like without modification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablename='messages'\n",
    "\n",
    "pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "            delimiter=\",\",\n",
    "            error_bad_lines=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Is this table different from other previous loaded table? YES\n",
    "#### Q2: Is this table similar to any table in the generic Booter database schema? NO\n",
    "    - This table looks like: \n",
    "    - The blacklist table must to have the columns: \n",
    "    \n",
    "#### Q3: Are there modifications required? \n",
    "    - On the table name: \n",
    "        o \n",
    "    - On the column type: \n",
    "        o    \n",
    "    - On the column name:\n",
    "        o  \n",
    "    - Add required columns:\n",
    "        o \n",
    "    - Split columns:\n",
    "        o "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.9. Table: 'news'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How this table looks like without modification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablename='news'\n",
    "\n",
    "pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "            delimiter=\",\",\n",
    "            error_bad_lines=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Is this table different from other previous loaded table? YES\n",
    "#### Q2: Is this table similar to any table in the generic Booter database schema? NO\n",
    "    - This table looks like: \n",
    "    - The blacklist table must to have the columns: \n",
    "    \n",
    "#### Q3: Are there modifications required? \n",
    "    - On the table name: \n",
    "        o \n",
    "    - On the column type: \n",
    "        o    \n",
    "    - On the column name:\n",
    "        o  \n",
    "    - Add required columns:\n",
    "        o \n",
    "    - Split columns:\n",
    "        o "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.10. Table: 'payments'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How this table looks like without modification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablename='payments'\n",
    "\n",
    "pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "            delimiter=\",\",\n",
    "            error_bad_lines=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Is this table different from other previous loaded table? YES\n",
    "#### Q2: Is this table similar to any table in the generic Booter database schema? NO\n",
    "    - This table looks like: payments\n",
    "    - The blacklist table must to have the columns: 'id','userid','username','amountpaid','paymentemail','planid','tid','date'\n",
    "    \n",
    "#### Q3: Are there modifications required? \n",
    "    - On the table name: \n",
    "        o \n",
    "    - On the column type: \n",
    "        o  date => timestamptodate  \n",
    "    - On the column name:\n",
    "        o ID => id\n",
    "        o paid => amountpaid\n",
    "        o plan => planid\n",
    "        o user => userid\n",
    "        o email => paymentemail\n",
    "    - Add required columns:\n",
    "        o username\n",
    "    - Split columns:\n",
    "        o "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying table: 'payments'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Loading the column converting columns with predefined functions\n",
    "df_payments = pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "                                delimiter=\",\",\n",
    "                                error_bad_lines=False,\n",
    "                       converters = {'date':timestamp2datetime})\n",
    "\n",
    "###Changing names of columns\n",
    "df_payments.rename(columns = {'ID': 'id',\n",
    "                              'paid': 'amountpaid',\n",
    "                             'plan': 'planid',\n",
    "                             'user':'userid',\n",
    "                             'email':'paymentemail'},\n",
    "                         inplace=True)\n",
    "\n",
    "###Creating empty columns (with \"\" [for future string] or np.nan [for future float])\n",
    "df_payments['username'] = \"\"\n",
    "\n",
    "###Showing some lines after adapt the table\n",
    "df_payments.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.11. Table: 'plans'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How this table looks like without modification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablename='plans'\n",
    "\n",
    "pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "            delimiter=\",\",\n",
    "            error_bad_lines=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Is this table different from other previous loaded table? YES\n",
    "#### Q2: Is this table similar to any table in the generic Booter database schema? YES\n",
    "    - This table looks like: plans\n",
    "    - The blacklist table must to have the columns: 'planid','planname','plandescr','price','maxboottime','concurrency'\n",
    "    \n",
    "#### Q3: Are there modifications required? \n",
    "    - On the table name: \n",
    "        o \n",
    "    - On the column type: \n",
    "\n",
    "    - On the column name:\n",
    "        o ID => planid\n",
    "        o name => planname\n",
    "        o description => plandescr\n",
    "        o mbt => maxboottime\n",
    "        o concurrents => concurrency\n",
    "    - Add required columns:\n",
    "        o \n",
    "    - Split columns:\n",
    "        o "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying table: 'plans'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Loading the column converting columns with predefined functions\n",
    "df_plans = pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "                                delimiter=\",\",\n",
    "                                error_bad_lines=False,)\n",
    "#                        converters = {'date':timestamp2datetime})\n",
    "\n",
    "###Changing names of columns\n",
    "df_plans.rename(columns = {'ID': 'planid',\n",
    "                          'name': 'planname',\n",
    "                          'description':'plandescr',\n",
    "                          'mbt': 'maxboottime',\n",
    "                          'concurrents':'concurrency'},\n",
    "                         inplace=True)\n",
    "\n",
    "###Creating empty columns (with \"\" [for future string] or np.nan [for future float])\n",
    "# df_plans['generic_column_name_missing'] = \"\"\n",
    "\n",
    "###Showing some lines after adapt the table\n",
    "df_plans.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.12. Table: 'referrals'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How this table looks like without modification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablename='referrals'\n",
    "\n",
    "pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "            delimiter=\",\",\n",
    "            error_bad_lines=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Is this table different from other previous loaded table? YES\n",
    "#### Q2: Is this table similar to any table in the generic Booter database schema? NO\n",
    "    - This table looks like: \n",
    "    - The blacklist table must to have the columns: \n",
    "    \n",
    "#### Q3: Are there modifications required? \n",
    "    - On the table name: \n",
    "        o \n",
    "    - On the column type: \n",
    "        o    \n",
    "    - On the column name:\n",
    "        o  \n",
    "    - Add required columns:\n",
    "        o \n",
    "    - Split columns:\n",
    "        o "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.13. Table: 'skype_api'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How this table looks like without modification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablename='skype_api'\n",
    "\n",
    "pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "            delimiter=\",\",\n",
    "            error_bad_lines=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Is this table different from other previous loaded table? YES\n",
    "#### Q2: Is this table similar to any table in the generic Booter database schema? NO\n",
    "    - This table looks like: \n",
    "    - The blacklist table must to have the columns: \n",
    "    \n",
    "#### Q3: Are there modifications required? \n",
    "    - On the table name: \n",
    "        o \n",
    "    - On the column type: \n",
    "        o    \n",
    "    - On the column name:\n",
    "        o  \n",
    "    - Add required columns:\n",
    "        o \n",
    "    - Split columns:\n",
    "        o "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.14. Read the raw table: 'users'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How this table looks like without modification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablename='users'\n",
    "\n",
    "pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "            delimiter=\",\",\n",
    "            error_bad_lines=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Is this table different from other previous loaded table? YES\n",
    "#### Q2: Is this table similar to any table in the generic Booter database schema? NO\n",
    "    - This table looks like: users \n",
    "    - This table must to have the columns: 'userid','username','useremail','password','expire','planid'\n",
    "    \n",
    "#### Q3: Are there modifications required? \n",
    "    - On the table name: \n",
    "        o \n",
    "    - On the column type: \n",
    "        o    \n",
    "    - On the column name:\n",
    "        o ID => userid\n",
    "        o email => useremail\n",
    "        o membership => planid\n",
    "    - Add required columns:\n",
    "        o \n",
    "    - Split columns:\n",
    "        o "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying table: 'users'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Loading the column converting columns with predefined functions\n",
    "df_users = pd.read_csv(read_inserted_table(dumpfile, tablename),\n",
    "                                delimiter=\",\",\n",
    "                                error_bad_lines=False,)\n",
    "#                        converters = {'date':timestamp2datetime})\n",
    "\n",
    "###Changing names of columns\n",
    "df_users.rename(columns = {'ID': 'userid',\n",
    "                            'email': 'useremail',\n",
    "                          'membership':'planid'},\n",
    "                         inplace=True)\n",
    "\n",
    "###Creating empty columns (with \"\" [for future string] or np.nan [for future float])\n",
    "# df_users['generic_column_name_missing'] = \"\"\n",
    "\n",
    "###Showing some lines after adapt the table\n",
    "df_users.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align='center'>======================================================================\n",
    "<br>Final step of the manual part\n",
    "======================================================================</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Adding missing tables accordingly to our generic Booter database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_users=pd.DataFrame(columns=['userid','username','useremail','password','expire','plan'])      \n",
    "# df_logins=pd.DataFrame(columns=['id','userid','username','userip','date'])\n",
    "df_attacks= pd.DataFrame(columns=['id','userid','username','targetip','targeturl','duration','port','type','date'])\n",
    "# df_payments=pd.DataFrame(columns=['id','userid','username','amountpaid','paymentemail','planid','tid','date'])\n",
    "df_settings=pd.DataFrame(columns=['url','sitename','siteemail'])\n",
    "# df_gateways=pd.DataFrame(columns=['email'])\n",
    "# df_friendsenemies=pd.DataFrame(columns=['id','ip','note','userid','type'])\n",
    "# df_blacklist=pd.DataFrame(columns=['id','ip','note'])\n",
    "df_webshells=pd.DataFrame(columns=['id','url','status','lastchecked','attacktype'])\n",
    "df_servers=pd.DataFrame(columns=['id','ip'])\n",
    "# df_plans=pd.DataFrame(columns=['planid','planname','plandescr','price','maxboottime','concurrency'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!<br>\n",
    "STEP 2: Data Enrichment<br>\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Depending of the size of the data, this part can take HOURS. I tested for both small and big datasets and it worked. Be pacient. This will pay-off!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Libraries needed to retrieve information from external databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import cfscrape\n",
    "from lxml import etree\n",
    "import os.path\n",
    "import random\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Function to enrich IP addresseswith AS information and country "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# THANKS TO: team-cymru.org\n",
    "def iptoasn(iplist_teamcymruformat_filelocation,outputfile):\n",
    "    cat = subprocess.Popen(['cat', iplist_teamcymruformat_filelocation], \n",
    "                            stdout=subprocess.PIPE)\n",
    "    \n",
    "    netcat = subprocess.Popen(['netcat', 'whois.cymru.com', '43'],\n",
    "                              stdin=cat.stdout,\n",
    "                              stdout=outputfile)\n",
    "    time.sleep(3) #for some reason the poll does not work! This was the way to overcome the waiting time.\n",
    "    \n",
    "    return netcat.stdout      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Function to check if an IP address was Tor node in a given moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THANKS TO: exonerator.torproject.org\n",
    "def WasTorNode(ip, date ):\n",
    "    url=\"https://exonerator.torproject.org/?ip=\"+ip+\"&timestamp=\"+date\n",
    "    scraper = cfscrape.create_scraper()\n",
    "    scraped_html=scraper.get(url).content    \n",
    "    html_tree = etree.HTML(scraped_html)\n",
    "    result=html_tree.xpath(\"//h3[@class='panel-title']/text()\") # I was looking for <h3 class=\"panel-title\">Result is positive</h3>\n",
    "    tor_node=True if result == ['Result is positive'] else False\n",
    "    return tor_node \n",
    "# 'date' MUST BE formated as: Year-month-day (2016-03-31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Discovering the middle date of the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    try:\n",
    "        middle_date=(min(df_attacks['date'])+((max(df_attacks['date'])-min(df_attacks['date']))/2))\n",
    "        raise\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        middle_date=(min(df_logins['date'])+((max(df_logins['date'])-min(df_logins['date']))/2))\n",
    "        raise\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        middle_date=(min(df_payments['date'])+((max(df_payments['date'])-min(df_payments['date']))/2))\n",
    "        raise\n",
    "    except:\n",
    "        pass\n",
    "except Exception:\n",
    "    print(\"There is no date in the entire dataset\")\n",
    "\n",
    "date_tor_check = middle_date.strftime('%Y-%m-%d')\n",
    "date_iptoasn_lookup= str(middle_date)\n",
    "print(date_tor_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Preparing to Perform IP to ASN info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_logins['middledate']=date_iptoasn_lookup\n",
    "df_attacks['middledate']=date_iptoasn_lookup\n",
    "df_friendsenemies['middledate']=date_iptoasn_lookup\n",
    "df_blacklist['middledate']=date_iptoasn_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1  Lookup IP to ASN info of table: logins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if (os.path.exists('enrichments/logins_iptoasn_out')== False):\n",
    "    logins_iptoasn_in = open('enrichments/logins_iptoasn_in', 'w+')\n",
    "    logins_iptoasn_in.write('begin\\nverbose\\n')\n",
    "    df_logins[['userip','middledate']].drop_duplicates().to_csv(logins_iptoasn_in,header=False,index=False,sep=\"\\t\") \n",
    "    logins_iptoasn_in.write('end')\n",
    "    logins_iptoasn_in.close()\n",
    "\n",
    "    logins_iptoasn_out = open('logins_iptoasn_out', 'w+')\n",
    "    iptoasn('logins_iptoasn_in',logins_iptoasn_out)\n",
    "    logins_iptoasn_out.close()\n",
    "else:\n",
    "    print(\"You already performed the lookup for this table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_logins_iptoasn = pd.read_csv('enrichments/logins_iptoasn_out',\\\n",
    "                                skiprows=1,\\\n",
    "                             delimiter=\"\\s+\\|\\s\",\\\n",
    "                             names = ['asn', 'ip', 'bgp_prefix', 'country','registry','info_date','info_request','as_name'])\n",
    "\n",
    "df_logins_extended= pd.merge(df_logins,\n",
    "                              df_logins_iptoasn,\n",
    "                              how = 'left',\n",
    "                              left_on = 'userip',\n",
    "                              right_on = 'ip')\n",
    "\n",
    "# Changing name of columns to avoid misunderstandings\n",
    "df_logins_extended.rename(columns={'asn':'srcasn', \n",
    "                                   'ip':'srcip', \n",
    "                                   'bgp_prefix':'srcbgp_prefix', \n",
    "                                   'country':'srccountry' ,\n",
    "                                   'registry':'srcregistry',\n",
    "                                   'info_date':'srcinfo_date',\n",
    "                                   'info_request':'srcinfo_request'},\n",
    "                         inplace=True)\n",
    "\n",
    "\n",
    "df_logins_extended.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.2  Lookup IP to ASN info of table: attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if (os.path.exists('enrichments/attacks_iptoasn_out')== False):\n",
    "    attacks_iptoasn_in = open('enrichments/attacks_iptoasn_in', 'w+')\n",
    "    attacks_iptoasn_in.write('begin\\nverbose\\n')\n",
    "    df_attacks[['targetip','middledate']].drop_duplicates().to_csv(attacks_iptoasn_in,header=False,index=False,sep=\"\\t\") \n",
    "    attacks_iptoasn_in.write('end')\n",
    "    attacks_iptoasn_in.close()\n",
    "\n",
    "    attacks_iptoasn_out = open('attacks_iptoasn_out', 'w+')\n",
    "    iptoasn('attacks_iptoasn_in',attacks_iptoasn_out)\n",
    "    attacks_iptoasn_out.close()\n",
    "else:\n",
    "    print(\"You already performed the lookup for this table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_attacks_iptoasn = pd.read_csv('enrichments/attacks_iptoasn_out',\\\n",
    "                                skiprows=1,\\\n",
    "                             delimiter=\"\\s+\\|\\s\",\\\n",
    "                             names = ['asn', 'ip', 'bgp_prefix', 'country','registry','info_date','info_request','as_name'])\n",
    "\n",
    "# Merging the iptoasn with the queried column\n",
    "df_attacks_extended= pd.merge(df_attacks,\n",
    "                              df_attacks_iptoasn,\n",
    "                              how = 'left',\n",
    "                              left_on = 'targetip',\n",
    "                              right_on = 'ip')\n",
    "\n",
    "# Changing name of columns to avoid misunderstandings\n",
    "df_attacks_extended.rename(columns={'asn':'targetasn', \n",
    "                                   'ip_y':'targetip', \n",
    "                                   'bgp_prefix':'targetbgp_prefix', \n",
    "                                   'country_y':'targetcountry' ,\n",
    "                                   'registry':'targetregistry',\n",
    "                                   'info_date':'targetinfo_date',\n",
    "                                   'info_request':'targetinfo_request'},\n",
    "                         inplace=True)\n",
    "df_attacks_extended.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.3  Lookup IP to ASN info of table: friendsenemies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if (os.path.exists('enrichments/friendsenemies_iptoasn_out')== False):\n",
    "    friendsenemies_iptoasn_in = open('enrichments/friendsenemies_iptoasn_in', 'w+')\n",
    "    friendsenemies_iptoasn_in.write('begin\\nverbose\\n')\n",
    "    df_friendsenemies[['ip','middledate']].drop_duplicates().to_csv(friendsenemies_iptoasn_in,header=False,index=False,sep=\"\\t\") \n",
    "    friendsenemies_iptoasn_in.write('end')\n",
    "    friendsenemies_iptoasn_in.close()\n",
    "\n",
    "    friendsenemies_iptoasn_out = open('friendsenemies_iptoasn_out', 'w+')\n",
    "    iptoasn('friendsenemies_iptoasn_in',friendsenemies_iptoasn_out)\n",
    "    friendsenemies_iptoasn_out.close()\n",
    "else:\n",
    "    print(\"You already performed the lookup for this table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_friendsenemies_iptoasn = pd.read_csv('enrichments/friendsenemies_iptoasn_out',\\\n",
    "                                skiprows=1,\\\n",
    "                             delimiter=\"\\s+\\|\\s\",\\\n",
    "                             names = ['asn', 'ip', 'bgp_prefix', 'country','registry','info_date','info_request','as_name'])\n",
    "\n",
    "# Merging the iptoasn with the queried column\n",
    "df_friendsenemies_extended= pd.merge(df_friendsenemies,\n",
    "                              df_friendsenemies_iptoasn,\n",
    "                              how = 'left',\n",
    "                              left_on = 'ip',\n",
    "                              right_on = 'ip')\n",
    "\n",
    "# Changing name of columns to avoid misunderstandings\n",
    "df_friendsenemies_extended.rename(columns={'asn':'friendsenemiesasn', \n",
    "                                   'ip':'friendsenemiesip', \n",
    "                                   'bgp_prefix':'friendsenemiesbgp_prefix', \n",
    "                                   'country':'friendsenemiescountry' ,\n",
    "                                   'registry':'friendsenemiesregistry',\n",
    "                                   'info_date':'friendsenemiesinfo_date',\n",
    "                                   'info_request':'friendsenemiesinfo_request',\n",
    "                                   'as_name': 'friendsenemiesas_name'},\n",
    "                         inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.4  Lookup IP to ASN info of table: blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if (os.path.exists('enrichments/blacklist_iptoasn_out')== False):\n",
    "    blacklist_iptoasn_in = open('enrichments/blacklist_iptoasn_in', 'w+')\n",
    "    blacklist_iptoasn_in.write('begin\\nverbose\\n')\n",
    "    df_blacklist[['ip','middledate']].drop_duplicates().to_csv(blacklist_iptoasn_in,header=False,index=False,sep=\"\\t\") \n",
    "    blacklist_iptoasn_in.write('end')\n",
    "    blacklist_iptoasn_in.close()\n",
    "\n",
    "    blacklist_iptoasn_out = open('blacklist_iptoasn_out', 'w+')\n",
    "    iptoasn('blacklist_iptoasn_in',blacklist_iptoasn_out)\n",
    "    blacklist_iptoasn_out.close()\n",
    "else:\n",
    "    print(\"You already performed the lookup for this table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_blacklist_iptoasn = pd.read_csv('enrichments/blacklist_iptoasn_out',\\\n",
    "                                skiprows=1,\\\n",
    "                             delimiter=\"\\s+\\|\\s\",\\\n",
    "                             names = ['asn', 'ip', 'bgp_prefix', 'country','registry','info_date','info_request','as_name'])\n",
    "\n",
    "# Merging the iptoasn with the queried column\n",
    "df_blacklist_extended= pd.merge(df_blacklist,\n",
    "                              df_blacklist_iptoasn,\n",
    "                              how = 'left',\n",
    "                              left_on = 'ip',\n",
    "                              right_on = 'ip')\n",
    "\n",
    "# Changing name of columns to avoid misunderstandings\n",
    "df_blacklist_extended.rename(columns={'asn':'blacklistasn', \n",
    "                                   'ip':'blacklistip', \n",
    "                                   'bgp_prefix':'blacklistbgp_prefix', \n",
    "                                   'country':'blacklistcountry' ,\n",
    "                                   'registry':'blacklistregistry',\n",
    "                                   'info_date':'blacklistinfo_date',\n",
    "                                   'info_request':'blacklistinfo_request',\n",
    "                                   'as_name': 'blacklistas_name'},\n",
    "                         inplace=True)\n",
    "df_blacklist_extended.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.1. Check if IP was a TOR node for table: login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date_tor_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if len(df_logins['userip'].unique())<1200:\n",
    "    if (os.path.exists('enrichments/logins_torcheck')== False):\n",
    "        print(\"Note: it can take a while to finish...\")\n",
    "        logins_torcheck = open('logins_torcheck', 'w+')\n",
    "        for i in df_logins['userip'].unique():\n",
    "            wasTor=WasTorNode(i,date_tor_check)\n",
    "            print(i, wasTor, file=logins_torcheck)\n",
    "    #         print(i, wasTor) #DEBUGING =D\n",
    "            time.sleep(random.randint(1,3)) #adding some random sleep time\n",
    "            logins_torcheck.flush()\n",
    "\n",
    "        logins_torcheck.close()\n",
    "    else:\n",
    "        print(\"The IP addresses from this table were already checked.\")\n",
    "else:\n",
    "        print(\"Aborted!!! It will take more than one hour to analyse!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_logins_torcheck = pd.read_csv('enrichments/logins_torcheck',\\\n",
    "                          delimiter=\"\\s+\",\\\n",
    "                          names = ['userip', 'tor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.2. Check if IP was a TOR node for table: attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if len(df_attacks['targetip'].unique())<1200:\n",
    "    if (os.path.exists('enrichments/attacks_torcheck')== False):\n",
    "        print(\"Note: it can take a while to finish...\",len(df_attacks['targetip'].unique())*3,\"seconds (in the worst case).\")\n",
    "\n",
    "        attacks_torcheck = open('attacks_torcheck', 'w+')\n",
    "\n",
    "        for i in df_attacks['targetip'].unique():\n",
    "            wasTor=WasTorNode(i,date_tor_check)\n",
    "            print(i, wasTor, file=attacks_torcheck)\n",
    "            print(i, wasTor) #DEBUGING =D\n",
    "            time.sleep(random.randint(1,3)) #adding some random sleep time\n",
    "            attacks_torcheck.flush()\n",
    "\n",
    "        attacks_torcheck.close()\n",
    "    else:\n",
    "        print(\"The IP addresses from this table were already checked.\") \n",
    "else:\n",
    "        print(\"Aborted!!! It will take more than one hour to analyse!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_attacks_torcheck = pd.read_csv('enrichments/attacks_torcheck',\\\n",
    "                          delimiter=\"\\s+\",\\\n",
    "                          names = ['targetip', 'tor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.3. Check if IP was a TOR node for table: friendsenemies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(df_friendsenemies['ip'].unique()) <1200:\n",
    "    if (os.path.exists('enrichments/friendsenemies_torcheck')== False):\n",
    "        print(\"Note: it can take a while to finish...\",len(df_friendsenemies['ip'].unique())*3,\"seconds (in the worst case).\")\n",
    "\n",
    "        friendsenemies_torcheck = open('enrichments/friendsenemies_torcheck', 'w+')\n",
    "\n",
    "        for i in df_friendsenemies['ip'].unique():\n",
    "            wasTor=WasTorNode(i,date_tor_check)\n",
    "            print(i, wasTor, file=friendsenemies_torcheck)\n",
    "        #     print(i, wasTor) #DEBUGING =D\n",
    "            time.sleep(random.randint(1,3)) #adding some random sleep time\n",
    "            friendsenemies_torcheck.flush()\n",
    "\n",
    "        friendsenemies_torcheck.close()\n",
    "    else:\n",
    "        print(\"The IP addresses from this table were already checked.\") \n",
    "else:\n",
    "        print(\"Aborted!!! It will take more than one hour to analyse!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_friendsenemies_torcheck = pd.read_csv('enrichments/friendsenemies_torcheck',\\\n",
    "                          delimiter=\"\\s+\",\\\n",
    "                          names = ['ip', 'tor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.4. Check if IP was a TOR node for table: blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(df_blacklist['ip'].unique()) < 1200:\n",
    "    if (os.path.exists('enrichments/blacklist_torcheck')== False):\n",
    "        print(\"Note: it can take a while to finish...\",len(df_blacklist['ip'].unique())*3,\"seconds (in the worst case).\")\n",
    "\n",
    "        blacklist_torcheck = open('enrichments/blacklist_torcheck', 'w+')\n",
    "\n",
    "        for i in df_blacklist['ip'].unique():\n",
    "            wasTor=WasTorNode(i,date_tor_check)\n",
    "            print(i, wasTor, file=blacklist_torcheck)\n",
    "    #         print(i, wasTor) #DEBUGING =D\n",
    "            time.sleep(random.randint(1,3)) #adding some random sleep time\n",
    "            blacklist_torcheck.flush()\n",
    "\n",
    "        blacklist_torcheck.close()\n",
    "    else:\n",
    "        print(\"The IP addresses from this table were already checked.\") \n",
    "else:\n",
    "        print(\"Aborted!!! It will take more than one hour to analyse!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_blacklist_torcheck = pd.read_csv('enrichments/blacklist_torcheck',\\\n",
    "                          delimiter=\"\\s+\",\\\n",
    "                          names = ['ip', 'tor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Relation between Attack dates and Login dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nearestDate(base_date, date_list):\n",
    "    nearest={}\n",
    "    for date in date_list:\n",
    "        if (base_date.timestamp() - date.timestamp())>=0:\n",
    "            nearest[base_date.timestamp() - date.timestamp()]= date\n",
    "    return nearest[min(nearest.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This is the TOTAL number records to be checks!!!!\n",
    "len(df_attacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_attacks['nearestlogin']=\"\"\n",
    "df_attacks['nearestlogin']=pd.to_datetime(df_attacks['nearestlogin'])\n",
    "    \n",
    "if len(df_attacks)>0 and len(df_logins)>0:\n",
    "    #When was the last login of the user that performed attacks\n",
    "    df_attacks['nearestlogin']=\"\"\n",
    "    df_attacks['nearestlogin']=pd.to_datetime(df_attacks['nearestlogin'])\n",
    "\n",
    "    for index, row in df_attacks.head(100).iterrows():\n",
    "        intermediate_df= df_logins[df_logins['username']==row['username']]\n",
    "        nearestlogindate= nearestDate(row['date'],intermediate_df['date'])\n",
    "        df_attacks.set_value(index, 'nearestlogin', nearestlogindate)\n",
    "\n",
    "        #DEBUGGING\n",
    "        if index % 1000 == 0:\n",
    "            print(index,\": +1000 records analysed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(df_attacks['nearestlogin'].value_counts()) >1:\n",
    "    df_attacks_and_logins = pd.merge(df_attacks_extended,\n",
    "                                     df_logins_extended,\n",
    "                                     how = 'left',\n",
    "                                     left_on = ['username','nearestlogin'],\n",
    "                                     right_on = ['username','date'])\n",
    "else:\n",
    "    df_attacks_and_logins=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!<br>\n",
    "STEP 3: Automatic Analysis\n",
    "<br>\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Libraries that I use to plot figs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import *\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "plt.style.use('seaborn-muted')\n",
    "# plt.rcParams['font.family'] = 'serif'\n",
    "# plt.rcParams['font.size'] = 12\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.1. Number of records per table (part of the generic Booter database schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df_users),len(df_logins),len(df_attacks),len(df_payments),len(df_settings),len(df_gateways), len(df_friendsenemies),len(df_blacklist),len(df_webshells),len(df_servers),len(df_plans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.2. Number of users, customers, attackers, and their intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(set(df_users['userid'].unique())) > 1:\n",
    "    users_set=set(df_users['userid'].unique())\n",
    "else:\n",
    "    users_set=set(df_users['username'].unique())\n",
    "    \n",
    "if len(set(df_logins['userid'].unique())) > 1:\n",
    "    userslogin_set=set(df_logins['userid'].unique())\n",
    "else:\n",
    "    userslogin_set=set(df_logins['username'].unique())\n",
    "\n",
    "if len(set(df_payments['userid'].unique())) > 1:\n",
    "    customers_set=set(df_payments['userid'].unique())\n",
    "else:\n",
    "    customers_set=set(df_payments['username'].unique())\n",
    "\n",
    "if len(set(df_attacks['userid'].unique())) > 1:\n",
    "    attackers_set=set(df_attacks['userid'].unique())\n",
    "else:\n",
    "    attackers_set=set(df_attacks['username'].unique())\n",
    "\n",
    "intersec_customers_attacker=pd.Series(list(customers_set.intersection(attackers_set)))\n",
    "intersec_users_customers=pd.Series(list(users_set.intersection(customers_set)))\n",
    "intersec_users_attackers=pd.Series(list(users_set.intersection(attackers_set)))\n",
    "intersec_users_customers_attackers=pd.Series(list(users_set.intersection(customers_set).intersection(attackers_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(users_set),len(userslogin_set),len(customers_set),len(attackers_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4,3))\n",
    "fig.suptitle('Users, Customers & Attackers', fontsize=14)\n",
    "\n",
    "ax = plt.subplot2grid((1,1), (0,0))\n",
    "\n",
    "venn=venn3(ax=ax,subsets = {'001':len(attackers_set)-len(intersec_customers_attacker)-len(intersec_users_attackers)+len(intersec_users_customers_attackers), \n",
    "                            '010':len(customers_set)-len(intersec_users_customers)-len(intersec_customers_attacker)+len(intersec_users_customers_attackers), \n",
    "                            '011':len(intersec_customers_attacker)-len(intersec_users_customers_attackers),\n",
    "                            '100':len(users_set)-len(intersec_users_customers)-len(intersec_users_attackers)+len(intersec_users_customers_attackers),\n",
    "                            '101':len(intersec_users_attackers)-len(intersec_users_customers_attackers),\n",
    "                            '110':len(intersec_users_customers)-len(intersec_users_customers_attackers),\n",
    "                            '111':len(intersec_users_customers_attackers)},\\\n",
    "          set_labels = ('Users', 'Customers','Attackers'),\\\n",
    "          alpha=1)\n",
    "try:\n",
    "    venn.get_patch_by_id('100').set_color('#404096')\n",
    "except:\n",
    "    print(\"*Users set is empty!\")  \n",
    "    \n",
    "try:\n",
    "    venn.get_patch_by_id('110').set_color('#DEA73A')\n",
    "except:\n",
    "    print(\"*Customers set is empty!\")   \n",
    "\n",
    "try:\n",
    "    venn.get_patch_by_id('001').set_color('#D92120')\n",
    "except:\n",
    "    print(\"*Attackers set is empty!\")\n",
    "\n",
    "fig.show()\n",
    "# fig.savefig('../figs/timeseries_attacks.eps', format='eps', dpi=1200) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.3. Distribution of login times per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(df_logins['userid'].value_counts()) > 0:\n",
    "    num_distinct_logins_per_user=df_logins['userid'].value_counts()\n",
    "else:\n",
    "    num_distinct_logins_per_user=df_logins['username'].value_counts()\n",
    "\n",
    "freq_distinct_logins_per_user=num_distinct_logins_per_user.value_counts()\n",
    "cum_dist_user_logins = np.linspace(0.,1.,len(num_distinct_logins_per_user))\n",
    "cdf_user_logins = pd.Series(cum_dist_user_logins, index=num_distinct_logins_per_user.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(num_distinct_logins_per_user)>0:\n",
    "    fig = plt.figure(figsize=(6, 3))\n",
    "    fig.suptitle('* Distribution of Login Times by Users:', fontsize=14, y=1.05,x=0.35)\n",
    "    \n",
    "    #Plot CDF\n",
    "    ax1 = plt.subplot2grid((1,2), (0,0))\n",
    "    ax1 = cdf_user_logins.plot(ax=ax1,lw=2, drawstyle='steps',legend=False)\n",
    "    ax1.set_xlabel(\"# logins\")\n",
    "    ax1.set_ylabel(\"CDF of Users\")\n",
    "    ax1.set_ylim(0,1.05)\n",
    "    ax1.set_title(\"\")\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    \n",
    "    ax2 = plt.subplot2grid((1,2), (0,1))\n",
    "    freq_distinct_logins_per_user.plot(ax=ax2,kind='pie', \n",
    "                                       labels=None, \n",
    "                                       legend=False,\n",
    "                                       startangle=270,\n",
    "#                                        colors=sns.color_palette()\n",
    "                                       )\n",
    "    ax2.set_ylabel(\"\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    ####\n",
    "    # For Pie Chart Better Legend box\n",
    "    porcent = (100.*freq_distinct_logins_per_user.values)/(freq_distinct_logins_per_user.values.sum())\n",
    "    labels = ['{0} - {1:1.2f} %'.\\\n",
    "              format(i,j) for i,j in zip(freq_distinct_logins_per_user.index, porcent)]\n",
    "    # To Sort Legend (defaulf: keeps the same order)\n",
    "#     patches, labels, dummy =  zip(*sorted(zip(patches, labels, freq_distinct_logins_per_user.values),\n",
    "#                                           key=lambda x: x[2],\n",
    "#                                           reverse=True))\n",
    "    legend_show_top=10\n",
    "    ax2.legend(ax2.patches[0:legend_show_top], \n",
    "               labels[0:legend_show_top], \n",
    "               bbox_to_anchor=(1.5, 1.),\n",
    "               fontsize=10)\n",
    "    ####\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    fig.savefig('figs/login_times.eps', bbox_inches='tight',format='eps', dpi=1200)\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Number of Users that Login via TOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df_logins_torcheck[df_logins_torcheck['tor']==True]['userip'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Number of Distinct IP addresses by Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(df_logins.groupby(['userid','userip']).size().reset_index()['userid'].value_counts()) >0:\n",
    "    num_distinct_ips_per_user=df_logins.groupby(['userid','userip']).size().reset_index()['userid'].value_counts()\n",
    "else:\n",
    "    num_distinct_ips_per_user=df_logins.groupby(['username','userip']).size().reset_index()['username'].value_counts()\n",
    "    \n",
    "freq_distinct_ips_per_user=num_distinct_ips_per_user.value_counts()\n",
    "cum_dist_user_ips = np.linspace(0.,1.,len(num_distinct_ips_per_user))\n",
    "cdf_user_ips = pd.Series(cum_dist_user_ips, index=num_distinct_ips_per_user.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(num_distinct_ips_per_user)>0:\n",
    "    fig = plt.figure(figsize=(6, 3))\n",
    "    fig.suptitle('* Distribution of Distinct IP address used by Users:', fontsize=14, y=1.05, x=0.4)\n",
    "\n",
    "    ax1 = plt.subplot2grid((1,2), (0,0))\n",
    "    ax1 = cdf_user_ips.plot( ax=ax1,lw=2, drawstyle='steps',legend=False)\n",
    "    ax1.set_xlabel(\"# IPs\")\n",
    "    ax1.set_ylabel(\"CDF of Users\")\n",
    "    ax1.set_ylim(0,1.05)\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    ax2 = plt.subplot2grid((1,2), (0,1))\n",
    "    freq_distinct_ips_per_user.plot(ax=ax2,kind='pie',\n",
    "                                    labels=None,legend=False,\n",
    "                                       startangle=270,\n",
    "#                                        colors=sns.color_palette()\n",
    "                                       )\n",
    "    ax2.set_ylabel(\"\")\n",
    "    ####\n",
    "    # For Pie Chart Better Legend box\n",
    "    porcent = (100.*freq_distinct_ips_per_user.values)/(freq_distinct_ips_per_user.values.sum())\n",
    "    labels = ['{0} - {1:1.2f} %'.\\\n",
    "              format(i,j) for i,j in zip(freq_distinct_ips_per_user.index, porcent)]\n",
    "    # To Sort Legend (defaulf: keeps the same order)\n",
    "#     patches, labels, dummy =  zip(*sorted(zip(patches, labels, freq_distinct_logins_per_user.values),\n",
    "#                                           key=lambda x: x[2],\n",
    "#                                           reverse=True))\n",
    "    legend_show_top=10\n",
    "    ax2.legend(ax2.patches[0:legend_show_top], \n",
    "               labels[0:legend_show_top], \n",
    "               bbox_to_anchor=(1.5, 1.),\n",
    "               fontsize=10)\n",
    "    ####\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    fig.savefig('figs/num_ips_by_users.eps', bbox_inches='tight',format='eps', dpi=1200)\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5. Number of Payments by Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(df_payments['userid'].value_counts())>0:\n",
    "    num_distinct_payments_per_user=df_payments['userid'].value_counts()\n",
    "else:\n",
    "    num_distinct_payments_per_user=df_payments['username'].value_counts()\n",
    "\n",
    "freq_distinct_payments_per_user=num_distinct_payments_per_user.value_counts()\n",
    "cum_dist_user_payments = np.linspace(0.,1.,len(num_distinct_payments_per_user))\n",
    "cdf_user_payments = pd.Series(cum_dist_user_payments, index=num_distinct_payments_per_user.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(num_distinct_payments_per_user)>0:\n",
    "    fig = plt.figure(figsize=(6, 3))\n",
    "    fig.suptitle('* Distribution of Number of Payments by Users:', fontsize=14, y=1.05, x=0.4)\n",
    "\n",
    "    ax1 = plt.subplot2grid((1,2), (0,0))\n",
    "    ax1 = cdf_user_payments.plot( ax=ax1,lw=2, drawstyle='steps',legend=False)\n",
    "    ax1.set_xlabel(\"# Payment\")\n",
    "    ax1.set_ylabel(\"CDF of Users\")\n",
    "    ax1.set_ylim(0,1.05)\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    ax2 = plt.subplot2grid((1,2), (0,1))\n",
    "    freq_distinct_payments_per_user.plot(ax=ax2,kind='pie', \n",
    "                                         labels=None,legend=False,\n",
    "                                         startangle=270)\n",
    "    ax2.set_ylabel(\"\")\n",
    "    ####\n",
    "    # For Pie Chart Better Legend box\n",
    "    porcent = (100.*freq_distinct_payments_per_user.values)/(freq_distinct_payments_per_user.values.sum())\n",
    "    labels = ['{0} - {1:1.2f} %'.\\\n",
    "              format(i,j) for i,j in zip(freq_distinct_payments_per_user.index, porcent)]\n",
    "    # To Sort Legend (defaulf: keeps the same order)\n",
    "#     patches, labels, dummy =  zip(*sorted(zip(patches, labels, freq_distinct_logins_per_user.values),\n",
    "#                                           key=lambda x: x[2],\n",
    "#                                           reverse=True))\n",
    "    legend_show_top=10\n",
    "    ax2.legend(ax2.patches[0:legend_show_top], \n",
    "               labels[0:legend_show_top], \n",
    "               bbox_to_anchor=(1.55, 1.),\n",
    "               fontsize=10)\n",
    "    ####\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    fig.savefig('figs/payments_distribution.eps', bbox_inches='tight',format='eps', dpi=1200)\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6. Total Amount of Money Earned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(df_payments['amountpaid']) >0:\n",
    "    total_earned=df_payments['amountpaid'].values.sum()\n",
    "    'US$ {:,.2f}'.format(float(total_earned))\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7. Amount of Money Paid by Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_distinct_payments_money_per_user=df_payments['amountpaid'].value_counts()\n",
    "freq_distinct_payments_money_per_user=num_distinct_payments_money_per_user.value_counts()\n",
    "cum_dist_user_payments_money = np.linspace(0.,1.,len(num_distinct_payments_money_per_user))\n",
    "cdf_user_payments_money = pd.Series(cum_dist_user_payments_money, index=num_distinct_payments_money_per_user.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(num_distinct_payments_money_per_user)>0:\n",
    "    fig = plt.figure(figsize=(6, 3))\n",
    "    fig.suptitle('* Distribution of Money Payments by Users:', fontsize=14, y=1.05, x=0.4)\n",
    "\n",
    "    ax1 = plt.subplot2grid((1,2), (0,0))\n",
    "    ax1 = cdf_user_payments_money.plot( ax=ax1,lw=2, drawstyle='steps',legend=False)\n",
    "    ax1.set_xlabel(\"$\")\n",
    "    ax1.set_ylabel(\"CDF of Users\")\n",
    "    ax1.set_ylim(0,1.05)\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    ax2 = plt.subplot2grid((1,2), (0,1))\n",
    "    freq_distinct_payments_money_per_user.plot(ax=ax2,kind='pie', \n",
    "                                               labels=None,legend=False,\n",
    "                                               startangle=270)\n",
    "    ax2.set_ylabel(\"\")\n",
    "    ####\n",
    "    # For Pie Chart Better Legend box\n",
    "    porcent = (100.*freq_distinct_payments_money_per_user.values)/(freq_distinct_payments_money_per_user.values.sum())\n",
    "    labels = ['${0:1.2f} - {1:1.2f} %'.\\\n",
    "              format(i,j) for i,j in zip(freq_distinct_payments_money_per_user.index, porcent)]\n",
    "    # To Sort Legend (defaulf: keeps the same order)\n",
    "#     patches, labels, dummy =  zip(*sorted(zip(patches, labels, freq_distinct_logins_per_user.values),\n",
    "#                                           key=lambda x: x[2],\n",
    "#                                           reverse=True))\n",
    "    legend_show_top=10\n",
    "    ax2.legend(ax2.patches[0:legend_show_top], \n",
    "               labels[0:legend_show_top], \n",
    "               bbox_to_anchor=(1.6, 1.),\n",
    "               fontsize=10)\n",
    "    ####\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    fig.savefig('figs/distribution_amount_paid.eps', bbox_inches='tight',format='eps', dpi=1200)\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5.8. Countries from where Users Access Booters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logins_country_distribution_sorted = df_logins_iptoasn['country'].value_counts(sort=True,ascending=True)\n",
    "logins_country_distribution = df_logins_iptoasn['country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(logins_country_distribution)>0:\n",
    "    fig = plt.figure(figsize=(6, 3))\n",
    "    fig.suptitle('* Users Accessing from Countries:', fontsize=14, y=1.05, x=0.28)\n",
    "\n",
    "    ax1 = plt.subplot2grid((1,2), (0,0))\n",
    "    logins_country_distribution_sorted.plot(ax=ax1,kind='barh')\n",
    "    ax1.set_ylabel(\"# access\")\n",
    "    ax1.set_xlabel(\"Country\")\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    ax2 = plt.subplot2grid((1,2), (0,1))\n",
    "    logins_country_distribution.plot(ax=ax2,kind='pie', \n",
    "                                     labels=None,legend=False,\n",
    "                                     startangle=270)\n",
    "    ax2.set_ylabel(\"\")\n",
    "    ####\n",
    "    # For Pie Chart Better Legend box\n",
    "    porcent = (100.*logins_country_distribution.values)/(logins_country_distribution.values.sum())\n",
    "    labels = ['{0} - {1:1.2f} %'.\\\n",
    "              format(i,j) for i,j in zip(logins_country_distribution.index, porcent)]\n",
    "    # To Sort Legend (defaulf: keeps the same order)\n",
    "#     patches, labels, dummy =  zip(*sorted(zip(patches, labels, freq_distinct_logins_per_user.values),\n",
    "#                                           key=lambda x: x[2],\n",
    "#                                           reverse=True))\n",
    "    legend_show_top=10\n",
    "    ax2.legend(ax2.patches[0:legend_show_top], \n",
    "               labels[0:legend_show_top], \n",
    "               bbox_to_anchor=(1.55, 1.),\n",
    "               fontsize=10)\n",
    "    ####\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    fig.savefig('figs/user_countries.eps', bbox_inches='tight',format='eps', dpi=1200)\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.9. Countries of Blacklisted IPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blacklist_country_distribution=df_blacklist_iptoasn['country'].value_counts()\n",
    "blacklist_country_distribution_sorted=df_blacklist_iptoasn['country'].value_counts(sort=True,ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(blacklist_country_distribution)>0:\n",
    "    fig = plt.figure(figsize=(6, 3))\n",
    "    fig.suptitle('* Countries of blacklisted IPs', fontsize=14, y=1.05, x=0.28)\n",
    "\n",
    "    ax1 = plt.subplot2grid((1,2), (0,0))\n",
    "    blacklist_country_distribution_sorted.plot(ax=ax1,kind='barh')\n",
    "    ax1.set_ylabel(\"# Access\")\n",
    "    ax1.set_xlabel(\"Country\")\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    ax2 = plt.subplot2grid((1,2), (0,1))\n",
    "    blacklist_country_distribution.plot(ax=ax2,kind='pie', \n",
    "                                        labels=None,legend=False,\n",
    "                                        startangle=270)\n",
    "    ax2.set_ylabel(\"\")\n",
    "    ####\n",
    "    # For Pie Chart Better Legend box\n",
    "    porcent = (100.*blacklist_country_distribution.values)/(logins_country_distribution.values.sum())\n",
    "    labels = ['{0} - {1:1.2f} %'.\\\n",
    "              format(i,j) for i,j in zip(blacklist_country_distribution.index, porcent)]\n",
    "    # To Sort Legend (defaulf: keeps the same order)\n",
    "#     patches, labels, dummy =  zip(*sorted(zip(patches, labels, freq_distinct_logins_per_user.values),\n",
    "#                                           key=lambda x: x[2],\n",
    "#                                           reverse=True))\n",
    "    legend_show_top=10\n",
    "    ax2.legend(ax2.patches[0:legend_show_top], \n",
    "               labels[0:legend_show_top], \n",
    "               bbox_to_anchor=(1.55, 1.),\n",
    "               fontsize=10)\n",
    "    ####\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    fig.savefig('figs/blacklist_countries.eps', bbox_inches='tight',format='eps', dpi=1200)\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.10. Countries of Target IPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attacks_country_distribution=df_attacks_iptoasn['country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(attacks_country_distribution)>0:\n",
    "    fig = plt.figure(figsize=(6, 3))\n",
    "    fig.suptitle('* Countries of target IPs', fontsize=14, y=1.05, x=0.28)\n",
    "\n",
    "    ax1 = plt.subplot2grid((1,2), (0,0))\n",
    "    attacks_country_distribution.plot(ax=ax1,kind='bar')\n",
    "    ax1.set_ylabel(\"# Access\")\n",
    "    ax1.set_xlabel(\"Country\")\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    ax2 = plt.subplot2grid((1,2), (0,1))\n",
    "    attacks_country_distribution.plot(ax=ax2,kind='pie', \n",
    "                                      labels=None,legend=False,\n",
    "                                      startangle=270)\n",
    "    ax2.set_ylabel(\"\")\n",
    "    ####\n",
    "    # For Pie Chart Better Legend box\n",
    "    porcent = (100.*attacks_country_distribution.values)/(attacks_country_distribution.values.sum())\n",
    "    labels = ['{0} - {1:1.2f} %'.\\\n",
    "              format(i,j) for i,j in zip(attacks_country_distribution.index, porcent)]\n",
    "    # To Sort Legend (defaulf: keeps the same order)\n",
    "#     patches, labels, dummy =  zip(*sorted(zip(patches, labels, freq_distinct_logins_per_user.values),\n",
    "#                                           key=lambda x: x[2],\n",
    "#                                           reverse=True))\n",
    "    legend_show_top=10\n",
    "    ax2.legend(ax2.patches[0:legend_show_top], \n",
    "               labels[0:legend_show_top], \n",
    "               bbox_to_anchor=(1.55, 1.),\n",
    "               fontsize=10)\n",
    "    ####\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    fig.savefig('figs/attack_countries_distribution.eps',bbox_inches='tight', format='eps', dpi=1200) \n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.12. Attacks on Same Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_attacks_on_sametarget=df_attacks['targetip'].value_counts()\n",
    "\n",
    "freq_num_attacks_on_sametarget=num_attacks_on_sametarget.value_counts()\n",
    "cum_num_attacks_on_sametarget = np.linspace(0.,1.,len(num_attacks_on_sametarget))\n",
    "cdf_num_attacks_on_sametarget = pd.Series(cum_num_attacks_on_sametarget, index=num_attacks_on_sametarget.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(num_attacks_on_sametarget)>0:\n",
    "    fig = plt.figure(figsize=(6, 3))\n",
    "    fig.suptitle('* Attacks on Same Targets:', fontsize=14, y=1.05,x=0.28)\n",
    "\n",
    "    ax1 = plt.subplot2grid((1,2), (0,0))\n",
    "    ax1 = cdf_num_attacks_on_sametarget.plot( ax=ax1,lw=2, drawstyle='steps',legend=False)\n",
    "    ax1.set_xlabel(\"# IPs\")\n",
    "    ax1.set_ylabel(\"CDF of Users\")\n",
    "    ax1.set_ylim(0,1.05)\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    ax2 = plt.subplot2grid((1,2), (0,1))\n",
    "    freq_num_attacks_on_sametarget.plot(ax=ax2,kind='pie',\n",
    "                                        labels=None,legend=False,\n",
    "                                        startangle=270)\n",
    "    ax2.set_ylabel(\"\")\n",
    "    ####\n",
    "    # For Pie Chart Better Legend box\n",
    "    porcent = (100.*freq_num_attacks_on_sametarget.values)/(freq_num_attacks_on_sametarget.values.sum())\n",
    "    labels = ['{0} - {1:1.2f} %'.\\\n",
    "              format(i,j) for i,j in zip(freq_num_attacks_on_sametarget.index, porcent)]\n",
    "    # To Sort Legend (defaulf: keeps the same order)\n",
    "#     patches, labels, dummy =  zip(*sorted(zip(patches, labels, freq_distinct_logins_per_user.values),\n",
    "#                                           key=lambda x: x[2],\n",
    "#                                           reverse=True))\n",
    "    legend_show_top=10\n",
    "    ax2.legend(ax2.patches[0:legend_show_top], \n",
    "               labels[0:legend_show_top], \n",
    "               bbox_to_anchor=(1.55, 1.),\n",
    "               fontsize=10)\n",
    "    ####\n",
    "    fig.tight_layout()\n",
    "\n",
    "    fig.show()\n",
    "    fig.savefig('figs/attacks_on_same_target.eps', bbox_inches='tight',format='eps', dpi=1200)\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.11. Attacks per day (timeseries) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(df_attacks)>0:\n",
    "    attack_timeseries=df_attacks.set_index(['date']).groupby(pd.TimeGrouper(freq='D')).agg(['count'])['action']\n",
    "    attack_mean_perday=attack_timeseries.mean()\n",
    "    attack_median_perday=attack_timeseries.median()\n",
    "else:\n",
    "    attack_timeseries=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(attack_timeseries)>0:\n",
    "    fig = plt.figure(figsize=(6,3))\n",
    "\n",
    "    ax1 = plt.subplot2grid((1,1), (0,0), rowspan=2)\n",
    "    attack_timeseries.plot(ax=ax1,\n",
    "                           lw=2,\n",
    "                           legend=False,\n",
    "    #                        style='--'\n",
    "                          )\n",
    "\n",
    "    # X and Y Labels and Ticks\n",
    "    ax1.set_xlabel(\"Time (bin=day)\")\n",
    "    ax1.set_ylabel(\"# Attacks\")\n",
    "\n",
    "    ax1.annotate(str(int(attack_median_perday[0]))+' (median)', \n",
    "                 (min(df_attacks['date']), attack_median_perday),\n",
    "                 xytext=(350, -1), \n",
    "                 textcoords='offset points',\n",
    "                 color='black', \n",
    "                 arrowprops=dict(arrowstyle='-|>',\n",
    "                                 color='black'))\n",
    "    fig.savefig('figs/attacks_timeseries.eps', bbox_inches='tight',format='eps', dpi=1200)\n",
    "\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.12. Time Between Logins and Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(df_attacks_and_logins)>0:\n",
    "    print(\"redo\")\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.13. Who attack whom? (users on the country level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(df_attacks_extended)>0 and len(df_logins_extended)>0:\n",
    "    merged_attacks_logins = pd.merge(df_attacks_extended,\n",
    "                                  df_logins_extended,\n",
    "                                  how = 'left',\n",
    "                                  left_on = 'date',\n",
    "                                  right_on = 'date')[['targetcountry','srccountry']]\n",
    "\n",
    "    who_against_whom = merged_attacks_logins.groupby(['targetcountry','srccountry'])\\\n",
    "                            .size()\\\n",
    "                            .reset_index()\\\n",
    "                            .pivot('srccountry','targetcountry',0)\n",
    "else:\n",
    "    who_against_whom =\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(who_against_whom)>0:\n",
    "    fig = plt.figure(figsize=(8,12))\n",
    "    fig.suptitle('Countries of Target IPs', fontsize=14, y=.92)\n",
    "\n",
    "    ax1 = plt.subplot2grid((1,1), (0,0))\n",
    "    sns.set()\n",
    "    sns.heatmap(who_against_whom,\n",
    "                ax=ax1,\n",
    "#                 cmap=\"YlGnBu\",\n",
    "    #             linewidths=.5,\n",
    "    #             annot=True\n",
    "                )\n",
    "\n",
    "    ax1.set_ylabel(\"Source Country\")\n",
    "    ax1.set_xlabel(\"Attack Target Country\")\n",
    "\n",
    "    fig.show()\n",
    "    fig.savefig('figs/who_attack_whom.eps', bbox_inches='tight',format='eps', dpi=1200)\n",
    "else:\n",
    "    print(\"Unfortunately, there is no data available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!<br>\n",
    "FIFTH PART: Query Interface<br>\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import widgets, interact, Dropdown,HTML\n",
    "from IPython.display import display,clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search_by_userid_submit(sender):\n",
    "    clear_output()\n",
    "    print(\"Searching by id =\",search_by_userid.value)\n",
    "    \n",
    "    if len(df_users[df_users['userid']== int(search_by_userid.value)])>0:\n",
    "        display(df_users[df_logins['userid']== int(search_by_userid.value)])\n",
    "    \n",
    "    if len(df_logins_extended[df_logins_extended['userid']== int(search_by_userid.value)])>0:\n",
    "        display(df_logins_extended[df_logins_extended['userid']== int(search_by_userid.value)])\n",
    "    \n",
    "    if len(df_attacks_extended[df_attacks_extended['userid']== int(search_by_userid.value)])>0:\n",
    "        display(df_attacks_extended[df_attacks_extended['userid']== int(search_by_userid.value)])\n",
    "    \n",
    "    if len(df_payments[df_payments['userid']== int(search_by_userid.value)])>0:\n",
    "        display(df_payments[df_payments['userid']== int(search_by_userid.value)])\n",
    "\n",
    "def search_by_username_submit(sender):\n",
    "    clear_output()\n",
    "    print(\"Searching by username =\",search_by_username.value,\"\\n\")\n",
    "    \n",
    "    if len(df_users[df_users['username']== search_by_username.value])>0:\n",
    "        print(\"Table df_users:\")\n",
    "        display(df_users[df_users['username']== search_by_username.value])\n",
    "    \n",
    "    if len(df_logins_extended[df_logins_extended['username']== search_by_username.value])>0:\n",
    "        print(\"Table df_logins:\")\n",
    "        display(df_logins_extended[df_logins_extended['username']== search_by_username.value])\n",
    "\n",
    "    if len(df_attacks_extended[df_attacks_extended['username']== search_by_username.value])>0:\n",
    "        print(\"Table df_attacks:\")\n",
    "        display(df_attacks_extended[df_attacks_extended['username']== search_by_username.value])\n",
    "    \n",
    "    if len(df_payments[df_payments['username']== search_by_username.value])>0:\n",
    "        print(\"Table df_payments:\")\n",
    "        display(df_payments[df_payments['username']== search_by_username.value])\n",
    "        \n",
    "def search_by_ip_submit(sender):\n",
    "    clear_output()\n",
    "    print(\"Searching by IP address =\",str(search_by_ip.value),\"\\n\")\n",
    "    \n",
    "    if len(df_logins_extended[df_logins_extended['userip']== str(search_by_ip.value)])>0:\n",
    "        print(\"Table df_logins (as ATTACKER:\")\n",
    "        display(df_logins_extended[df_logins_extended['userip']== str(search_by_ip.value)])\n",
    "     \n",
    "    if len(df_attacks_extended[df_attacks_extended['targetip']== str(search_by_ip.value)])>0:\n",
    "        print(\"Table df_attacks (as TARGET):\")\n",
    "        display(df_attacks_extended[df_attacks_extended['targetip']== str(search_by_ip.value)])  \n",
    "      \n",
    "       \n",
    "def search_by_asn_submit(sender):\n",
    "    clear_output()\n",
    "    print(\"Searching by Autonomous System Number (ASN) =\",search_by_asn.value,\"\\n\")\n",
    "    \n",
    "    if len(df_logins_extended[df_logins_extended['srcasn']== int(search_by_asn.value)])>0:\n",
    "        print(\"Table df_logins (as ATTACKER):\")\n",
    "        display(df_logins_extended[df_logins_extended['srcasn']== int(search_by_asn.value)])\n",
    "    \n",
    "    if len(df_attacks_extended[df_attacks_extended['targetasn']== str(search_by_asn.value)])>0:\n",
    "        print(\"Table df_attacks (as TARGET):\")\n",
    "        display(df_attacks_extended[df_attacks_extended['targetasn']== str(search_by_asn.value)])\n",
    "\n",
    "    if len(df_attacks_extended[df_attacks_extended['targetasn']== str(search_by_asn.value)])>0:\n",
    "        print(\"Table df_attacks (as TARGET):\")\n",
    "        display(df_attacks_extended[df_attacks_extended['targetasn']== str(search_by_asn.value)])\n",
    "\n",
    "        \n",
    "country_list=pd.read_csv('https://raw.githubusercontent.com/datasets/country-list/master/data.csv',delimiter=\",\",error_bad_lines=False)\n",
    "def search_by_country_submit():\n",
    "    country_code=country_list[country_list['Name']==search_by_country.value]['Code'].values[0]\n",
    "    clear_output()\n",
    "    print(\"Searching by Country =\",search_by_country.value,\"\\n\")\n",
    "    \n",
    "    if len(df_logins_extended[df_logins_extended['srccountry']== country_code])>0:\n",
    "        print(\"Table df_logins (as ATTACKER):\")\n",
    "        display(df_logins_extended[df_logins_extended['srccountry']== country_code])\n",
    "    \n",
    "    if len(df_attacks_extended[df_attacks_extended['targetcountry']== country_code])>0:\n",
    "        print(\"Table df_attacks (as TARGET):\")\n",
    "        display(df_attacks_extended[df_attacks_extended['targetcountry']== country_code])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(HTML('<h3>*Search by (only) one field per time:</h3>'))\n",
    "\n",
    "search_by_username = widgets.Text(description=\"username\")\n",
    "display(search_by_username)\n",
    "search_by_username.on_submit(search_by_username_submit)\n",
    "\n",
    "search_by_userid = widgets.Text(description=\"userid\")\n",
    "display(search_by_userid)\n",
    "search_by_userid.on_submit(search_by_userid_submit)\n",
    "\n",
    "search_by_ip = widgets.Text(description=\"IP\")\n",
    "display(search_by_ip)\n",
    "search_by_ip.on_submit(search_by_ip_submit)\n",
    "\n",
    "search_by_asn = widgets.Text(description=\"ASN\")\n",
    "display(search_by_asn)\n",
    "search_by_asn.on_submit(search_by_asn_submit)\n",
    "\n",
    "search_by_country = Dropdown(description=\"Country\", options=country_list['Name'].tolist())\n",
    "search_by_country.on_trait_change(search_by_country_submit, name=\"value\")\n",
    "display(search_by_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
